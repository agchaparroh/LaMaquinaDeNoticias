# Tests End-to-End (E2E)

Esta carpeta contiene los tests end-to-end que validan flujos completos del sistema "La MÃ¡quina de Noticias".

## ğŸ“ Estructura

```
tests/e2e/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_article_minimal_flow.py      # Flujo mÃ­nimo de un artÃ­culo
â”œâ”€â”€ test_editorial_feedback_flow.py   # Flujo de feedback editorial
â””â”€â”€ test_multiple_articles_flow.py    # Procesamiento mÃºltiple
```

## ğŸ§ª Tests Implementados

### 1. test_article_minimal_flow.py
Valida el recorrido completo de un artÃ­culo:
- **Scraper** â†’ genera archivo .json.gz
- **Connector** â†’ lee y envÃ­a a Pipeline
- **Pipeline** â†’ procesa y extrae hechos
- **Database** â†’ almacena en Supabase
- **Dashboard** â†’ muestra artÃ­culo procesado

**Tests incluidos:**
- `test_article_minimal_flow()` - Flujo bÃ¡sico exitoso
- `test_article_minimal_flow_with_timing()` - Con mediciÃ³n de tiempos

### 2. test_editorial_feedback_flow.py
Valida el flujo de feedback editorial:
- Editor ve artÃ­culo con importancia automÃ¡tica
- Editor ajusta importancia via API
- Backend actualiza base de datos
- Dashboard refleja cambios
- Feedback se registra para mejora del algoritmo

**Tests incluidos:**
- `test_editorial_feedback_flow()` - Flujo bÃ¡sico de feedback
- `test_multiple_feedback_changes()` - MÃºltiples cambios secuenciales
- `test_feedback_validation_errors()` - Manejo de errores

### 3. test_multiple_articles_flow.py
Valida procesamiento concurrente:
- Scraper genera batch de 5+ artÃ­culos
- Connector valida todos
- Pipeline procesa en paralelo
- Dashboard muestra todos correctamente

**Tests incluidos:**
- `test_multiple_articles_flow()` - 5 artÃ­culos simultÃ¡neos
- `test_multiple_articles_with_failures()` - Manejo de fallos
- `test_concurrent_load_stress()` - Test de carga con mÃºltiples batches

## ğŸš€ Ejecutar Tests

### Todos los tests E2E:
```bash
# Desde la carpeta tests/
pytest e2e/ -v -s

# O usando el script
python run_e2e_tests.py
```

### Un test especÃ­fico:
```bash
pytest e2e/test_article_minimal_flow.py -v -s
```

### Con mÃ¡s detalle:
```bash
pytest e2e/ -v -s --tb=long
```

## ğŸ“Š MÃ©tricas Capturadas

Los tests E2E capturan:
- **Tiempos de procesamiento** por etapa
- **Tasas de Ã©xito/fallo**
- **Throughput** (artÃ­culos/segundo)
- **Integridad de datos** en el flujo

## âš ï¸ Consideraciones

1. **Mocks vs Real**: Los tests usan mocks para simular componentes. Para tests contra el sistema real, ajustar las implementaciones.

2. **Timeouts**: Los tests asumen tiempos de respuesta rÃ¡pidos. En un sistema real, ajustar timeouts.

3. **Concurrencia**: Los tests de carga usan ThreadPoolExecutor. El sistema real puede usar asyncio o diferentes estrategias.

4. **Base de Datos**: Los tests simulan Supabase. Para tests reales, usar una BD de pruebas.

## ğŸ”§ Extender Tests

Para agregar nuevos tests E2E:

1. Crear archivo `test_nuevo_flujo.py`
2. Implementar clase que simule el flujo
3. Agregar validaciones en cada etapa
4. Documentar el flujo testeado

Ejemplo:
```python
class NuevoFlujoE2E:
    def __init__(self):
        # Inicializar datos de prueba
        pass
    
    def etapa_1(self):
        # Simular primera etapa
        pass
    
    def etapa_2(self):
        # Simular segunda etapa
        pass

def test_nuevo_flujo():
    flow = NuevoFlujoE2E()
    # Ejecutar y validar flujo
```
