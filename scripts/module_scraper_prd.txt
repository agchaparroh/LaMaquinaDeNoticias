# PRD: Módulo de Recopilación - Scrapy (module_scraper)

## Visión General
Implementar un módulo de scrapping robusto y eficiente usando Scrapy para la recolección automática de contenido periodístico de fuentes web predefinidas. El sistema debe ser gratuito, eficaz y sostenible, evitando bloqueos y priorizando el respeto hacia los servidores objetivo.

## Objetivos del Proyecto
- Crear un sistema de scrapping basado en Scrapy que alimente continuamente el sistema con nuevos artículos
- Integrar soporte para spiders generados por Portia además de spiders codificados manualmente
- Implementar almacenamiento dual: HTML original comprimido y datos extraídos estructurados
- Establecer configuraciones conservadoras que minimicen el impacto en servidores objetivo
- Proveer robustez mediante manejo de errores, evitación de duplicados y monitoreo

## Funcionalidades Clave

### 1. Estructura Base del Proyecto Scrapy
- Configuración completa de Scrapy con settings.py optimizado
- Estructura de directorios estándar para spiders, items, pipelines y middlewares
- Integración con Docker para contenedorización
- Configuración de logging y debugging

### 2. Sistema de Items y Pipelines
- Definición de ArticuloInItem con todos los campos requeridos
- Pipeline para validación y limpieza de datos usando ItemLoaders
- Pipeline para almacenamiento en Supabase (tabla articles)
- Pipeline para compresión y almacenamiento de HTML original en Supabase Storage
- Pipeline DeltaFetch para evitar duplicados

### 3. Spiders Base y Especializados
- Clase BaseArticleSpider para funcionalidad común
- Clase BaseSitemapSpider para procesamiento de sitemaps XML
- Clase BaseCrawlSpider para navegación de sitios
- Spiders específicos para medios configurados (La Nación, El País, etc.)
- Soporte para integración con spiders de Portia

### 4. Configuración y Middlewares
- Middleware de User-Agent rotativo
- Middleware de delays y throttling
- Configuración de ROBOTSTXT_OBEY y políticas de cortesía
- Configuración de concurrent requests y dominio
- Middleware para manejo de errores y reintentos

### 5. Integración con Portia
- Configuración de volúmenes compartidos para spiders de Portia
- Cargador dinámico de spiders generados por Portia
- Compatibilidad entre formatos de output Portia y sistema interno

### 6. Monitoreo y Alertas
- Integración con Spidermon para monitoreo de salud de spiders
- Configuración de alertas para errores y anomalías
- Logging estructurado para debugging y análisis
- Métricas de rendimiento y estadísticas de extracción

### 7. Renderizado JavaScript (Opcional)
- Integración con Playwright para sitios que requieren JavaScript
- Configuración selectiva para usar solo cuando es necesario
- Optimización de recursos y tiempo de ejecución

## Tecnologías y Herramientas
- Scrapy como framework principal de scrapping
- Portia para generación visual de spiders
- Supabase para almacenamiento de datos y archivos
- Docker para contenedorización
- Playwright para renderizado JavaScript (cuando sea necesario)
- Spidermon para monitoreo
- Python 3.9+ como lenguaje base

## Entregables
1. Estructura completa del proyecto Scrapy configurado
2. Sistema de items y pipelines funcionando
3. Spiders base y especializados implementados
4. Integración con Portia configurada
5. Monitoreo y alertas implementadas
6. Documentación técnica y de uso
7. Tests unitarios y de integración
8. Dockerfile y configuración de contenedor

## Criterios de Éxito
- Capacidad de extraer artículos de múltiples fuentes de forma automática
- Almacenamiento correcto en Supabase sin duplicados
- Respeto a robots.txt y políticas de cortesía de sitios web
- Integración exitosa con spiders de Portia
- Sistema de monitoreo funcional con alertas
- Contenedor Docker operacional y escalable