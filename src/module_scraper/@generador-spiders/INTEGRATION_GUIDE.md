# GUÃA DE INTEGRACIÃ“N - La MÃ¡quina de Noticias

## ðŸŽ¯ OBJETIVO
Asegurar que los spiders generados se integren perfectamente con la arquitectura existente de La MÃ¡quina de Noticias.

## ðŸ“‹ CHECKLIST DE INTEGRACIÃ“N

### **1. VerificaciÃ³n del entorno**
- [ ] Variables de entorno de Supabase configuradas
- [ ] Estructura de directorios correcta
- [ ] Dependencias instaladas (`requirements.txt`)
- [ ] Base de datos con tabla `articulos` creada
- [ ] scrapy-crawl-once instalado

### **2. VerificaciÃ³n del cÃ³digo generado**
- [ ] Hereda de `BaseArticleSpider`
- [ ] Importa `ArticuloInItem` correctamente
- [ ] Configura los 3 pipelines del proyecto
- [ ] Implementa filtrado por secciÃ³n
- [ ] Configura scrapy-crawl-once para deduplicaciÃ³n

### **3. VerificaciÃ³n de funcionamiento**
- [ ] Spider se ejecuta sin errores
- [ ] Items pasan validaciÃ³n
- [ ] Datos se almacenan en Supabase
- [ ] Logs se generan correctamente
- [ ] No procesa artÃ­culos duplicados

## ðŸ”§ PASOS DE INTEGRACIÃ“N

### **PASO 1: Preparar el entorno**

```bash
# 1. Navegar al directorio del proyecto
cd /path/to/LaMaquinaDeNoticias/src/module_scraper

# 2. Verificar/crear archivo de configuraciÃ³n
cp config/.env.test.example config/.env.test

# 3. Editar configuraciÃ³n con credenciales
# SUPABASE_URL=https://xxxxx.supabase.co
# SUPABASE_KEY=xxxxx
# SUPABASE_SERVICE_ROLE_KEY=xxxxx

# 4. Instalar dependencias (incluye scrapy-crawl-once)
pip install -r requirements.txt
```

### **PASO 2: Ubicar el spider generado**

```bash
# El spider debe ir en:
scraper_core/spiders/{medio}_{seccion}_spider.py

# Ejemplo:
scraper_core/spiders/elpais_internacional_spider.py
```

### **PASO 3: Verificar imports y herencia**

```python
# âœ… Verificar que el spider tenga estos imports
from scraper_core.spiders.base.base_article import BaseArticleSpider
from scraper_core.items import ArticuloInItem
from scraper_core.spiders.base.utils import parse_date_string

# âœ… Verificar herencia correcta
class ElpaisInternacionalSpider(BaseArticleSpider):
    # NO heredar directamente de scrapy.Spider
```

### **PASO 4: Configurar pipelines y middlewares**

```python
# En custom_settings del spider
custom_settings = {
    **BaseArticleSpider.custom_settings,
    
    # ConfiguraciÃ³n scrapy-crawl-once
    'CRAWL_ONCE_ENABLED': True,
    'CRAWL_ONCE_PATH': '.scrapy/crawl_once/',
    'CRAWL_ONCE_DEFAULT': False,
    
    # Pipelines del proyecto
    'ITEM_PIPELINES': {
        'scraper_core.pipelines.validation.DataValidationPipeline': 100,
        'scraper_core.pipelines.cleaning.DataCleaningPipeline': 200,
        'scraper_core.pipelines.storage.SupabaseStoragePipeline': 300,
    }
}
```

### **PASO 5: Prueba inicial**

```bash
# Test de sintaxis
python -m py_compile scraper_core/spiders/elpais_internacional_spider.py

# Test de importaciÃ³n
python -c "from scraper_core.spiders.elpais_internacional_spider import ElpaisInternacionalSpider"

# Test de ejecuciÃ³n (limitado)
scrapy crawl elpais_internacional -s CLOSESPIDER_ITEMCOUNT=3 -L DEBUG
```

## ðŸ“Š ESTRUCTURA DE DATOS

### **Campos obligatorios de ArticuloInItem:**
```python
# El spider DEBE llenar estos campos
item['url'] = response.url
item['titular'] = 'TÃ­tulo del artÃ­culo'
item['medio'] = 'El PaÃ­s'
item['pais_publicacion'] = 'EspaÃ±a'
item['tipo_medio'] = 'diario'  # diario/agencia/revista
item['fecha_publicacion'] = datetime_object_or_string
item['contenido_texto'] = 'Contenido extraÃ­do'
item['fuente'] = self.name  # Nombre del spider
```

### **Campos adicionales importantes:**
```python
item['contenido_html'] = response.text  # HTML original
item['autor'] = 'Nombre Autor' or 'RedacciÃ³n'
item['idioma'] = 'es'
item['seccion'] = 'internacional'
item['fecha_recopilacion'] = datetime.utcnow()
item['metadata'] = {
    'spider_type': 'scraping',
    'extraction_method': 'html_parsing'
}
```

## ðŸ—„ï¸ INTEGRACIÃ“N CON SUPABASE

### **Verificar tabla articulos:**
```sql
-- La tabla debe existir con esta estructura
CREATE TABLE articulos (
    id BIGSERIAL PRIMARY KEY,
    url TEXT UNIQUE NOT NULL,
    storage_path TEXT,
    fuente TEXT NOT NULL,
    medio TEXT NOT NULL,
    medio_url_principal TEXT,
    pais_publicacion TEXT NOT NULL,
    tipo_medio TEXT NOT NULL,
    titular TEXT NOT NULL,
    fecha_publicacion TIMESTAMPTZ,
    autor TEXT,
    idioma TEXT DEFAULT 'es',
    seccion TEXT,
    etiquetas_fuente TEXT[],
    es_opinion BOOLEAN DEFAULT FALSE,
    es_oficial BOOLEAN DEFAULT FALSE,
    resumen TEXT,
    categorias_asignadas TEXT[],
    puntuacion_relevancia INTEGER,
    fecha_recopilacion TIMESTAMPTZ DEFAULT NOW(),
    fecha_procesamiento TIMESTAMPTZ,
    estado_procesamiento TEXT DEFAULT 'pendiente',
    error_detalle TEXT,
    contenido_texto TEXT NOT NULL,
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

### **Verificar bucket de storage:**
```python
# El pipeline guarda HTML comprimido en:
# Bucket: 'articulos'
# Path: '{medio}/{fecha}/{hash}.html.gz'
```

## ðŸ•·ï¸ SISTEMA DE DEDUPLICACIÃ“N CON SCRAPY-CRAWL-ONCE

### **ConfiguraciÃ³n scrapy-crawl-once:**
```python
# CRÃTICO: Sin esto, procesarÃ¡ duplicados
custom_settings = {
    'CRAWL_ONCE_ENABLED': True,
    'CRAWL_ONCE_PATH': '.scrapy/crawl_once/',
    'CRAWL_ONCE_DEFAULT': False,  # Control explÃ­cito
}

# En cada request de artÃ­culo:
yield scrapy.Request(
    url,
    callback=self.parse_article,
    meta={'crawl_once': True}  # Activar deduplicaciÃ³n
)
```

### **Base de datos de deduplicaciÃ³n:**
```
module_scraper/
â”œâ”€â”€ .scrapy/
â”‚   â””â”€â”€ crawl_once/
â”‚       â”œâ”€â”€ elpais_internacional.sqlite
â”‚       â”œâ”€â”€ infobae_america_latina_rss.sqlite
â”‚       â””â”€â”€ ...
```

## ðŸ“… PROGRAMACIÃ“N PERIÃ“DICA

### **Configurar cron:**
```bash
# Editar crontab
crontab -e

# Agregar entrada segÃºn estrategia:
# RSS - cada 30 minutos
*/30 * * * * cd /path/to/project && /usr/bin/python3 -m scrapy crawl elpais_internacional_rss >> logs/elpais_internacional_rss.log 2>&1

# Scraping - cada hora
0 * * * * cd /path/to/project && /usr/bin/python3 -m scrapy crawl elpais_internacional >> logs/elpais_internacional.log 2>&1

# Playwright - cada 2 horas
0 */2 * * * cd /path/to/project && /usr/bin/python3 -m scrapy crawl elpais_internacional_playwright >> logs/elpais_internacional_playwright.log 2>&1
```

### **Monitoreo de logs:**
```bash
# Ver Ãºltimas ejecuciones
tail -f logs/elpais_internacional.log

# Buscar errores
grep ERROR logs/elpais_internacional.log

# EstadÃ­sticas de items
grep "item_scraped_count" logs/elpais_internacional.log
```

## ðŸ” VERIFICACIÃ“N FINAL

### **1. Verificar items en base de datos:**
```sql
-- En Supabase SQL Editor
SELECT COUNT(*) FROM articulos WHERE fuente = 'elpais_internacional';

-- Ver Ãºltimos artÃ­culos
SELECT titular, fecha_publicacion, url 
FROM articulos 
WHERE fuente = 'elpais_internacional'
ORDER BY fecha_recopilacion DESC
LIMIT 10;
```

### **2. Verificar deduplicaciÃ³n:**
```bash
# Ver base de datos de URLs procesadas
ls -la .scrapy/crawl_once/

# Ejecutar spider nuevamente
scrapy crawl elpais_internacional -s CLOSESPIDER_ITEMCOUNT=10

# Verificar en logs que no procesa URLs repetidas
grep "crawl-once" logs/elpais_internacional.log
```

### **3. Verificar storage:**
```sql
-- Verificar que storage_path se llena
SELECT storage_path 
FROM articulos 
WHERE fuente = 'elpais_internacional' 
AND storage_path IS NOT NULL
LIMIT 5;
```

## ðŸš¨ TROUBLESHOOTING COMÃšN

### **Spider no encuentra la clase base:**
```bash
# Agregar al inicio del spider
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
```

### **Pipelines no se ejecutan:**
```bash
# Verificar en logs
grep "DataValidationPipeline" logs/spider.log
grep "SupabaseStoragePipeline" logs/spider.log
```

### **Sin conexiÃ³n a Supabase:**
```python
# Test de conexiÃ³n
from scraper_core.utils.supabase_client import get_supabase_client
client = get_supabase_client()
response = client.table('articulos').select("count").execute()
print(f"ArtÃ­culos en DB: {response.data}")
```

### **scrapy-crawl-once no funciona:**
```python
# Verificar que estÃ© en settings.py
SPIDER_MIDDLEWARES = {
    'scrapy_crawl_once.CrawlOnceMiddleware': 100,
}

DOWNLOADER_MIDDLEWARES = {
    'scrapy_crawl_once.CrawlOnceMiddleware': 50,
}

# Verificar permisos de escritura
import os
crawl_once_path = '.scrapy/crawl_once/'
os.makedirs(crawl_once_path, exist_ok=True)
```

## ðŸ“ˆ MÃ‰TRICAS DE Ã‰XITO

### **KPIs del spider integrado:**
1. **Tasa de Ã©xito**: > 90% de artÃ­culos extraÃ­dos
2. **DeduplicaciÃ³n**: 0% de duplicados en DB
3. **ValidaciÃ³n**: > 95% pasan validaciÃ³n
4. **Storage**: 100% con HTML almacenado
5. **Tiempo ejecuciÃ³n**: < 30 minutos

### **Consulta de mÃ©tricas:**
```sql
-- MÃ©tricas por spider
SELECT 
    fuente,
    COUNT(*) as total_articulos,
    COUNT(DISTINCT url) as urls_unicas,
    AVG(LENGTH(contenido_texto)) as promedio_contenido,
    MIN(fecha_recopilacion) as primera_ejecucion,
    MAX(fecha_recopilacion) as ultima_ejecucion
FROM articulos
WHERE fuente = 'elpais_internacional'
GROUP BY fuente;

-- Verificar que no hay duplicados
SELECT url, COUNT(*) as veces
FROM articulos
WHERE fuente = 'elpais_internacional'
GROUP BY url
HAVING COUNT(*) > 1;
```

---

**ðŸ“š Documentos relacionados:**
- `MAIN_WORKFLOW.md` - Proceso principal
- `ERROR_HANDLING.md` - Manejo de errores
- `EJEMPLOS_COMPLETOS.md` - Casos de uso reales
