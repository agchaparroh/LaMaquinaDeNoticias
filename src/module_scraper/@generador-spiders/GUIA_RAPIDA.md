# GU√çA R√ÅPIDA - Generador de Spiders

## üöÄ INICIO R√ÅPIDO

### **Input del usuario:**
```yaml
url_seccion: "https://elpais.com/internacional"
nombre_medio: "El Pa√≠s"
pais_publicacion: "Espa√±a"
tipo_medio: "diario"
rss_disponible: "No"  # o "S√≠" con url_rss
```

### **Decisi√≥n r√°pida:**
```
¬øRSS disponible?
‚îú‚îÄ S√ç ‚Üí RSS Spider (sin an√°lisis)
‚îî‚îÄ NO ‚Üí ¬øNecesita JS? 
         ‚îú‚îÄ S√ç ‚Üí Playwright Spider
         ‚îî‚îÄ NO ‚Üí Scraping Spider
```

## üìã CHEAT SHEET

### **Clases y herencia:**
```python
# ‚úÖ CORRECTO
from scraper_core.spiders.base.base_article import BaseArticleSpider
class MiSpider(BaseArticleSpider):

# ‚ùå INCORRECTO
class MiSpider(scrapy.Spider):
```

### **Items obligatorios:**
```python
# ‚úÖ CORRECTO
from scraper_core.items import ArticuloInItem
item = ArticuloInItem()
item['titular'] = '...'  # NO 'titulo'

# ‚ùå INCORRECTO
item = {}  # No usar dict
```

### **Pipelines requeridos:**
```python
'ITEM_PIPELINES': {
    'scraper_core.pipelines.validation.DataValidationPipeline': 100,
    'scraper_core.pipelines.cleaning.DataCleaningPipeline': 200,
    'scraper_core.pipelines.storage.SupabaseStoragePipeline': 300,
}
```

### **Deduplicaci√≥n con scrapy-crawl-once:**
```python
# En custom_settings
'CRAWL_ONCE_ENABLED': True,
'CRAWL_ONCE_PATH': '.scrapy/crawl_once/',
'CRAWL_ONCE_DEFAULT': False,

# En cada request
meta={'crawl_once': True}
```

## üîß CONFIGURACIONES POR ESTRATEGIA

### **RSS Spider:**
```python
# Configuraci√≥n
name = 'medio_seccion_rss'
download_delay = 2.0
items_limit = 50
frecuencia = '*/30 * * * *'  # 30 min

# M√©todos principales
def parse_feed(self, response)
def parse_article(self, response)
```

### **Scraping Spider:**
```python
# Configuraci√≥n
name = 'medio_seccion'
download_delay = 3.0
items_limit = 30
frecuencia = '0 * * * *'  # 1 hora

# M√©todos principales
def parse(self, response)
def parse_article(self, response)
```

### **Playwright Spider:**
```python
# Configuraci√≥n
name = 'medio_seccion_playwright'
download_delay = 5.0
items_limit = 20
frecuencia = '0 */2 * * *'  # 2 horas

# Meta Playwright
meta = {
    "playwright": True,
    "playwright_page_methods": [...],
    "crawl_once": True  # Tambi√©n en Playwright
}
```

## üìä CAMPOS CR√çTICOS

### **Siempre requeridos:**
```python
item['url'] = response.url
item['titular'] = '...'  # MIN 10 chars
item['medio'] = 'Nombre Medio'
item['pais_publicacion'] = 'Pa√≠s'
item['tipo_medio'] = 'diario'  # diario/agencia/revista
item['fecha_publicacion'] = datetime_or_string
item['contenido_texto'] = '...'  # MIN 100 chars
item['fuente'] = self.name
```

### **Con defaults:**
```python
item['autor'] = 'Redacci√≥n'  # Si no hay
item['idioma'] = 'es'
item['es_opinion'] = False
item['es_oficial'] = False
```

## üö® ERRORES COMUNES

| S√≠ntoma | Causa | Soluci√≥n |
|---------|-------|----------|
| `ImportError` | Path incorrecto | Verificar PYTHONPATH |
| `ValidationError` | Campos faltantes | Llenar campos requeridos |
| Sin deduplicaci√≥n | scrapy-crawl-once no configurado | Configurar `CRAWL_ONCE_*` |
| Sin almacenamiento | Sin Supabase env | Configurar `.env` |
| Contenido vac√≠o | Necesita JS | Usar Playwright |

## üìù COMANDOS √öTILES

```bash
# Probar spider
scrapy crawl nombre_spider -s CLOSESPIDER_ITEMCOUNT=3

# Ver items extra√≠dos
scrapy crawl nombre_spider -o test.json -s CLOSESPIDER_ITEMCOUNT=5

# Debug completo
scrapy crawl nombre_spider -L DEBUG

# Verificar sintaxis
python -m py_compile path/to/spider.py

# Ver base de datos scrapy-crawl-once
ls -la .scrapy/crawl_once/

# Limpiar deduplicaci√≥n si es necesario
rm .scrapy/crawl_once/nombre_spider.sqlite
```

## üîç VALIDACIONES R√ÅPIDAS

### **Filtro de secci√≥n:**
```python
# URLs v√°lidas para secci√≥n
‚úÖ /internacional/noticia-123
‚úÖ /internacional/2024/01/articulo

# URLs a excluir
‚ùå /otra-seccion/noticia
‚ùå /archivo/internacional
‚ùå /tags/internacional
```

### **Activar deduplicaci√≥n:**
```python
# En cada request de art√≠culo
yield scrapy.Request(
    url,
    callback=self.parse_article,
    meta={'crawl_once': True}  # IMPORTANTE
)
```

### **Contenido m√≠nimo:**
```python
# Validaci√≥n b√°sica
if len(item['titular']) < 10:
    return None
if len(item['contenido_texto']) < 100:
    return None
```

## üìÖ FRECUENCIAS EST√ÅNDAR

```bash
# RSS - Cada 30 minutos
*/30 * * * * scrapy crawl spider_rss

# Scraping - Cada hora
0 * * * * scrapy crawl spider_scraping

# Playwright - Cada 2 horas
0 */2 * * * scrapy crawl spider_playwright
```

## üéØ FLUJO COMPLETO RESUMIDO

1. **Recibir input** ‚Üí Validar campos
2. **Decidir estrategia** ‚Üí RSS/Scraping/Playwright
3. **Generar spider** ‚Üí Heredar de BaseArticleSpider
4. **Configurar** ‚Üí Pipelines, scrapy-crawl-once, l√≠mites
5. **Probar** ‚Üí 3-5 items
6. **Programar** ‚Üí Cron seg√∫n estrategia

## üí° TIPS FINALES

- **Siempre** heredar de `BaseArticleSpider`
- **Siempre** usar `ArticuloInItem`
- **Siempre** configurar scrapy-crawl-once
- **Siempre** activar `crawl_once` en requests
- **Siempre** filtrar por secci√≥n
- **Nunca** procesar > 50 items/ejecuci√≥n
- **Nunca** delay < 2 segundos

---

**Para proceso detallado ‚Üí** `MAIN_WORKFLOW.md`  
**Para generar c√≥digo ‚Üí** `CODE_GENERATION.md`  
**Para errores ‚Üí** `ERROR_HANDLING.md`
