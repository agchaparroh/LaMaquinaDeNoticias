# MANEJO DE ERRORES - La M√°quina de Noticias

## üéØ OBJETIVO
Proporcionar soluciones para errores comunes durante la generaci√≥n y ejecuci√≥n de spiders, manteniendo la consistencia con la arquitectura del proyecto.

## üö® ERRORES DURANTE LA GENERACI√ìN

### **1. Error: Falta informaci√≥n del usuario**
```python
# Problema
user_input = {
    'url_seccion': 'https://elpais.com/internacional',
    # Falta: medio, pais, tipo_medio
}

# Soluci√≥n
def validar_input_usuario(input_data):
    """Validar que tenemos toda la informaci√≥n necesaria."""
    required_fields = [
        'url_seccion',
        'nombre_medio',
        'pais_publicacion',
        'tipo_medio',
        'rss_disponible'
    ]
    
    missing = [f for f in required_fields if not input_data.get(f)]
    
    if missing:
        raise ValueError(f"Faltan campos requeridos: {missing}")
    
    # Validar tipo_medio
    valid_tipos = ['diario', 'agencia', 'revista', 'television', 'radio']
    if input_data['tipo_medio'] not in valid_tipos:
        raise ValueError(f"tipo_medio debe ser uno de: {valid_tipos}")
    
    return True
```

### **2. Error: URL de secci√≥n inv√°lida**
```python
# Problema: URLs que no son secciones
invalid_urls = [
    'https://elpais.com',  # Home, no secci√≥n
    'https://elpais.com/articulo-individual',  # Art√≠culo
    'https://elpais.com/tag/economia',  # Tag, no secci√≥n
]

# Soluci√≥n
def validar_url_seccion(url):
    """Verificar que es una URL de secci√≥n v√°lida."""
    parsed = urlparse(url)
    path_parts = parsed.path.strip('/').split('/')
    
    # Debe tener al menos un nivel de path
    if not path_parts or not path_parts[0]:
        raise ValueError("URL debe ser de una secci√≥n, no la home")
    
    # Excluir patrones no v√°lidos
    invalid_patterns = ['tag', 'autor', 'buscar', 'search', 'archivo']
    if any(p in path_parts[0] for p in invalid_patterns):
        raise ValueError(f"URL no parece ser una secci√≥n: {url}")
    
    return True
```

### **3. Error: An√°lisis con Firecrawl falla**
```python
# Problema: Firecrawl no puede acceder al sitio
try:
    result = firecrawl_scrape(url_seccion)
except Exception as e:
    # Manejo del error
    pass

# Soluci√≥n con fallbacks
async def analizar_con_fallbacks(url_seccion):
    """Intentar an√°lisis con m√∫ltiples estrategias."""
    
    # Intento 1: Firecrawl normal
    try:
        return await firecrawl_scrape(url_seccion)
    except Exception as e:
        logger.warning(f"Firecrawl fall√≥: {e}")
    
    # Intento 2: Firecrawl con opciones diferentes
    try:
        return await firecrawl_scrape(
            url_seccion,
            wait=5000,
            formats=['html', 'links']
        )
    except Exception as e:
        logger.warning(f"Segundo intento fall√≥: {e}")
    
    # Fallback: Generar con configuraci√≥n gen√©rica
    logger.info("Usando configuraci√≥n gen√©rica sin an√°lisis")
    return {
        'fallback': True,
        'content': '',
        'links': []
    }
```

## üêõ ERRORES DURANTE LA EJECUCI√ìN

### **1. Error: ImportError de m√≥dulos**
```python
# Error com√∫n
ImportError: cannot import name 'ArticuloInItem' from 'scraper_core.items'

# Soluciones:
# 1. Verificar estructura del proyecto
assert os.path.exists('scraper_core/items/articulo.py')
assert os.path.exists('scraper_core/items/__init__.py')

# 2. Verificar que __init__.py exporta la clase
# En scraper_core/items/__init__.py:
from .articulo import ArticuloInItem
__all__ = ['ArticuloInItem']

# 3. Verificar PYTHONPATH
import sys
project_root = '/path/to/LaMaquinaDeNoticias/src/module_scraper'
if project_root not in sys.path:
    sys.path.insert(0, project_root)
```

### **2. Error: Pipelines no se ejecutan**
```python
# Problema: Items no pasan por pipelines
# S√≠ntoma: No hay validaci√≥n ni almacenamiento

# Verificaci√≥n 1: Configuraci√≥n en settings
custom_settings = {
    'ITEM_PIPELINES': {
        'scraper_core.pipelines.validation.DataValidationPipeline': 100,
        'scraper_core.pipelines.cleaning.DataCleaningPipeline': 200,
        'scraper_core.pipelines.storage.SupabaseStoragePipeline': 300,
    }
}

# Verificaci√≥n 2: Items son del tipo correcto
def parse_article(self, response):
    # ‚úÖ Correcto
    item = ArticuloInItem()
    
    # ‚ùå Incorrecto
    item = {}  # Los pipelines esperan ArticuloInItem
    
    return item

# Verificaci√≥n 3: No hay excepciones en pipelines
# Revisar logs para errores como:
# - Campos faltantes en validaci√≥n
# - Errores de conexi√≥n a Supabase
```

### **3. Error: Campos requeridos faltantes**
```python
# Error: DataValidationPipeline rechaza items
ValidationError: Required field 'titular' missing

# Soluci√≥n: Asegurar que todos los campos se llenan
def create_complete_item(self, response):
    """Crear item con todos los campos requeridos."""
    item = ArticuloInItem()
    
    # Campos absolutamente requeridos
    item['url'] = response.url
    item['titular'] = self.extract_article_title(response) or 'Sin t√≠tulo'
    item['medio'] = self.medio_nombre
    item['pais_publicacion'] = self.pais
    item['tipo_medio'] = self.tipo_medio
    item['fecha_publicacion'] = self.extract_publication_date(response) or datetime.utcnow()
    item['contenido_texto'] = self.extract_article_content(response) or ''
    
    # Validar antes de retornar
    if not item['contenido_texto'] or len(item['contenido_texto']) < 100:
        self.logger.warning(f"Contenido insuficiente: {response.url}")
        return None
        
    return item
```

### **4. Error: Supabase connection failed**
```python
# Error: SupabaseStoragePipeline no puede conectar
SupabaseError: Invalid API key

# Verificaci√≥n 1: Variables de entorno
import os
required_vars = [
    'SUPABASE_URL',
    'SUPABASE_KEY', 
    'SUPABASE_SERVICE_ROLE_KEY'
]

for var in required_vars:
    if not os.getenv(var):
        print(f"ERROR: {var} no est√° configurada")

# Verificaci√≥n 2: Archivo .env cargado
# En scrapy.cfg o settings.py:
from dotenv import load_dotenv
load_dotenv('config/.env')  # o config/.env.test

# Verificaci√≥n 3: Permisos en Supabase
# - Verificar que la tabla 'articulos' existe
# - Verificar permisos RLS si est√°n habilitados
# - Verificar que el bucket 'articulos' existe
```

### **5. Error: Duplicados no se detectan**
```python
# Problema: El spider procesa los mismos art√≠culos repetidamente

# Soluci√≥n 1: Verificar JOBDIR configurado
custom_settings = {
    'DUPEFILTER_CLASS': 'scrapy.dupefilters.RFPDupeFilter',
    'JOBDIR': f'./crawl_state_{spider_name}',  # CR√çTICO
}

# Soluci√≥n 2: Verificar permisos de escritura
import os
jobdir = './crawl_state_test'
os.makedirs(jobdir, exist_ok=True)
test_file = os.path.join(jobdir, 'test.txt')
with open(test_file, 'w') as f:
    f.write('test')
os.remove(test_file)

# Soluci√≥n 3: Limpiar estado corrupto
def reset_spider_state(spider_name):
    """Resetear estado del spider si est√° corrupto."""
    import shutil
    jobdir = f'./crawl_state_{spider_name}'
    if os.path.exists(jobdir):
        shutil.rmtree(jobdir)
        logger.info(f"Estado reseteado para {spider_name}")
```

## üîß ERRORES DE EXTRACCI√ìN

### **1. Selectores no encuentran contenido**
```python
# Problema: Selectores retornan None o vac√≠o

# Soluci√≥n: Implementar fallbacks progresivos
def extract_with_fallbacks(self, response, field_type):
    """Extraer con m√∫ltiples estrategias."""
    
    # Estrategia 1: Selectores espec√≠ficos del medio
    if hasattr(self, f'{field_type}_selectors'):
        selectors = getattr(self, f'{field_type}_selectors')
        for selector in selectors:
            result = response.css(selector).get()
            if result and result.strip():
                return result.strip()
    
    # Estrategia 2: Usar m√©todos de BaseArticleSpider
    if field_type == 'title':
        return self.extract_article_title(response)
    elif field_type == 'content':
        return self.extract_article_content(response)
    elif field_type == 'date':
        return self.extract_publication_date(response)
    
    # Estrategia 3: Selectores gen√©ricos
    generic_selectors = {
        'title': ['h1::text', 'title::text'],
        'content': ['article ::text', 'main ::text'],
        'date': ['time::text', '.date::text']
    }
    
    for selector in generic_selectors.get(field_type, []):
        result = response.css(selector).get()
        if result and result.strip():
            return result.strip()
    
    return None
```

### **2. Contenido con JavaScript no se carga**
```python
# Error: P√°gina vac√≠a o incompleta

# Soluci√≥n 1: Verificar si Playwright est√° configurado
if 'scrapy_playwright' not in self.custom_settings.get('DOWNLOAD_HANDLERS', {}):
    # Configurar Playwright
    self.custom_settings['DOWNLOAD_HANDLERS'] = {
        "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
        "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
    }

# Soluci√≥n 2: Esperar elementos espec√≠ficos
yield scrapy.Request(
    url,
    meta={
        "playwright": True,
        "playwright_page_methods": [
            {"method": "wait_for_selector", 
             "args": [".article-content", {"timeout": 30000}]},
            {"method": "wait_for_load_state", "args": ["networkidle"]},
        ]
    }
)

# Soluci√≥n 3: Fallback a BaseArticleSpider con Playwright
if not content and self.settings.getbool('USE_PLAYWRIGHT_FOR_EMPTY_CONTENT'):
    # BaseArticleSpider maneja esto autom√°ticamente
    pass
```

## üìä TABLA DE ERRORES COMUNES

| Error | Causa | Soluci√≥n |
|-------|-------|----------|
| `ImportError: ArticuloInItem` | Path incorrecto | Verificar PYTHONPATH y estructura |
| `ValidationError: Required field` | Campo faltante | Llenar todos los campos requeridos |
| `SupabaseError: Connection` | Credenciales | Verificar variables de entorno |
| `No items scraped` | Selectores incorrectos | Usar fallbacks y m√©todos base |
| `Duplicates processed` | Sin JOBDIR | Configurar JOBDIR correctamente |
| `JavaScript content missing` | Sin Playwright | Configurar Playwright o usar template apropiado |

## üöë RECUPERACI√ìN DE ERRORES

### **Estrategia de recuperaci√≥n general:**
```python
class RobustSpider(BaseArticleSpider):
    """Spider con manejo robusto de errores."""
    
    def parse_article(self, response):
        try:
            # Intentar extracci√≥n normal
            item = self._extract_article(response)
            
            if not item:
                # Intentar con Playwright si est√° disponible
                if not response.meta.get('playwright_retry'):
                    return self._retry_with_playwright(response)
                    
            return item
            
        except Exception as e:
            self.logger.error(f"Error en {response.url}: {e}", exc_info=True)
            
            # Registrar error pero continuar
            self.crawler.stats.inc_value('articles_failed')
            
            # Opcional: Guardar para an√°lisis posterior
            self._save_failed_url(response.url, str(e))
            
            return None
    
    def _retry_with_playwright(self, response):
        """Reintentar con renderizado JavaScript."""
        return scrapy.Request(
            response.url,
            callback=self.parse_article,
            meta={
                'playwright': True,
                'playwright_retry': True
            },
            dont_filter=True
        )
    
    def _save_failed_url(self, url, error):
        """Guardar URLs fallidas para an√°lisis."""
        with open(f'failed_urls_{self.name}.txt', 'a') as f:
            f.write(f"{datetime.utcnow().isoformat()}\t{url}\t{error}\n")
```

## üìù LOGS Y DEBUGGING

### **Configurar logging detallado:**
```python
# En settings.py o custom_settings
LOG_LEVEL = 'DEBUG'
LOG_FILE = f'logs/{spider_name}.log'

LOGGERS = {
    'scraper_core.pipelines': 'DEBUG',
    'scraper_core.spiders': 'DEBUG',
    'scrapy.core.engine': 'INFO',
}
```

### **Mensajes de log √∫tiles:**
```python
# En el spider
self.logger.info(f"Iniciando extracci√≥n de {self.target_section}")
self.logger.debug(f"Selectores utilizados: {selectors}")
self.logger.warning(f"Sin contenido en {response.url}")
self.logger.error(f"Error cr√≠tico: {error}", exc_info=True)

# Estad√≠sticas
self.crawler.stats.set_value('custom/articles_by_section', {})
self.crawler.stats.inc_value(f'custom/articles_by_section/{section}')
```

---

**üìö Documentos relacionados:**
- `MAIN_WORKFLOW.md` - Proceso principal
- `CODE_GENERATION.md` - Generaci√≥n de c√≥digo
- `INTEGRATION_GUIDE.md` - Gu√≠a de integraci√≥n
