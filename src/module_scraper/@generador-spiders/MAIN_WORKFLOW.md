# FLUJO PRINCIPAL - Generaci√≥n de Spiders para La M√°quina de Noticias

## üéØ PROP√ìSITO
Generar **spiders especializados** que conviertan secciones de medios digitales en fuentes tipo RSS, completamente integrados con la arquitectura de La M√°quina de Noticias.

## üìã REQUISITOS PREVIOS

### **1. Configuraci√≥n de Supabase**
```env
# En config/.env.test o config/.env
SUPABASE_URL=https://tu-proyecto.supabase.co
SUPABASE_KEY=tu-anon-key
SUPABASE_SERVICE_ROLE_KEY=tu-service-role-key
```

### **2. Estructura del proyecto**
```
module_scraper/
‚îú‚îÄ‚îÄ .scrapy/
‚îÇ   ‚îî‚îÄ‚îÄ crawl_once/              # Directorio para scrapy-crawl-once
‚îú‚îÄ‚îÄ scraper_core/
‚îÇ   ‚îú‚îÄ‚îÄ items/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ articulo.py          # ArticuloInItem
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation.py        # DataValidationPipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaning.py          # DataCleaningPipeline
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ storage.py           # SupabaseStoragePipeline
‚îÇ   ‚îî‚îÄ‚îÄ spiders/
‚îÇ       ‚îî‚îÄ‚îÄ base/
‚îÇ           ‚îî‚îÄ‚îÄ base_article.py  # BaseArticleSpider
```

## üì• INPUT DEL USUARIO

### **Informaci√≥n requerida:**
```yaml
url_seccion: "https://elpais.com/internacional"
nombre_medio: "El Pa√≠s"
pais_publicacion: "Espa√±a"
tipo_medio: "diario"  # diario/agencia/revista
rss_disponible: "No"  # S√≠/No
url_rss: ""  # Solo si rss_disponible = S√≠
```

## üîÑ PROCESO PASO A PASO

### **PASO 1: An√°lisis Inicial**
```python
# Determinar estrategia basada en disponibilidad de RSS
if rss_disponible:
    estrategia = "rss_feed"
    analisis_requerido = False
else:
    # An√°lisis m√≠nimo con Firecrawl
    estrategia = determinar_estrategia_scraping(url_seccion)
    analisis_requerido = True
```

### **PASO 2: Configuraci√≥n Base del Spider**
```python
# Configuraci√≥n que todos los spiders deben tener
configuracion_base = {
    "hereda_de": "BaseArticleSpider",
    "items": "ArticuloInItem",
    "pipelines": [
        "DataValidationPipeline",
        "DataCleaningPipeline",
        "SupabaseStoragePipeline"
    ],
    "custom_settings": {
        'ROBOTSTXT_OBEY': True,
        'DOWNLOAD_DELAY': 3.0,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'CLOSESPIDER_ITEMCOUNT': 50,  # L√≠mite por ejecuci√≥n
        
        # Configuraci√≥n scrapy-crawl-once
        'CRAWL_ONCE_ENABLED': True,
        'CRAWL_ONCE_PATH': 'crawl_once/section_{nombre_seccion}',
        'CRAWL_ONCE_DEFAULT': False,
    }
}
```

### **PASO 3: An√°lisis de la Secci√≥n (si no hay RSS)**
```python
# Solo si no hay RSS disponible
if analisis_requerido:
    # 1. Obtener p√°gina de secci√≥n
    seccion_data = firecrawl_scrape(url_seccion)
    
    # 2. Detectar patr√≥n de URLs de art√≠culos
    patron_articulos = detectar_patron_urls(seccion_data)
    
    # 3. Verificar si necesita JavaScript
    requiere_js = verificar_contenido_dinamico(seccion_data)
    
    # 4. Obtener muestra de art√≠culo
    if patron_articulos:
        articulo_muestra = firecrawl_scrape(patron_articulos[0])
        selectores = extraer_selectores(articulo_muestra)
```

### **PASO 4: Generaci√≥n del Spider**

#### **Opci√≥n A: Spider con RSS**
```python
class ElpaisInternacionalRssSpider(BaseArticleSpider):
    name = 'elpais_internacional_rss'
    allowed_domains = ['elpais.com']
    
    # Configuraci√≥n RSS
    feed_url = 'https://elpais.com/rss/internacional'
    
    custom_settings = {
        **BaseArticleSpider.custom_settings,
        'CRAWL_ONCE_ENABLED': True,
        'CRAWL_ONCE_PATH': 'crawl_once/section_internacional_rss',
    }
    
    def parse_feed(self, response):
        """Parsear feed RSS"""
        # Los art√≠culos del RSS se marcan con crawl_once
        for entry in feed.entries:
            yield self.make_request(
                entry.link,
                self.parse_article,
                meta={
                    'crawl_once': True,  # Activar deduplicaci√≥n
                    'rss_data': entry_data
                }
            )
```

#### **Opci√≥n B: Spider Scraping**
```python
class ElpaisInternacionalSpider(BaseArticleSpider):
    name = 'elpais_internacional'
    allowed_domains = ['elpais.com']
    start_urls = ['https://elpais.com/internacional']
    
    # Patr√≥n para filtrar solo art√≠culos de la secci√≥n
    section_pattern = re.compile(r'/internacional/')
    
    custom_settings = {
        **BaseArticleSpider.custom_settings,
        'CRAWL_ONCE_ENABLED': True,
        'CRAWL_ONCE_PATH': 'crawl_once/section_internacional',
    }
    
    def parse(self, response):
        """Extraer lista de art√≠culos de la secci√≥n"""
        article_links = response.css('article a::attr(href)').getall()
        
        for link in article_links:
            if self.section_pattern.search(link):
                yield response.follow(
                    link, 
                    self.parse_article,
                    meta={'crawl_once': True}  # No reprocesar
                )
```

## üîß INTEGRACI√ìN CON EL PROYECTO

### **1. Estructura de Items**
```python
# El spider debe llenar TODOS los campos requeridos
item = ArticuloInItem()
item['url'] = response.url
item['titular'] = self.extract_article_title(response)
item['contenido_texto'] = self.extract_article_content(response)
item['contenido_html'] = response.text
item['medio'] = self.medio_nombre
item['pais_publicacion'] = self.pais
item['tipo_medio'] = self.tipo_medio
item['fecha_publicacion'] = self.extract_publication_date(response)
item['autor'] = self.extract_author(response)
item['idioma'] = 'es'
item['seccion'] = self.target_section
item['fuente'] = self.name
```

### **2. Configuraci√≥n de Pipelines**
```python
# En settings.py del spider
custom_settings = {
    **BaseArticleSpider.custom_settings,
    'ITEM_PIPELINES': {
        'scraper_core.pipelines.validation.DataValidationPipeline': 100,
        'scraper_core.pipelines.cleaning.DataCleaningPipeline': 200,
        'scraper_core.pipelines.storage.SupabaseStoragePipeline': 300,
    }
}
```

### **3. Sistema de Deduplicaci√≥n con scrapy-crawl-once**
```python
# Configuraci√≥n para evitar duplicados
custom_settings = {
    'CRAWL_ONCE_ENABLED': True,
    'CRAWL_ONCE_PATH': f'crawl_once/section_{self.target_section}',
    'CRAWL_ONCE_DEFAULT': False,  # Control expl√≠cito
}

# En los requests a art√≠culos
yield Request(
    url,
    callback=self.parse_article,
    meta={'crawl_once': True}  # Activar deduplicaci√≥n
)
```

## üìä CONFIGURACI√ìN POR TIPO DE ESTRATEGIA

### **RSS Feed Spider**
- **Frecuencia**: Cada 30 minutos
- **Items por ejecuci√≥n**: 50
- **An√°lisis requerido**: No
- **Complejidad**: Baja
- **Deduplicaci√≥n**: `crawl_once/section_{seccion}_rss`

### **HTML Scraping Spider**
- **Frecuencia**: Cada 60 minutos
- **Items por ejecuci√≥n**: 30
- **An√°lisis requerido**: 2-3 requests
- **Complejidad**: Media
- **Deduplicaci√≥n**: `crawl_once/section_{seccion}`

### **Playwright Spider**
- **Frecuencia**: Cada 120 minutos
- **Items por ejecuci√≥n**: 20
- **An√°lisis requerido**: 3-4 requests
- **Complejidad**: Alta
- **Deduplicaci√≥n**: `crawl_once/section_{seccion}_playwright`

## üö® VALIDACIONES CR√çTICAS

### **1. Filtrado por secci√≥n**
```python
def is_section_article(self, url):
    """Validar que el art√≠culo pertenece a la secci√≥n"""
    # Debe contener el path de la secci√≥n
    return self.section_pattern.search(url) is not None
```

### **2. Contenido v√°lido**
```python
def validate_article_data(self, article_data):
    """Validar antes de yield"""
    required = ['titular', 'url', 'contenido_texto']
    return all(article_data.get(field) for field in required)
```

### **3. L√≠mites de ejecuci√≥n**
```python
# Siempre configurar l√≠mites
'CLOSESPIDER_ITEMCOUNT': 50,  # M√°ximo items
'CLOSESPIDER_TIMEOUT': 1800,  # 30 minutos m√°ximo
'DEPTH_LIMIT': 2,  # No profundizar demasiado
```

## üìã CHECKLIST DE GENERACI√ìN

- [ ] Spider hereda de `BaseArticleSpider`
- [ ] Usa `ArticuloInItem` con todos los campos
- [ ] Configura los 3 pipelines del proyecto
- [ ] Implementa filtrado estricto por secci√≥n
- [ ] Configura scrapy-crawl-once correctamente
- [ ] Activa `crawl_once` en requests de art√≠culos
- [ ] Respeta robots.txt
- [ ] Implementa rate limiting
- [ ] Maneja errores con m√©todos de la clase base
- [ ] Incluye logging apropiado

## üéØ RESULTADO ESPERADO

### **Archivos generados:**
```
scraper_core/spiders/
‚îî‚îÄ‚îÄ elpais_internacional_spider.py  # Spider completo
```

### **Estructura de deduplicaci√≥n:**
```
.scrapy/
‚îî‚îÄ‚îÄ crawl_once/
    ‚îî‚îÄ‚îÄ section_internacional/      # Base de datos de URLs procesadas
```

### **Configuraci√≥n para cron:**
```bash
# RSS (cada 30 min)
*/30 * * * * cd /path/to/project && scrapy crawl elpais_internacional_rss

# Scraping (cada hora)
0 * * * * cd /path/to/project && scrapy crawl elpais_internacional
```

### **Documentaci√≥n incluida:**
- Docstring completo en el spider
- Configuraci√≥n espec√≠fica documentada
- Instrucciones de deployment

---

**üéØ OBJETIVO:** Spider funcional que convierte la secci√≥n en un feed RSS automatizado

**‚ö° PRINCIPIO:** Integraci√≥n total con la arquitectura existente

**üìö SIGUIENTE:** Consultar `CODE_GENERATION.md` para generar el c√≥digo final
