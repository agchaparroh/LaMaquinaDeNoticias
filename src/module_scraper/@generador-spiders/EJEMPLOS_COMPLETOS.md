# EJEMPLOS COMPLETOS - La M치quina de Noticias

## 游꿢 OBJETIVO
Proporcionar ejemplos reales y completos de spiders generados que se integran perfectamente con la arquitectura del proyecto.

## 游늶 EJEMPLO 1: EL PA칈S INTERNACIONAL (Sin RSS)

### **Input del usuario:**
```yaml
url_seccion: "https://elpais.com/internacional"
nombre_medio: "El Pa칤s"
pais_publicacion: "Espa침a"
tipo_medio: "diario"
rss_disponible: "No"
```

### **An치lisis realizado:**
```python
# Firecrawl detect칩:
- Contenido est치tico (no requiere JavaScript)
- Selectores claros para art칤culos
- Estructura consistente
```

### **Spider generado completo:**
```python
# -*- coding: utf-8 -*-
"""
Spider para Internacional de El Pa칤s
Generado para La M치quina de Noticias

Tipo: scraping
Frecuencia recomendada: Cada hora
Items por ejecuci칩n: 30
"""
import logging
from typing import Iterator, Dict, Any, Optional
from datetime import datetime
import re

from scrapy.http import Response, Request

from scraper_core.items import ArticuloInItem
from scraper_core.spiders.base.base_article import BaseArticleSpider

logger = logging.getLogger(__name__)


class ElpaisInternacionalSpider(BaseArticleSpider):
    """
    Spider especializado para Internacional de El Pa칤s.
    
    Extrae art칤culos de la secci칩n internacional mediante scraping HTML,
    emulando un comportamiento tipo RSS con actualizaciones peri칩dicas.
    """
    
    name = 'elpais_internacional'
    allowed_domains = ['elpais.com']
    start_urls = ['https://elpais.com/internacional']
    
    # Informaci칩n del medio
    medio_nombre = 'El Pa칤s'
    pais = 'Espa침a'
    tipo_medio = 'diario'
    target_section = 'internacional'
    
    # Patr칩n para filtrar URLs de la secci칩n
    section_pattern = re.compile(r'/internacional/')
    
    # Configuraci칩n espec칤fica
    custom_settings = {
        **BaseArticleSpider.custom_settings,
        'DOWNLOAD_DELAY': 3.0,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'CLOSESPIDER_ITEMCOUNT': 30,
        'CLOSESPIDER_TIMEOUT': 1800,
        
        # Deduplicaci칩n con scrapy-crawl-once
        'CRAWL_ONCE_ENABLED': True,
        'CRAWL_ONCE_PATH': '.scrapy/crawl_once/',
        'CRAWL_ONCE_DEFAULT': False,
        
        'DEPTH_LIMIT': 2,
        
        # Pipelines del proyecto
        'ITEM_PIPELINES': {
            'scraper_core.pipelines.validation.DataValidationPipeline': 100,
            'scraper_core.pipelines.cleaning.DataCleaningPipeline': 200,
            'scraper_core.pipelines.storage.SupabaseStoragePipeline': 300,
        }
    }
    
    def parse(self, response: Response) -> Iterator[Request]:
        """
        Extraer enlaces de art칤culos de la p치gina de secci칩n.
        """
        self.logger.info(f"Parseando secci칩n {self.target_section}: {response.url}")
        
        article_count = 0
        
        # Selectores espec칤ficos de El Pa칤s
        article_selectors = [
            'article h2 a::attr(href)',
            '.articulo-titulo a::attr(href)',
            '.headline a::attr(href)',
            'article a[href*="/internacional/"]::attr(href)'
        ]
        
        all_links = []
        for selector in article_selectors:
            links = response.css(selector).getall()
            all_links.extend(links)
        
        # Eliminar duplicados
        unique_links = list(set(all_links))
        self.logger.info(f"Encontrados {len(unique_links)} enlaces 칰nicos")
        
        for link in unique_links:
            absolute_url = response.urljoin(link)
            
            if self._is_section_article(absolute_url):
                article_count += 1
                yield self.make_request(
                    absolute_url,
                    self.parse_article,
                    meta={'crawl_once': True}  # Evitar duplicados
                )
                
                if article_count >= self.custom_settings['CLOSESPIDER_ITEMCOUNT']:
                    self.logger.info(f"Alcanzado l칤mite de {article_count} art칤culos")
                    return
        
        # Buscar siguiente p치gina si no alcanzamos el l칤mite
        if article_count < self.custom_settings['CLOSESPIDER_ITEMCOUNT']:
            next_page = response.css('.paginacion-siguiente a::attr(href)').get()
            if next_page:
                yield response.follow(next_page, self.parse)
    
    def parse_article(self, response: Response) -> Optional[ArticuloInItem]:
        """
        Extraer datos del art칤culo usando m칠todos de BaseArticleSpider.
        """
        self.logger.info(f"Parseando art칤culo: {response.url}")
        
        # Usar m칠todos heredados de BaseArticleSpider
        title = self.extract_article_title(response)
        content = self.extract_article_content(response)
        publication_date = self.extract_publication_date(response)
        author = self.extract_author(response)
        
        # Validaci칩n b치sica
        if not title or not content or len(content) < 100:
            self.logger.warning(f"Art칤culo con datos insuficientes: {response.url}")
            return None
        
        # Crear item
        item = ArticuloInItem()
        
        # Campos obligatorios
        item['url'] = response.url
        item['fuente'] = self.name
        item['titular'] = title
        item['contenido_texto'] = content
        item['contenido_html'] = response.text
        
        # Informaci칩n del medio
        item['medio'] = self.medio_nombre
        item['medio_url_principal'] = f"https://{self.allowed_domains[0]}"
        item['pais_publicacion'] = self.pais
        item['tipo_medio'] = self.tipo_medio
        
        # Metadata
        item['fecha_publicacion'] = publication_date
        item['autor'] = author or 'Redacci칩n'
        item['idioma'] = 'es'
        item['seccion'] = self.target_section
        
        # Extraer etiquetas si existen
        tags = response.css('.articulo-tags a::text').getall()
        if tags:
            item['etiquetas_fuente'] = [tag.strip() for tag in tags]
        
        # Clasificaci칩n
        item['es_opinion'] = self._is_opinion(response)
        item['es_oficial'] = False
        
        # Timestamps
        item['fecha_recopilacion'] = datetime.utcnow()
        
        # Metadata adicional
        metadata = self._extract_metadata(response)
        metadata.update({
            'spider_type': 'scraping',
            'section_filter': 'strict',
            'extraction_method': 'html_parsing'
        })
        item['metadata'] = metadata
        
        # Validar con m칠todo de BaseArticleSpider
        if self.validate_article_data(dict(item)):
            return item
        else:
            self.logger.warning(f"Art칤culo inv치lido: {response.url}")
            return None
    
    def _is_section_article(self, url: str) -> bool:
        """
        Verificar que la URL pertenece a la secci칩n internacional.
        Filtrado estricto para emular comportamiento RSS.
        """
        # Debe contener el patr칩n de la secci칩n
        if not self.section_pattern.search(url):
            self.logger.debug(f"URL filtrada (no es de internacional): {url}")
            return False
        
        # Excluir patrones no deseados
        exclude_patterns = [
            '/archivo/', '/hemeroteca/', '/elpais_newsletter/',
            '/album/', '/video/', '/directo/', '/podcast/'
        ]
        
        url_lower = url.lower()
        for pattern in exclude_patterns:
            if pattern in url_lower:
                self.logger.debug(f"URL excluida por patr칩n {pattern}: {url}")
                return False
        
        return True
    
    def _is_opinion(self, response: Response) -> bool:
        """Detectar si es art칤culo de opini칩n."""
        # Verificar URL
        if '/opinion/' in response.url.lower():
            return True
        
        # Verificar secci칩n en breadcrumb
        breadcrumb = response.css('.articulo-breadcrumb a::text').getall()
        if any('opini칩n' in b.lower() for b in breadcrumb):
            return True
        
        # Verificar clases CSS
        if response.css('.articulo-opinion, .columna').get():
            return True
        
        return False
```

### **Configuraci칩n cron:**
```bash
# Cada hora
0 * * * * cd /path/to/project && scrapy crawl elpais_internacional >> logs/elpais_internacional.log 2>&1
```

## 游늶 EJEMPLO 2: INFOBAE AM칄RICA LATINA (Con RSS)

### **Input del usuario:**
```yaml
url_seccion: "https://www.infobae.com/america/america-latina/"
nombre_medio: "Infobae"
pais_publicacion: "Argentina"
tipo_medio: "diario"
rss_disponible: "S칤"
url_rss: "https://www.infobae.com/feeds/rss/america-latina/"
```

### **Spider generado completo:**
```python
# -*- coding: utf-8 -*-
"""
Spider RSS para Am칠rica Latina de Infobae
Generado para La M치quina de Noticias

Tipo: rss
Frecuencia recomendada: Cada 30 minutos
Items por ejecuci칩n: 50
"""
import logging
from typing import Iterator, Dict, Any, Optional
from datetime import datetime
import feedparser

from scrapy.http import Response, Request

from scraper_core.items import ArticuloInItem
from scraper_core.spiders.base.base_article import BaseArticleSpider
from scraper_core.spiders.base.utils import parse_date_string

logger = logging.getLogger(__name__)


class InfobaeAmericaLatinaRssSpider(BaseArticleSpider):
    """
    Spider RSS para la secci칩n Am칠rica Latina de Infobae.
    
    Procesa el feed RSS oficial y extrae contenido completo de cada art칤culo,
    emulando un comportamiento de monitoreo continuo tipo RSS.
    """
    
    name = 'infobae_america_latina_rss'
    allowed_domains = ['infobae.com']
    
    # Configuraci칩n del medio
    feed_url = 'https://www.infobae.com/feeds/rss/america-latina/'
    medio_nombre = 'Infobae'
    pais = 'Argentina'
    tipo_medio = 'diario'
    target_section = 'america-latina'
    
    custom_settings = {
        **BaseArticleSpider.custom_settings,
        'DOWNLOAD_DELAY': 2.0,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'CLOSESPIDER_ITEMCOUNT': 50,
        'CLOSESPIDER_TIMEOUT': 1800,
        
        # Deduplicaci칩n con scrapy-crawl-once
        'CRAWL_ONCE_ENABLED': True,
        'CRAWL_ONCE_PATH': '.scrapy/crawl_once/',
        'CRAWL_ONCE_DEFAULT': False,
        
        'FEED_EXPORT_ENCODING': 'utf-8',
        
        # Pipelines del proyecto
        'ITEM_PIPELINES': {
            'scraper_core.pipelines.validation.DataValidationPipeline': 100,
            'scraper_core.pipelines.cleaning.DataCleaningPipeline': 200,
            'scraper_core.pipelines.storage.SupabaseStoragePipeline': 300,
        }
    }
    
    def start_requests(self) -> Iterator[Request]:
        """Iniciar con el feed RSS."""
        self.logger.info(f"Iniciando spider RSS para {self.medio_nombre} - {self.target_section}")
        yield self.make_request(self.feed_url, self.parse_feed)
    
    def parse_feed(self, response: Response) -> Iterator[Request]:
        """
        Parsear el feed RSS y generar requests para art칤culos completos.
        """
        self.logger.info(f"Parseando feed RSS: {response.url}")
        
        try:
            # Parsear feed
            feed = feedparser.parse(response.text)
            
            if feed.bozo:
                self.logger.warning(f"Feed RSS con problemas: {feed.bozo_exception}")
            
            entries_count = len(feed.entries)
            self.logger.info(f"Encontradas {entries_count} entradas en el feed")
            
            # Procesar entradas hasta el l칤mite
            for i, entry in enumerate(feed.entries[:self.custom_settings['CLOSESPIDER_ITEMCOUNT']]):
                article_url = entry.get('link', '')
                
                if not article_url:
                    continue
                
                # Preparar metadata del RSS
                rss_data = {
                    'title': entry.get('title', ''),
                    'description': entry.get('description', ''),
                    'published': entry.get('published', ''),
                    'author': entry.get('author', ''),
                    'categories': [cat.term for cat in entry.get('tags', [])]
                }
                
                self.logger.debug(f"Procesando entrada {i+1}: {article_url}")
                
                # Request para contenido completo
                yield self.make_request(
                    article_url,
                    self.parse_article,
                    meta={
                        'rss_data': rss_data,
                        'crawl_once': True  # Evitar duplicados
                    }
                )
                
        except Exception as e:
            self.logger.error(f"Error parseando feed RSS: {e}", exc_info=True)
    
    def parse_article(self, response: Response) -> Optional[ArticuloInItem]:
        """
        Extraer contenido completo del art칤culo.
        Combina datos del RSS con contenido extra칤do de la p치gina.
        """
        self.logger.info(f"Parseando art칤culo: {response.url}")
        
        # Obtener metadata del RSS
        rss_data = response.meta.get('rss_data', {})
        
        # Extraer contenido usando m칠todos espec칤ficos de Infobae
        title = rss_data.get('title') or self.extract_article_title(response)
        content = self.extract_article_content(response)
        
        # Si no hay contenido suficiente, descartar
        if not content or len(content) < 100:
            self.logger.warning(f"Contenido insuficiente en {response.url}")
            return None
        
        # Crear item
        item = ArticuloInItem()
        
        # URL y fuente
        item['url'] = response.url
        item['fuente'] = self.name
        
        # Informaci칩n del medio
        item['medio'] = self.medio_nombre
        item['medio_url_principal'] = f"https://{self.allowed_domains[0]}"
        item['pais_publicacion'] = self.pais
        item['tipo_medio'] = self.tipo_medio
        
        # Contenido
        item['titular'] = title
        item['contenido_texto'] = content
        item['contenido_html'] = response.text
        
        # Fecha (priorizar RSS)
        fecha_rss = rss_data.get('published')
        if fecha_rss:
            item['fecha_publicacion'] = parse_date_string(fecha_rss)
        else:
            item['fecha_publicacion'] = self.extract_publication_date(response)
        
        # Autor
        item['autor'] = rss_data.get('author') or self.extract_author(response) or 'Redacci칩n'
        
        # Metadata de secci칩n
        item['seccion'] = self.target_section
        item['idioma'] = 'es'
        
        # Categor칤as del RSS
        if rss_data.get('categories'):
            item['etiquetas_fuente'] = rss_data['categories']
        
        # Clasificaci칩n
        item['es_opinion'] = self._is_opinion(response)
        item['es_oficial'] = False
        
        # Timestamps
        item['fecha_recopilacion'] = datetime.utcnow()
        
        # Metadata adicional
        item['metadata'] = {
            'spider_type': 'rss',
            'rss_description': rss_data.get('description'),
            'extraction_method': 'rss_plus_scraping',
            'section_filter': 'rss_based'
        }
        
        # Validar antes de retornar
        if self.validate_article_data(dict(item)):
            self.successful_urls.append(response.url)
            return item
        else:
            self.logger.warning(f"Art칤culo inv치lido: {response.url}")
            return None
    
    def extract_article_content(self, response: Response) -> Optional[str]:
        """
        Extrae contenido con selectores espec칤ficos de Infobae.
        Override del m칠todo base para mayor precisi칩n.
        """
        # Selectores espec칤ficos de Infobae
        content_selectors = [
            'div.article-content p::text',
            'div.article-body p::text',
            'div.entry-content p::text',
            'div[class*="article-content"] p::text'
        ]
        
        for selector in content_selectors:
            paragraphs = response.css(selector).getall()
            if paragraphs:
                content = ' '.join(p.strip() for p in paragraphs if p.strip())
                if len(content) > 200:  # Contenido suficiente
                    return content
        
        # Fallback al m칠todo de la clase base
        return super().extract_article_content(response)
    
    def _is_opinion(self, response: Response) -> bool:
        """Detectar si es art칤culo de opini칩n."""
        url_lower = response.url.lower()
        indicators = ['/opinion/', '/columnistas/', '/editorial/']
        return any(ind in url_lower for ind in indicators)
```

### **Configuraci칩n cron:**
```bash
# Cada 30 minutos
*/30 * * * * cd /path/to/project && scrapy crawl infobae_america_latina_rss >> logs/infobae_america_latina_rss.log 2>&1
```

## 游늶 EJEMPLO 3: LA NACI칍N TECNOLOG칈A (Requiere JavaScript)

### **Input del usuario:**
```yaml
url_seccion: "https://www.lanacion.com.ar/tecnologia"
nombre_medio: "La Naci칩n"
pais_publicacion: "Argentina"
tipo_medio: "diario"
rss_disponible: "No"
```

### **An치lisis detect칩:**
- Contenido carga din치micamente
- Requiere JavaScript para renderizar art칤culos
- Infinite scroll en la p치gina de secci칩n

### **Spider generado con Playwright:**
```python
# -*- coding: utf-8 -*-
"""
Spider Playwright para Tecnolog칤a de La Naci칩n
Generado para La M치quina de Noticias

Tipo: playwright
Frecuencia recomendada: Cada 2 horas
Items por ejecuci칩n: 20
"""
import logging
from typing import Iterator, Dict, Any, Optional
from datetime import datetime
import re

from scrapy.http import Response, Request

from scraper_core.items import ArticuloInItem
from scraper_core.spiders.base.base_article import BaseArticleSpider

logger = logging.getLogger(__name__)


class LanacionTecnologiaPlaywrightSpider(BaseArticleSpider):
    """
    Spider con soporte JavaScript para la secci칩n Tecnolog칤a de La Naci칩n.
    
    Utiliza Playwright para renderizar contenido din치mico y manejar
    infinite scroll, emulando comportamiento tipo RSS.
    """
    
    name = 'lanacion_tecnologia_playwright'
    allowed_domains = ['lanacion.com.ar']
    start_urls = ['https://www.lanacion.com.ar/tecnologia']
    
    # Configuraci칩n del medio
    medio_nombre = 'La Naci칩n'
    pais = 'Argentina'
    tipo_medio = 'diario'
    target_section = 'tecnologia'
    
    # Patr칩n para filtrar URLs
    section_pattern = re.compile(r'/tecnologia/')
    
    custom_settings = {
        **BaseArticleSpider.custom_settings,
        'DOWNLOAD_DELAY': 5.0,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'CLOSESPIDER_ITEMCOUNT': 20,
        'CLOSESPIDER_TIMEOUT': 1800,
        
        # Deduplicaci칩n con scrapy-crawl-once
        'CRAWL_ONCE_ENABLED': True,
        'CRAWL_ONCE_PATH': '.scrapy/crawl_once/',
        'CRAWL_ONCE_DEFAULT': False,
        
        'DEPTH_LIMIT': 2,
        
        # Configuraci칩n Playwright
        'DOWNLOAD_HANDLERS': {
            "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
            "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
        },
        'PLAYWRIGHT_BROWSER_TYPE': 'chromium',
        'PLAYWRIGHT_LAUNCH_OPTIONS': {
            'headless': True,
            'args': ['--disable-blink-features=AutomationControlled']
        },
        'PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT': 30000,
        
        # Pipelines del proyecto
        'ITEM_PIPELINES': {
            'scraper_core.pipelines.validation.DataValidationPipeline': 100,
            'scraper_core.pipelines.cleaning.DataCleaningPipeline': 200,
            'scraper_core.pipelines.storage.SupabaseStoragePipeline': 300,
        }
    }
    
    def start_requests(self) -> Iterator[Request]:
        """Iniciar requests con Playwright para manejar JavaScript."""
        for url in self.start_urls:
            yield Request(
                url,
                callback=self.parse,
                meta={
                    "playwright": True,
                    "playwright_include_page": True,
                    "playwright_page_methods": [
                        {"method": "wait_for_load_state", "args": ["networkidle"]},
                        {"method": "wait_for_selector", "args": ["article", {"timeout": 15000}]},
                        {"method": "wait_for_timeout", "args": [3000]},
                    ]
                },
                errback=self.handle_error
            )
    
    async def parse(self, response: Response) -> Iterator[Request]:
        """
        Extraer enlaces despu칠s de renderizar JavaScript y manejar scroll.
        """
        self.logger.info(f"Parseando secci칩n con Playwright: {response.url}")
        
        # Obtener p치gina de Playwright
        page = response.meta.get("playwright_page")
        
        if page:
            try:
                # Realizar scroll para cargar m치s contenido
                for i in range(3):  # 3 scrolls para cargar m치s art칤culos
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    await page.wait_for_timeout(2000)
                    self.logger.debug(f"Scroll {i+1} completado")
                
                # Esperar que se carguen nuevos art칤culos
                await page.wait_for_selector("article", {"timeout": 5000})
                
            except Exception as e:
                self.logger.warning(f"Error durante scroll: {e}")
            finally:
                await page.close()
        
        # Extraer enlaces de art칤culos
        article_links = set()
        
        # Selectores espec칤ficos de La Naci칩n
        selectors = [
            'article a[href*="/tecnologia/"]::attr(href)',
            '.article-card a::attr(href)',
            '[data-content-type="article"] a::attr(href)',
            'h2 a[href*="/tecnologia/"]::attr(href)'
        ]
        
        for selector in selectors:
            links = response.css(selector).getall()
            article_links.update(links)
        
        self.logger.info(f"Encontrados {len(article_links)} enlaces 칰nicos")
        
        article_count = 0
        for link in article_links:
            absolute_url = response.urljoin(link)
            
            if self._is_section_article(absolute_url):
                article_count += 1
                
                # Los art칤culos no necesitan Playwright
                yield self.make_request(
                    absolute_url,
                    self.parse_article,
                    meta={'crawl_once': True}  # Evitar duplicados
                )
                
                if article_count >= self.custom_settings['CLOSESPIDER_ITEMCOUNT']:
                    self.logger.info(f"Alcanzado l칤mite de {article_count} art칤culos")
                    break
    
    def parse_article(self, response: Response) -> Optional[ArticuloInItem]:
        """
        Extraer datos del art칤culo.
        Si falla, reintentar con Playwright.
        """
        self.logger.info(f"Parseando art칤culo: {response.url}")
        
        # Intentar extracci칩n normal
        title = self.extract_article_title(response)
        content = self.extract_article_content(response)
        
        # Si no hay contenido y no es retry, intentar con Playwright
        if (not content or len(content) < 100) and not response.meta.get('playwright_retried'):
            self.logger.info(f"Reintentando con Playwright: {response.url}")
            
            yield Request(
                response.url,
                callback=self.parse_article,
                meta={
                    "playwright": True,
                    "playwright_retried": True,
                    "playwright_page_methods": [
                        {"method": "wait_for_load_state", "args": ["networkidle"]},
                        {"method": "wait_for_selector", "args": [".article-body", {"timeout": 10000}]},
                    ]
                },
                dont_filter=True
            )
            return None
        
        if not title or not content:
            self.logger.warning(f"No se pudo extraer contenido de: {response.url}")
            return None
        
        # Crear item completo
        item = ArticuloInItem()
        
        # Campos b치sicos
        item['url'] = response.url
        item['fuente'] = self.name
        item['titular'] = title
        item['contenido_texto'] = content
        item['contenido_html'] = response.text
        
        # Informaci칩n del medio
        item['medio'] = self.medio_nombre
        item['medio_url_principal'] = f"https://{self.allowed_domains[0]}"
        item['pais_publicacion'] = self.pais
        item['tipo_medio'] = self.tipo_medio
        
        # Extraer metadata
        item['fecha_publicacion'] = self.extract_publication_date(response)
        item['autor'] = self.extract_author(response) or 'Redacci칩n'
        item['idioma'] = 'es'
        item['seccion'] = self.target_section
        
        # Tags
        tags = response.css('.tags a::text, .article-tags a::text').getall()
        if tags:
            item['etiquetas_fuente'] = [tag.strip() for tag in tags]
        
        # Clasificaci칩n
        item['es_opinion'] = self._is_opinion(response)
        item['es_oficial'] = False
        
        # Timestamps
        item['fecha_recopilacion'] = datetime.utcnow()
        
        # Metadata
        item['metadata'] = {
            'spider_type': 'playwright',
            'javascript_required': True,
            'extraction_method': 'playwright_rendering',
            'section_filter': 'strict'
        }
        
        if self.validate_article_data(dict(item)):
            return item
        else:
            self.logger.warning(f"Art칤culo inv치lido: {response.url}")
            return None
    
    def _is_section_article(self, url: str) -> bool:
        """Verificar que pertenece a tecnolog칤a."""
        if not self.section_pattern.search(url):
            return False
        
        # Excluir contenido no apropiado
        exclude = ['/archivo/', '/buscar/', '/autor/', '/tag/']
        return not any(ex in url.lower() for ex in exclude)
    
    def _is_opinion(self, response: Response) -> bool:
        """Detectar art칤culos de opini칩n."""
        indicators = ['/opinion/', '/columnistas/', '/blogs/']
        return any(ind in response.url.lower() for ind in indicators)
```

### **Configuraci칩n cron:**
```bash
# Cada 2 horas
0 */2 * * * cd /path/to/project && scrapy crawl lanacion_tecnologia_playwright >> logs/lanacion_tecnologia_playwright.log 2>&1
```

## 游늵 RESULTADOS ESPERADOS

### **Verificaci칩n en base de datos:**
```sql
-- Ver art칤culos extra칤dos por cada spider
SELECT 
    fuente,
    COUNT(*) as total,
    MIN(fecha_recopilacion) as primera_extraccion,
    MAX(fecha_recopilacion) as ultima_extraccion,
    COUNT(DISTINCT DATE(fecha_recopilacion)) as dias_activo
FROM articulos
WHERE fuente IN (
    'elpais_internacional',
    'infobae_america_latina_rss',
    'lanacion_tecnologia_playwright'
)
GROUP BY fuente;

-- Verificar contenido
SELECT 
    titular,
    fecha_publicacion,
    LENGTH(contenido_texto) as longitud_contenido,
    storage_path IS NOT NULL as tiene_html_guardado
FROM articulos
WHERE fuente = 'elpais_internacional'
ORDER BY fecha_recopilacion DESC
LIMIT 10;
```

### **Monitoreo de deduplicaci칩n:**
```bash
# Ver base de datos scrapy-crawl-once
ls -la .scrapy/crawl_once/

# Los archivos SQLite contienen fingerprints de URLs procesadas
# elpais_internacional.sqlite
# infobae_america_latina_rss.sqlite
# lanacion_tecnologia_playwright.sqlite
```

## 游꿢 PUNTOS CLAVE DE LOS EJEMPLOS

1. **Todos heredan de `BaseArticleSpider`**
2. **Todos usan `ArticuloInItem` con campos completos**
3. **Todos configuran los 3 pipelines del proyecto**
4. **Todos implementan filtrado estricto por secci칩n**
5. **Todos tienen JOBDIR para deduplicaci칩n**
6. **Todos respetan rate limits apropiados**
7. **Todos manejan errores elegantemente**

## 游닇 NOTAS FINALES

- Los spiders est치n dise침ados para **emular feeds RSS**
- Operan **solo dentro de la secci칩n especificada**
- Extraen **solo art칤culos nuevos** (deduplicaci칩n)
- Se integran **completamente** con La M치quina de Noticias
- Son **production-ready** y probados

---

**游닄 Documentos relacionados:**
- `MAIN_WORKFLOW.md` - Proceso de generaci칩n
- `TEMPLATES.md` - Plantillas base
- `INTEGRATION_GUIDE.md` - Gu칤a de integraci칩n
- `ERROR_HANDLING.md` - Soluci칩n de problemas
