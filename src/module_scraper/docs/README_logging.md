# Sistema de Logging - La M√°quina de Noticias

## üéØ Objetivo

Proporcionar un sistema de logging robusto y escalable para el m√≥dulo scraper que facilite el debugging en desarrollo y el monitoreo en producci√≥n.

## üöÄ Inicio R√°pido

### Configuraci√≥n B√°sica

1. **Variables de Ambiente** (opcional):
```bash
# En tu archivo .env o config/.env
ENVIRONMENT=development  # o staging, production
LOG_LEVEL=DEBUG         # Override del nivel por defecto
LOG_TO_FILE=true        # Habilitar logs a archivo en development
```

2. **Uso en tu Spider**:
```python
from scraper_core.spiders.base import BaseArticleSpider

class MySpider(BaseArticleSpider):
    name = 'my_spider'
    
    def parse(self, response):
        self.logger.info(f"Parsing {response.url}")
        # El logger ya est√° configurado y listo
```

### Verificar la Instalaci√≥n

```bash
# Ejecutar el script de verificaci√≥n
python scripts/verify_logging.py

# Ver el spider de ejemplo
scrapy crawl logging_example -L DEBUG
```

## üìÅ Estructura de Archivos

```
scraper_core/
‚îú‚îÄ‚îÄ logging_config.py      # Configuraci√≥n centralizada
‚îú‚îÄ‚îÄ log_rotation.py        # Manejo de rotaci√≥n de logs
‚îú‚îÄ‚îÄ extensions/
‚îÇ   ‚îî‚îÄ‚îÄ log_rotation.py    # Extensi√≥n de Scrapy para rotaci√≥n
‚îî‚îÄ‚îÄ utils/
    ‚îî‚îÄ‚îÄ logging_utils.py   # Utilidades y decoradores

logs/                      # Directorio de logs (creado autom√°ticamente)
‚îú‚îÄ‚îÄ development/
‚îú‚îÄ‚îÄ staging/
‚îî‚îÄ‚îÄ production/
```

## üîß Caracter√≠sticas

### 1. Configuraci√≥n por Ambiente

| Ambiente | Log Level | Formato | Rotaci√≥n | A Archivo |
|----------|-----------|---------|----------|-----------|
| development | DEBUG | Detallado | Por tama√±o (5MB) | Opcional |
| staging | INFO | Est√°ndar | Diaria (7 d√≠as) | S√≠ |
| production | WARNING | M√≠nimo | Diaria (30 d√≠as) | S√≠ |

### 2. Decoradores √ötiles

#### `@log_execution_time`
```python
@log_execution_time
def slow_operation(self):
    # Se loggea autom√°ticamente si toma > 1 segundo
    time.sleep(2)
```

#### `@log_exceptions`
```python
@log_exceptions(include_traceback=True)
def risky_operation(self):
    # Las excepciones se loggean antes de propagarse
    raise ValueError("Algo sali√≥ mal")
```

#### `@log_item_processing`
```python
class MyPipeline:
    @log_item_processing
    def process_item(self, item, spider):
        # Loggea entrada/salida del item autom√°ticamente
        return item
```

### 3. Logging Estructurado

Para sistemas de agregaci√≥n de logs:

```python
from scraper_core.utils.logging_utils import StructuredLogger

logger = StructuredLogger('my_module')
logger.info('user_action', 
           user_id=123, 
           action='parse_article',
           duration_ms=150)
# Output: {"event": "user_action", "user_id": 123, ...}
```

### 4. Sanitizaci√≥n Autom√°tica

```python
from scraper_core.utils.logging_utils import sanitize_log_data

data = {
    'url': 'https://example.com',
    'api_key': 'secret123',  # Ser√° ocultado
    'password': 'mypass'     # Ser√° ocultado
}

safe_data = sanitize_log_data(data)
self.logger.info(f"Data: {safe_data}")
# Output: Data: {'url': 'https://example.com', 'api_key': '***REDACTED***', ...}
```

## üìä Monitoreo y An√°lisis

### Ver Logs en Tiempo Real

```bash
# Todos los logs
tail -f logs/development/scrapy_*.log

# Solo errores
tail -f logs/development/scrapy_*.log | grep -E "ERROR|CRITICAL"

# Logs de un spider espec√≠fico
tail -f logs/development/spiders/infobae_*.log
```

### Analizar Logs Estructurados

```bash
# Contar eventos por tipo
grep '"event":' logs/production/scrapy_*.log | \
  jq -r '.event' | sort | uniq -c

# Ver todos los errores de hoy
grep '"level": "ERROR"' logs/production/scrapy_$(date +%Y-%m-%d).log | \
  jq '.'
```

### M√©tricas de Performance

```bash
# Tiempos de procesamiento promedio
grep '"event": "item_processed"' logs/production/scrapy_*.log | \
  jq '.processing_time_ms' | \
  awk '{sum+=$1; count++} END {print "Promedio:", sum/count, "ms"}'
```

## üõ†Ô∏è Personalizaci√≥n

### Cambiar Nivel de Log Temporalmente

```bash
# Por comando
scrapy crawl my_spider -L WARNING

# Por variable de ambiente
LOG_LEVEL=WARNING scrapy crawl my_spider
```

### Configurar Componentes Espec√≠ficos

En `settings.py`:

```python
LOGGERS = {
    'scrapy.downloadermiddlewares': 'WARNING',  # Menos verbose
    'scraper_core.pipelines': 'DEBUG',          # M√°s verbose
    'my_custom_module': 'INFO',                 # Personalizado
}
```

### Habilitar Logs por Spider

```python
LOG_ROTATION_PER_SPIDER = True  # Un archivo por spider
```

## üêõ Soluci√≥n de Problemas

### No se crean archivos de log

1. Verifica permisos en el directorio del proyecto
2. En development, aseg√∫rate que `LOG_TO_FILE=true`
3. Revisa que la extensi√≥n est√© habilitada en `EXTENSIONS`

### Logs muy verbosos

1. Ajusta `LOG_LEVEL` a WARNING o ERROR
2. Configura componentes espec√≠ficos en `LOGGERS`
3. Desactiva `AUTOTHROTTLE_DEBUG` si est√° habilitado

### Espacio en disco

1. Revisa la configuraci√≥n de rotaci√≥n
2. Ajusta `backup_count` para menos archivos hist√≥ricos
3. Configura logrotate en sistemas Linux

## üìö Referencias

- [Ejemplo completo](../examples/logging_example_spider.py)
- [Gu√≠a detallada](../docs/logging_guide.md)
- [Tests](../tests/test_logging/)

## ‚úÖ Checklist de Implementaci√≥n

- [x] Configuraci√≥n centralizada por ambiente
- [x] Rotaci√≥n autom√°tica de logs
- [x] Decoradores para logging consistente
- [x] Sanitizaci√≥n de datos sensibles
- [x] Logging estructurado para an√°lisis
- [x] Integraci√≥n con Scrapy
- [x] Tests unitarios
- [x] Documentaci√≥n completa
- [x] Script de verificaci√≥n
- [x] Ejemplos de uso

---

üí° **Tip**: Ejecuta `python scripts/verify_logging.py` para verificar que todo est√© configurado correctamente.
