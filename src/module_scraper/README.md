# Module Scraper - La MÃ¡quina de Noticias

Este mÃ³dulo es responsable de la recopilaciÃ³n automÃ¡tica de contenido periodÃ­stico de fuentes web predefinidas utilizando el framework Scrapy.

## ğŸš€ Quick Start

```bash
# 1. Configurar el entorno
cp config/.env.test.example config/.env.test
# Editar config/.env.test con tus credenciales

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Ejecutar un spider de prueba
scrapy crawl infobae

# 4. Ejecutar tests
pytest tests/
```

## ğŸ“ Estructura del Proyecto

La estructura ha sido reorganizada para mayor claridad y mantenibilidad:

```
module_scraper/
â”œâ”€â”€ .dev/                    # Configuraciones de herramientas de desarrollo
â”œâ”€â”€ config/                  # Configuraciones del proyecto y entorno
â”œâ”€â”€ docs/                    # DocumentaciÃ³n organizada
â”‚   â”œâ”€â”€ architecture/        # DocumentaciÃ³n tÃ©cnica y arquitectural
â”‚   â””â”€â”€ development/         # GuÃ­as para desarrolladores
â”œâ”€â”€ examples/                # Ejemplos de uso y plantillas
â”œâ”€â”€ scraper_core/           # CÃ³digo principal del scraper
â”‚   â”œâ”€â”€ items/              # Definiciones de items de datos
â”‚   â”œâ”€â”€ pipelines/          # Pipelines de procesamiento de datos
â”‚   â”œâ”€â”€ spiders/            # Spiders para extracciÃ³n de contenido
â”‚   â”‚   â””â”€â”€ base/           # Clases base reutilizables
â”‚   â”œâ”€â”€ utils/              # Utilidades compartidas
â”‚   â””â”€â”€ middlewares/        # Middlewares personalizados
â”œâ”€â”€ scripts/                # Scripts de utilidad
â”œâ”€â”€ tests/                  # Todo lo relacionado con testing y calidad
â”‚   â”œâ”€â”€ unit/               # Tests unitarios
â”‚   â”œâ”€â”€ integration/        # Tests de integraciÃ³n
â”‚   â”œâ”€â”€ e2e/                # Tests end-to-end
â”‚   â”œâ”€â”€ performance/        # Tests de rendimiento
â”‚   â””â”€â”€ fixtures/           # Datos de prueba
â””â”€â”€ STRUCTURE.md            # DocumentaciÃ³n detallada de la estructura
```

Ver [STRUCTURE.md](STRUCTURE.md) para documentaciÃ³n completa de la organizaciÃ³n.

## ğŸ—ï¸ Arquitectura

El sistema utiliza una arquitectura de pipelines que procesa los datos en etapas secuenciales:

```
ExtracciÃ³n â†’ ValidaciÃ³n â†’ Limpieza â†’ Almacenamiento
    â†“            â†“           â†“           â†“
 Spiders   DataValidation DataCleaning SupabaseStorage
```

### Componentes Principales

1. **Spiders** - Extraen datos de fuentes web especÃ­ficas
2. **Items & ItemLoaders** - Definen estructura de datos y procesamiento
3. **Pipelines** - Procesan, validan y limpian los datos
4. **Storage** - Almacenan datos en Supabase (PostgreSQL + Storage)

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

Las configuraciones se centralizan en `config/`:

```bash
# Copiar plantilla de configuraciÃ³n
cp config/.env.test.example config/.env.test

# Editar con tus credenciales (NO usar producciÃ³n para tests)
```

Variables principales:
```env
SUPABASE_URL=https://tu-proyecto-test.supabase.co
SUPABASE_KEY=tu-anon-key
SUPABASE_SERVICE_ROLE_KEY=tu-service-role-key
LOG_LEVEL=INFO
```

Ver [config/README.md](config/README.md) para guÃ­a completa de configuraciÃ³n.

### Settings de Scrapy

ConfiguraciÃ³n principal en `scraper_core/settings.py`:

```python
# Orden de pipelines (prioridad por nÃºmero)
ITEM_PIPELINES = {
    "scraper_core.pipelines.validation.DataValidationPipeline": 100,
    "scraper_core.pipelines.cleaning.DataCleaningPipeline": 200,
    "scraper_core.pipelines.SupabaseStoragePipeline": 300,
}
```

## ğŸ•·ï¸ Desarrollo de Spiders

### Crear un Nuevo Spider

1. **Usar clases base**: Hereda de `BaseArticleSpider`, `BaseSitemapSpider`, o `BaseCrawlSpider`
2. **Definir selectores**: Configura extractores especÃ­ficos del sitio
3. **Implementar parsing**: Override mÃ©todos segÃºn tus necesidades

```python
from scraper_core.spiders.base import BaseArticleSpider
from scraper_core.items import ArticuloInItem

class MiPeriodicoSpider(BaseArticleSpider):
    name = 'mi_periodico'
    allowed_domains = ['miperiodico.com']
    start_urls = ['https://miperiodico.com/noticias']
    
    def parse_article(self, response):
        item = ArticuloInItem()
        item['url'] = response.url
        item['titular'] = response.css('h1::text').get()
        item['contenido_texto'] = self.extract_article_content(response)
        # Usar mÃ©todos de la clase base cuando sea posible
        yield item
```

Ver [examples/example_spiders.py](examples/example_spiders.py) para ejemplos completos.

## ğŸ§ª Testing y Calidad

### Ejecutar Tests

```bash
# Todos los tests
pytest

# Tests especÃ­ficos
pytest tests/unit/                    # Tests unitarios
pytest tests/integration/             # Tests de integraciÃ³n
pytest tests/test_pipelines/          # Tests de pipelines

# Con coverage
pytest --cov=scraper_core --cov-report=html
```

### Estructura de Tests

- **`tests/unit/`**: Tests unitarios para componentes individuales
- **`tests/integration/`**: Tests de integraciÃ³n con Supabase
- **`tests/test_pipelines/`**: Tests especÃ­ficos de pipelines
- **`tests/e2e/`**: Tests end-to-end del flujo completo
- **`tests/performance/`**: Tests de rendimiento y carga

Ver [tests/docs/README_tests.md](tests/docs/README_tests.md) para guÃ­a completa de testing.

## ğŸ“Š Pipelines de Procesamiento

### 1. DataValidationPipeline
- âœ… Valida campos requeridos
- âœ… Verifica tipos de datos
- âœ… Normaliza fechas y URLs
- âœ… Rechaza contenido insuficiente

### 2. DataCleaningPipeline
- ğŸ§¹ Limpia HTML y texto
- ğŸ”§ Normaliza caracteres especiales
- ğŸ“… Estandariza fechas
- ğŸ·ï¸ Deduplica etiquetas

### 3. SupabaseStoragePipeline
- ğŸ’¾ Almacena metadatos en PostgreSQL
- ğŸ—œï¸ Comprime y guarda HTML original
- ğŸ”„ Maneja reintentos inteligentes

Ver [docs/architecture/pipelines_documentation.md](docs/architecture/pipelines_documentation.md) para documentaciÃ³n detallada.

## ğŸš€ EjecuciÃ³n

### Comandos BÃ¡sicos

```bash
# Ejecutar spider especÃ­fico
scrapy crawl infobae

# Con configuraciÃ³n personalizada
scrapy crawl infobae -s LOG_LEVEL=DEBUG -s CONCURRENT_REQUESTS=1

# Listar spiders disponibles
scrapy list

# Verificar configuraciÃ³n
scrapy check
```

### Para Desarrollo

```bash
# Ejecutar con recarga automÃ¡tica
scrapy crawl infobae -s AUTOTHROTTLE_ENABLED=True

# Debug mode completo
scrapy crawl infobae -L DEBUG -s HTTPCACHE_ENABLED=False
```

## ğŸ“ˆ Monitoreo y Debugging

### Logging

```python
# En settings.py
LOG_LEVEL = 'INFO'
LOGGERS = {
    'scraper_core.pipelines.validation': 'DEBUG',
    'scraper_core.pipelines.cleaning': 'DEBUG',
    'scraper_core.utils.supabase_client': 'INFO'
}
```

### EstadÃ­sticas

Los pipelines mantienen estadÃ­sticas automÃ¡ticas:
- Items procesados/vÃ¡lidos/invÃ¡lidos
- Tipos de errores encontrados
- Operaciones de limpieza realizadas
- Tiempos de procesamiento

## ğŸ› Troubleshooting

### Problemas Comunes

| Problema | SoluciÃ³n |
|----------|----------|
| Items rechazados | Revisar logs de validaciÃ³n, verificar campos requeridos |
| ConexiÃ³n Supabase | Verificar credenciales en `config/.env.test` |
| Contenido vacÃ­o | Revisar selectores CSS/XPath, considerar usar Playwright |
| Rendimiento lento | Ajustar `CONCURRENT_REQUESTS` y delays |

### Debug Tips

1. **Habilitar cache HTTP** para desarrollo: `HTTPCACHE_ENABLED = True`
2. **Usar Scrapy shell** para probar selectores: `scrapy shell "https://example.com"`
3. **Verificar logs** en orden: spider â†’ pipelines â†’ storage

## ğŸ“š DocumentaciÃ³n

- [**Arquitectura**](docs/architecture/) - Decisiones tÃ©cnicas y componentes
- [**Desarrollo**](docs/development/) - GuÃ­as para desarrolladores
- [**Ejemplos**](examples/) - CÃ³digo de ejemplo y plantillas
- [**Tests**](tests/docs/) - DocumentaciÃ³n de testing
- [**ConfiguraciÃ³n**](config/README.md) - GuÃ­a de configuraciÃ³n

## ğŸ›£ï¸ Roadmap

### PrÃ³ximas Mejoras

- [ ] Sistema de mÃ©tricas avanzado (Prometheus/Grafana)
- [ ] Cache inteligente de pÃ¡ginas renderizadas
- [ ] DetecciÃ³n automÃ¡tica de sitios que requieren JavaScript
- [ ] Rate limiting dinÃ¡mico por dominio
- [ ] IntegraciÃ³n con Spidermon para alertas

### En Desarrollo

- [ ] Spiders para mÃ¡s fuentes de noticias
- [ ] Mejoras en el sistema de detecciÃ³n de duplicados
- [ ] Dashboard de monitoreo en tiempo real

## ğŸ¤ Contribuir

1. **Fork** el repositorio
2. **Crear rama** para tu feature: `git checkout -b feature/nueva-funcionalidad`
3. **Hacer commit** de cambios: `git commit -am 'Agregar nueva funcionalidad'`
4. **Push** a la rama: `git push origin feature/nueva-funcionalidad`
5. **Crear Pull Request**

## ğŸ“„ Licencia

[Especificar licencia del proyecto]

---

**Nota**: Este proyecto es parte de La MÃ¡quina de Noticias, un sistema integral de procesamiento de informaciÃ³n periodÃ­stica.
