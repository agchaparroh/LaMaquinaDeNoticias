# Module Scraper - La M√°quina de Noticias

Sistema de web scraping especializado en extracci√≥n autom√°tica de contenido period√≠stico de medios latinoamericanos.

## üéØ Prop√≥sito

Extrae, procesa y almacena art√≠culos period√≠sticos de fuentes web predefinidas usando Scrapy con capacidades de renderizado JavaScript v√≠a Playwright. Los datos se validan, limpian y almacenan en Supabase (PostgreSQL + Storage).

## ‚ö° Caracter√≠sticas Principales

- **Web Scraping Inteligente**: Scrapy con Playwright para sitios JavaScript
- **Pipelines de Procesamiento**: Validaci√≥n, limpieza y almacenamiento autom√°tico
- **Rate Limiting Din√°mico**: Configuraci√≥n por dominio para scraping respetuoso
- **Almacenamiento Dual**: Metadatos en PostgreSQL, HTML comprimido en Storage
- **Reintentos Inteligentes**: Tenacity para operaciones cr√≠ticas con Supabase
- **Monitoreo Integrado**: Logging detallado y rotaci√≥n autom√°tica
- **Dockerizado**: Desarrollo y deployment simplificado

## üöÄ Setup R√°pido

### 1. Prerequisitos

- Docker y Docker Compose
- Cuenta de Supabase con proyecto creado

### 2. Configuraci√≥n

```bash
# Clonar y navegar al directorio
cd module_scraper

# Configurar variables de entorno
cp config/.env config/.env.local
# Editar .env.local con tus credenciales de Supabase

# Levantar entorno de desarrollo
docker-compose --profile dev up -d scraper-dev

# Entrar al contenedor
docker-compose exec scraper-dev bash
```

### 3. Primera Ejecuci√≥n

```bash
# Dentro del contenedor
scrapy list                    # Ver spiders disponibles
scrapy crawl infobae          # Ejecutar spider de prueba
```

## üìä Uso B√°sico

### Spiders Disponibles

```bash
# Medios latinoamericanos
scrapy crawl infobae_spider              # Infobae (Argentina/Latinoam√©rica)
scrapy crawl elpais_latinoamerica        # El Pa√≠s - secci√≥n Latinoam√©rica  
scrapy crawl elnacional_latinoamerica    # El Nacional - Latinoam√©rica
scrapy crawl europapress_sudamerica      # Europa Press - Sudam√©rica
```

### Ejecuci√≥n con Configuraci√≥n Personalizada

```bash
# Debug mode con cache habilitado
scrapy crawl infobae -L DEBUG -s HTTPCACHE_ENABLED=True

# Configuraci√≥n de concurrencia
scrapy crawl infobae -s CONCURRENT_REQUESTS=4 -s DOWNLOAD_DELAY=3

# Usando Playwright para sitios JavaScript
scrapy crawl infobae -s USE_PLAYWRIGHT_FOR_EMPTY_CONTENT=True
```

### Desarrollo y Testing

```bash
# Ejecutar tests
docker-compose --profile test up scraper-test

# Shell interactivo para debugging
scrapy shell "https://www.infobae.com/america/"

# Verificar configuraci√≥n
scrapy check
```

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno Principales

```env
# Supabase (requerido)
SUPABASE_URL="https://tu-proyecto.supabase.co"
SUPABASE_SERVICE_ROLE_KEY="tu-service-role-key"
SUPABASE_HTML_BUCKET="html_content"

# Scrapy
LOG_LEVEL="INFO"
CONCURRENT_REQUESTS="8"
DOWNLOAD_DELAY="2"

# Playwright
PLAYWRIGHT_MAX_RETRIES="2"
PLAYWRIGHT_TIMEOUT="30000"
USE_PLAYWRIGHT_FOR_EMPTY_CONTENT="True"
```

### Rate Limiting por Dominio

El sistema incluye configuraci√≥n espec√≠fica por dominio en `config/rate_limits/domain_config.py`:

```python
DOMAIN_RATE_LIMITS = {
    'infobae.com': {
        'delay': 3,
        'concurrency': 1
    },
    'elpais.com': {
        'delay': 2,
        'concurrency': 2
    }
}
```

## üèóÔ∏è Arquitectura

### Pipeline de Procesamiento

```
Extracci√≥n ‚Üí Validaci√≥n ‚Üí Limpieza ‚Üí Almacenamiento
    ‚Üì            ‚Üì           ‚Üì           ‚Üì
 Spiders   DataValidation DataCleaning SupabaseStorage
```

### Componentes Principales

- **`scraper_core/spiders/`**: Spiders especializados por medio
- **`scraper_core/pipelines/`**: Procesamiento de datos (validaci√≥n, limpieza, storage)
- **`scraper_core/middlewares/`**: Playwright, rate limiting, user agents
- **`scraper_core/utils/`**: Clientes Supabase, compresi√≥n, logging

### Flujo de Datos

1. **Spider extrae** art√≠culo de sitio web
2. **DataValidationPipeline** valida campos requeridos
3. **DataCleaningPipeline** limpia HTML y normaliza texto
4. **SupabaseStoragePipeline** almacena metadatos en PostgreSQL y HTML comprimido en Storage

## üìÅ Estructura del Proyecto

```
module_scraper/
‚îú‚îÄ‚îÄ scraper_core/           # C√≥digo principal
‚îÇ   ‚îú‚îÄ‚îÄ spiders/           # Spiders por medio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base/          # Clases base reutilizables
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [medio]_spider.py
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/         # Procesamiento de datos
‚îÇ   ‚îú‚îÄ‚îÄ middlewares/       # Middlewares personalizados
‚îÇ   ‚îú‚îÄ‚îÄ utils/            # Utilidades (Supabase, compresi√≥n)
‚îÇ   ‚îú‚îÄ‚îÄ items.py          # Definici√≥n de datos
‚îÇ   ‚îî‚îÄ‚îÄ settings.py       # Configuraci√≥n principal
‚îú‚îÄ‚îÄ config/               # Configuraciones y entorno
‚îú‚îÄ‚îÄ tests/               # Testing completo
‚îú‚îÄ‚îÄ scripts/             # Scripts de utilidad
‚îú‚îÄ‚îÄ examples/            # Ejemplos de uso
‚îî‚îÄ‚îÄ docs/               # Documentaci√≥n t√©cnica
```

## üß™ Testing

### Ejecutar Tests

```bash
# Todos los tests
docker-compose --profile test up scraper-test

# Tests espec√≠ficos (dentro del contenedor)
pytest tests/unit/                    # Tests unitarios
pytest tests/integration/             # Tests de integraci√≥n
pytest tests/test_pipelines/          # Tests de pipelines

# Con coverage
pytest --cov=scraper_core --cov-report=html
```

### Tipos de Tests

- **Unitarios**: Componentes individuales
- **Integraci√≥n**: Interacci√≥n con Supabase
- **End-to-End**: Flujo completo de scraping
- **Pipelines**: Validaci√≥n y limpieza de datos

## üîß Desarrollo

### Crear Nuevo Spider

```python
from scraper_core.spiders.base import BaseArticleSpider
from scraper_core.items import ArticuloInItem

class MiMedioSpider(BaseArticleSpider):
    name = 'mi_medio'
    allowed_domains = ['mimedio.com']
    start_urls = ['https://mimedio.com/noticias']
    
    def parse(self, response):
        item = ArticuloInItem()
        item['url'] = response.url
        item['titular'] = response.css('h1::text').get()
        item['contenido_texto'] = self.extract_article_content(response)
        # Usar m√©todos heredados para funcionalidad com√∫n
        yield item
```

### Debugging

```bash
# Logs detallados
scrapy crawl mi_spider -L DEBUG

# Cache para desarrollo
scrapy crawl mi_spider -s HTTPCACHE_ENABLED=True

# Shell interactivo
scrapy shell "https://mimedio.com/articulo"
```

## üêõ Troubleshooting

### Problemas Comunes

| Problema | Soluci√≥n |
|----------|----------|
| Items rechazados | Revisar logs de `DataValidationPipeline` |
| Contenido vac√≠o | Habilitar `USE_PLAYWRIGHT_FOR_EMPTY_CONTENT=True` |
| Error Supabase | Verificar credenciales en `config/.env` |
| Rate limiting | Ajustar `DOWNLOAD_DELAY` y `CONCURRENT_REQUESTS` |

### Logs √ötiles

```bash
# Ver logs en tiempo real
docker-compose logs -f scraper-dev

# Logs espec√≠ficos de pipelines
tail -f logs/scraper_core.pipelines.validation.log
```

## üîÑ Pipelines de Datos

### DataValidationPipeline

- Valida campos requeridos: `url`, `titulo`, `medio`, `fecha_publicacion`
- Normaliza URLs y fechas
- Rechaza contenido insuficiente

### DataCleaningPipeline

- Limpia HTML y normaliza whitespace
- Estandariza caracteres especiales
- Deduplica etiquetas

### SupabaseStoragePipeline

- Almacena metadatos en tabla `articulos`
- Comprime y guarda HTML original en Storage
- Implementa reintentos con Tenacity

## üìù Items de Datos

### ArticuloInItem

Campos principales extra√≠dos:

```python
item['url']                    # URL original
item['titular']                # T√≠tulo del art√≠culo  
item['medio']                  # Nombre del medio
item['fecha_publicacion']      # Fecha de publicaci√≥n
item['contenido_texto']        # Texto limpio
item['contenido_html']         # HTML original
item['autor']                  # Autor(es)
item['seccion']               # Secci√≥n del medio
item['pais_publicacion']      # Pa√≠s de origen
```

## üöÄ Deployment

### Docker Compose

```bash
# Producci√≥n
docker-compose up -d scraper

# Desarrollo con vol√∫menes
docker-compose --profile dev up -d scraper-dev

# Solo testing  
docker-compose --profile test up scraper-test
```

### Variables de Entorno Cr√≠ticas

- `SUPABASE_URL`: URL del proyecto Supabase
- `SUPABASE_SERVICE_ROLE_KEY`: Key con permisos de escritura
- `SUPABASE_HTML_BUCKET`: Bucket para almacenar HTML

## üìö Documentaci√≥n Adicional

- [STRUCTURE.md](STRUCTURE.md) - Estructura detallada del proyecto
- [CONFIGURACION.md](CONFIGURACION.md) - Gu√≠a de configuraci√≥n completa
- [tests/docs/](tests/docs/) - Documentaci√≥n de testing
- [examples/](examples/) - Ejemplos de c√≥digo

## ü§ù Contribuir

1. Fork el repositorio
2. Crear rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit cambios: `git commit -am 'Agregar funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Crear Pull Request

### Est√°ndares

- Tests para nuevos spiders
- Logging apropiado
- Documentaci√≥n actualizada
- Respeto por rate limiting

## üìÑ Licencia

[Definir licencia del proyecto]

---

**Parte de La M√°quina de Noticias** - Sistema integral de procesamiento de informaci√≥n period√≠stica.
