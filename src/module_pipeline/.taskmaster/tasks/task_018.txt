# Task ID: 18
# Title: Implement Phase 4: Normalization, Linking, and Relationships
# Status: done
# Dependencies: 5, 8, 9, 12, 13
# Priority: high
# Description: Create the fourth phase of the processing pipeline in src/pipeline/fase_4_normalizacion.py.
# Details:
1. Create src/pipeline/fase_4_normalizacion.py
2. Implement ejecutar_fase_4() function
3. Integrate with Groq API using the prompt from 'Prompt_4_relaciones.md'
4. Use EntityNormalizer service (from Task 13) for entity normalization and linking
5. Extract relationships between facts and entities
6. Normalize and consolidate all extracted information
7. Return ResultadoFase4 object with fully processed and linked data

# Test Strategy:
Create comprehensive unit tests for normalization, linking, and relationship extraction. Mock Supabase calls for testing entity linking. Test integration with EntityNormalizer service. Ensure tests verify that sequential IDs are maintained throughout the normalization process.

# Subtasks:
## 1. Implement ResultadoFase4 class and basic structure [done]
### Dependencies: None
### Description: Create the ResultadoFase4 class with all required fields and methods to store the final processed data including normalized entities and relationships.
### Details:
1. Define ResultadoFase4 class with fields for normalized entities, relationships, and metadata
2. Implement constructor, getters, setters, and validation methods
3. Create data structures for storing entity relationships
4. Add error handling and logging mechanisms
5. Implement toString() and toJson() methods for debugging and serialization
6. Add documentation following project standards
<info added on 2025-06-06T08:43:34.234Z>
## Implementation Details for ResultadoFase4 and Basic Structure

### File Structure
- Create `src/pipeline/fase_4_normalizacion.py` following the pattern established in previous phases
- Follow the same architectural approach as in phases 1-3 for consistency

### Key Components
1. Main function: `ejecutar_fase_4(resultado_fase_3: ResultadoFase3CitasDatos) -> ResultadoFase4Normalizacion`
2. Helper functions:
   - `_formatear_elementos_normalizados()` - Prepares normalized elements for processing
   - `_normalizar_entidades()` - Handles entity normalization using EntityNormalizer service
   - `_extraer_relaciones()` - Processes relationships between entities

### ResultadoFase4Normalizacion Implementation
- Use existing class from `src/models/procesamiento.py`
- Ensure proper field validation with Pydantic
- Implement relationship structures using complex type hints (List, Dict, Optional)
- Generate unique UUIDs for relationships and final result

### Integration Points
- EntityNormalizer service integration for standardizing entities
- Input: Process ELEMENTOS_BASICOS_NORMALIZADOS from previous phases
- Output: Follow JSON format specified in `prompts/Prompt_4_relaciones.md`

### Technical Requirements
- Implement proper error handling for normalization failures
- Add comprehensive logging throughout the process
- Ensure type safety with Python typing annotations
- Follow project documentation standards for all methods and classes
</info added on 2025-06-06T08:43:34.234Z>
<info added on 2025-06-06T08:45:29.645Z>
## DOCUMENTOS DEL REPOSITORIO A REVISAR:
- `src/pipeline/fase_1_triaje.py`, `fase_2_extraccion.py`, `fase_3_citas_datos.py` - Para seguir el patrón estructural establecido en fases anteriores
- `src/models/procesamiento.py` - Para entender `ResultadoFase4Normalizacion` ya definido y sus campos requeridos
- `prompts/Prompt_4_relaciones.md` - Para entender formato de entrada (ELEMENTOS_BASICOS_NORMALIZADOS) y salida JSON
- `src/services/entity_normalizer.py` - Para entender integración con servicio de normalización existente

## CONTEXT7 DOCUMENTATION A CONSULTAR:
- **Pydantic**: BaseModel inheritance patterns, field validation, nested models para estructuras de relaciones
- **Python typing**: Type hints complejos con List, Dict, Optional para relaciones hecho-entidad
- **Python pathlib**: Manejo de rutas para imports relativos desde pipeline hacia servicios
- **UUID generation**: uuid4() para IDs únicos de relaciones y resultado final

## IMPLEMENTACIÓN ESPECÍFICA:
- Crear archivo `src/pipeline/fase_4_normalizacion.py` siguiendo estructura de fases anteriores
- Importar dependencias: `ResultadoFase4Normalizacion` desde models, entity_normalizer, groq_service
- Definir función `ejecutar_fase_4(resultado_fase_1, resultado_fase_2, resultado_fase_3)` como entry point
- CRÍTICO: Esta fase necesita TODOS los resultados de fases anteriores para formatear ELEMENTOS_BASICOS_NORMALIZADOS
- Crear funciones helper privadas: `_formatear_elementos_para_prompt()`, `_normalizar_entidades_extraidas()`, `_parsear_relaciones_json()`
- NOTA: `ResultadoFase4Normalizacion` ya existe en procesamiento.py, usar directamente sin modificar
- Las relaciones deben mapear exactamente al JSON schema del Prompt_4_relaciones.md: hecho_entidad, hecho_relacionado, entidad_relacion, contradicciones
</info added on 2025-06-06T08:45:29.645Z>

## 2. Implement EntityNormalizer service integration [done]
### Dependencies: 18.1
### Description: Integrate with the existing EntityNormalizer service to normalize and deduplicate entities extracted in previous phases.
### Details:
1. Create an EntityNormalizerClient class to interface with the service
2. Implement methods to send entities for normalization
3. Process normalization responses and handle errors
4. Add caching mechanism to avoid redundant normalization requests
5. Implement entity mapping to maintain references between original and normalized entities
6. Add comprehensive logging for debugging normalization issues
<info added on 2025-06-06T08:45:50.379Z>
7. Review existing EntityNormalizer service in src/services/entity_normalizer.py
8. Implement _normalizar_entidades_extraidas() function to process extracted entities
9. Update EntidadProcesada objects with normalization fields (id_entidad_normalizada, nombre_entidad_normalizada, similitud_normalizacion, uri_wikidata)
10. Apply similarity threshold logic (>0.8 for automatic matching, <0.8 for new entity)
11. Implement batch processing for optimizing multiple RPC calls
12. Create temporary cache during phase execution to avoid redundant normalization requests
13. Add structured logging with Loguru for tracking successful/failed normalizations
14. Integrate retry mechanisms using Tenacity library with exponential backoff
15. Implement async/await patterns for parallel normalization of multiple entities
16. Preserve original entity list integrity while returning modified list with normalization data
</info added on 2025-06-06T08:45:50.379Z>
<info added on 2025-06-07T10:12:30.123Z>
17. Enriquecer objetos EntidadProcesada existentes en lugar de crear nuevos
18. Mantener id_entidad (int secuencial) intacto durante todo el proceso
19. Solo añadir id_entidad_normalizada (UUID de DB) como campo adicional
20. Implementar lógica para preservar IDs secuenciales internos durante normalización
21. Asegurar que las referencias entre entidades y hechos mantengan coherencia de IDs
22. Documentar claramente la diferencia entre IDs internos (secuenciales) e IDs de DB (UUID)
</info added on 2025-06-07T10:12:30.123Z>

## 3. Implement Groq API integration for relationship extraction [done]
### Dependencies: 18.1
### Description: Integrate with Groq API using the relationship prompt to extract relationships between facts and entities.
### Details:
1. Create a RelationshipExtractor class using the GroqClient
2. Implement prompt construction using Prompt_4_relaciones.md template
3. Process API responses to extract relationship data
4. Implement retry logic and error handling for API failures
5. Add rate limiting and request batching for efficiency
6. Create data structures to store extracted relationships
7. Implement validation for relationship data
<info added on 2025-06-06T08:46:17.050Z>
## DOCUMENTOS DEL REPOSITORIO A REVISAR:
- `prompts/Prompt_4_relaciones.md` - Para entender las variables del prompt: {{ELEMENTOS_BASICOS_NORMALIZADOS}}, {{ELEMENTOS_COMPLEMENTARIOS}}
- `src/pipeline/fase_1_triaje.py`, `fase_2_extraccion.py` - Funciones `_llamar_groq_api_*()` como referencia de integración Groq
- `src/services/groq_service.py` - Para entender el servicio Groq existente y configuración de parámetros
- `src/utils/prompt_loader.py` - Para cargar plantilla de prompt desde archivo externo
- `src/utils/error_handling.py` - Para decoradores de retry específicos para APIs LLM

## CONTEXT7 DOCUMENTATION A CONSULTAR:
- **Groq SDK**: Client configuration para prompts complejos, chat completions con JSON output
- **Python json**: JSON parsing para respuesta compleja con arrays anidados (hecho_entidad, entidad_relacion)
- **Tenacity library**: Retry patterns específicos para prompts largos con mayor timeout
- **Pydantic**: Model validation para parsear estructuras JSON complejas de relaciones

## IMPLEMENTACIÓN ESPECÍFICA:
- Crear función `_llamar_groq_api_relaciones()` siguiendo patrón de fases anteriores
- CRÍTICO: Formatear {{ELEMENTOS_BASICOS_NORMALIZADOS}} desde hechos y entidades normalizadas de fases 2
- CRÍTICO: Formatear {{ELEMENTOS_COMPLEMENTARIOS}} desde citas y datos de fase 3  
- Variables del prompt: {{TITULO_O_DOCUMENTO}}, {{FUENTE_O_TIPO}}, {{PAIS_ORIGEN}}, {{FECHA_FUENTE}} desde metadata original
- Configurar parámetros Groq para análisis complejo: mayor max_tokens, temperature baja para consistencia
- Implementar parsing robusto para JSON de respuesta con 4 secciones: hecho_entidad, hecho_relacionado, entidad_relacion, contradicciones
- Manejo de errores específicos: JSON malformado, relaciones con IDs inexistentes, arrays vacíos
- Usar decorador `@retry_groq_api()` existente con timeout extendido para prompts complejos
</info added on 2025-06-06T08:46:17.050Z>
<info added on 2025-06-07T10:13:45.456Z>
8. Asegurar que las relaciones extraídas mantengan los IDs secuenciales internos
9. No generar nuevos IDs para entidades o hechos durante el proceso de extracción
10. Mantener coherencia con los IDs secuenciales utilizados en fases anteriores
11. Implementar validación para verificar que todos los IDs referenciados existen
12. Documentar claramente el formato de IDs esperado en las relaciones extraídas
</info added on 2025-06-07T10:13:45.456Z>

## 4. Implement relationship processing and linking logic [done]
### Dependencies: 18.2, 18.3
### Description: Develop logic to process extracted relationships and link them to normalized entities.
### Details:
1. Create a RelationshipProcessor class to handle relationship data
2. Implement methods to link relationships to normalized entities
3. Add validation and conflict resolution for contradictory relationships
4. Implement confidence scoring for relationships
5. Create data structures for efficient relationship querying
6. Add methods to filter and prioritize relationships based on confidence
7. Implement logging for relationship processing steps
<info added on 2025-06-06T08:46:50.310Z>
8. Review repository documents for relationship structure understanding:
   - `prompts/Prompt_4_relaciones.md` for JSON relationship structure
   - `src/models/procesamiento.py` for normalized ID fields
   - `src/pipeline/fase_2_extraccion.py` and `fase_3_citas_datos.py` for JSON parsers
   - `src/utils/error_handling.py` for relationship validation errors

9. Implement specific parser functions:
   - `_parsear_relaciones_hecho_entidad()`
   - `_parsear_relaciones_hecho_hecho()`
   - `_parsear_relaciones_entidad_entidad()`
   - `_parsear_contradicciones()`

10. Validate relationship types according to defined mappings:
    - hecho_entidad: protagonista, mencionado, afectado, declarante, ubicacion, etc.
    - hecho_relacionado: causa, consecuencia, contexto_historico, respuesta_a, etc.
    - entidad_relacion: miembro_de, subsidiaria_de, aliado_con, opositor_a, etc.

11. Implement numerical validation for relationship attributes:
    - relevancia_en_hecho (1-10)
    - fuerza_relacion (1-10)
    - grado_contradiccion (1-5)

12. Create indexed data structures for efficient relationship querying

13. Implement circular relationship detection and logical contradiction identification

14. Ensure all referenced IDs in relationships exist in extracted facts/entities
</info added on 2025-06-06T08:46:50.310Z>
<info added on 2025-06-07T10:15:00.789Z>
15. Mantener coherencia de IDs secuenciales en todas las relaciones procesadas
16. Verificar que las relaciones solo referencien IDs existentes en el conjunto de datos
17. Implementar validación cruzada entre IDs de entidades y hechos referenciados
18. Documentar claramente la estructura de datos para mantener trazabilidad de IDs
19. Asegurar que el procesamiento de relaciones no altere los IDs secuenciales originales
</info added on 2025-06-07T10:15:00.789Z>

## 5. Implement final data consolidation and PayloadBuilder integration [done]
### Dependencies: 18.4
### Description: Consolidate all processed data and implement integration with PayloadBuilder for final output.
### Details:
1. Create a DataConsolidator class to assemble all processed information
2. Implement methods to merge data from all previous phases
3. Add validation for the final consolidated data structure
4. Create PayloadBuilderAdapter to format data for PayloadBuilder
5. Implement error handling and fallback mechanisms
6. Add comprehensive logging for the consolidation process
7. Create unit and integration tests for the entire Phase 4 implementation
<info added on 2025-06-06T08:47:32.661Z>
Implement the `ejecutar_fase_4()` function that consolidates results from all previous phases into a unified `ResultadoFase4Normalizacion` structure. The function should:

1. Extract `id_fragmento` from `resultado_fase_1`
2. Use EntityNormalizer to create normalized entities
3. Generate automatic normalization summary with process statistics
4. Include the formatted prompt used in Groq
5. Set normalization status (Complete, Partial, Failed) based on component success/failure
6. Add technical metrics in metadata (tokens, duration, processed entities, extracted relationships)
7. Validate referential integrity ensuring all relationships point to existing entities/facts
8. Call `.touch()` to update timestamps before returning
9. Prepare data for PayloadBuilder by serializing UUIDs, formatting dates, validating JSONB structure
10. Implement robust error handling with fallbacks to return unnormalized data if normalization fails
11. Add comprehensive logging for final metrics, normalized vs. new entities, processed relationships, and total pipeline time

Review repository documents:
- `src/models/procesamiento.py` for complete `ResultadoFase4Normalizacion` structure
- `src/services/payload_builder.py` for JSONB payload building methods
- Phase 1-3 implementation files for final assembly pattern and error handling
- `src/utils/error_handling.py` for error handling patterns and fallbacks

Consult documentation for:
- Python UUID generation and serialization
- Loguru structured logging for completeness metrics
- Comprehensive exception handling for multi-component processes
- Pydantic model validation, .touch() method, and JSON serialization
</info added on 2025-06-06T08:47:32.661Z>
<info added on 2025-06-07T10:16:30.234Z>
12. Modificar la firma de la función a `ejecutar_fase_4(processor: FragmentProcessor, resultado_fase_1, resultado_fase_2, resultado_fase_3)`
13. Utilizar el mismo processor recibido como parámetro para mantener coherencia con fases anteriores
14. No realizar conversión final a UUIDs en esta fase, eso ocurre en persistencia
15. Mantener los IDs secuenciales internos durante todo el proceso
16. Documentar claramente que solo id_entidad_normalizada (UUID de DB) es externo
17. Asegurar que el mismo processor usado en fases anteriores se utilice aquí
18. Implementar validación para verificar coherencia de IDs entre fases
19. Añadir logging específico para seguimiento de IDs durante el proceso
</info added on 2025-06-07T10:16:30.234Z>

## 6. Update ejecutar_fase_4 function signature for FragmentProcessor [done]
### Dependencies: 18.1
### Description: Modify the function signature to receive FragmentProcessor as parameter to maintain sequential ID coherence across all phases.
### Details:
1. Update function signature to `ejecutar_fase_4(processor: FragmentProcessor, resultado_fase_1, resultado_fase_2, resultado_fase_3)`
2. Use the processor parameter to maintain ID sequence consistency with previous phases
3. Document the importance of using the same processor instance across all phases
4. Implement validation to ensure processor state is consistent with previous phase results
5. Add logging for processor state at beginning and end of phase execution
6. Update unit tests to include processor parameter in function calls
7. Create helper methods that utilize processor for ID management if needed
<info added on 2025-06-06T13:48:29.821Z>
Especificar que la función debe recibir resultado_fase_3: ResultadoFase3CitasDatos y processor: FragmentProcessor. El processor debe ser el mismo usado en fases anteriores para mantener consistencia de IDs a través de todo el pipeline. Verificar que el processor contenga el estado acumulado de las fases anteriores antes de proceder con la normalización y vinculación de entidades.
</info added on 2025-06-06T13:48:29.821Z>

## 7. Implement ID preservation strategy [done]
### Dependencies: 18.2, 18.3
### Description: Ensure that sequential internal IDs are maintained throughout the normalization process while properly handling external database UUIDs.
### Details:
1. Document the distinction between internal sequential IDs and external database UUIDs
2. Implement logic to enrich existing EntidadProcesada objects rather than creating new ones
3. Maintain id_entidad (int sequential) intact throughout the entire process
4. Only add id_entidad_normalizada (DB UUID) as an additional field
5. Ensure all entity references in relationships use the original sequential IDs
6. Add validation to verify ID integrity across all processed data
7. Implement logging specifically for ID tracking during normalization
8. Create unit tests that verify ID preservation across the entire phase
<info added on 2025-06-06T13:48:45.730Z>
IMPORTANTE: La fase de normalización NO debe crear nuevos objetos EntidadProcesada bajo ninguna circunstancia. El proceso debe limitarse exclusivamente a enriquecer los objetos existentes añadiendo los campos de normalización correspondientes. Los identificadores secuenciales internos (id_entidad) deben mantenerse intactos durante todo el proceso y nunca ser modificados o reemplazados. La conversión de identificadores secuenciales a UUIDs es responsabilidad exclusiva del PayloadBuilder y no debe realizarse en esta fase. Cualquier implementación que cree nuevos objetos o modifique los IDs secuenciales existentes debe considerarse errónea.
</info added on 2025-06-06T13:48:45.730Z>

