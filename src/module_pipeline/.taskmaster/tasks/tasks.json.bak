{
  "tasks": [
    {
      "id": 1,
      "title": "Set up project structure and environment",
      "description": "Initialize the project repository with the required directory structure and set up the development environment.",
      "details": "1. Create the project directory structure as specified in section 4.1 of the PRD.\n2. Initialize a git repository.\n3. Create a virtual environment using Python 3.8+.\n4. Create a .gitignore file to exclude the virtual environment and sensitive files.\n5. Copy the .env.example file and rename it to .env for local development.",
      "testStrategy": "Verify the correct directory structure is in place and the virtual environment can be activated.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Install and configure dependencies",
      "description": "Install all required dependencies and configure the project settings, ensuring version consistency with existing modules.",
      "details": "1. Install dependencies from requirements.txt using pip, referencing versions from module_connector and module_scraper for shared libraries:\n   - fastapi==0.116.2\n   - uvicorn[standard]==0.35.1\n   - groq==0.26.0\n   - spacy==3.8.7\n   - sentence-transformers==2.9.0\n   - supabase (use same version as in module_connector/module_scraper)\n   - psycopg2-binary==2.9.10\n   - pydantic (use same version as in module_connector/module_scraper)\n   - python-dotenv==1.1.0\n   - httpx==0.27.2\n   - tenacity (use same version as in module_connector/module_scraper)\n   - loguru (use same version as in module_connector/module_scraper)\n   - numpy>=1.21.0,<2.0.0\n2. Check module_connector and module_scraper repositories to identify the specific versions of shared libraries (supabase, tenacity, pydantic, loguru) to maintain consistency.\n3. Update requirements.txt with the consistent versions.\n4. Configure settings in src/config/settings.py to load environment variables.",
      "testStrategy": "1. Run 'pip freeze' to verify all dependencies are installed correctly.\n2. Compare versions of shared libraries with those in module_connector and module_scraper to ensure consistency.\n3. Test importing key libraries in a Python shell to verify compatibility.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Review dependency versions in existing modules",
          "description": "Check module_connector and module_scraper repositories to identify the specific versions of shared libraries.",
          "details": "üéØ OBJETIVO ESPEC√çFICO: \nIdentificar las versiones exactas de las librer√≠as compartidas (supabase, tenacity, pydantic, loguru) utilizadas en module_connector y module_scraper para garantizar compatibilidad arquitect√≥nica y evitar conflictos de versiones que puedan causar fallos en integraci√≥n entre m√≥dulos.\n\nüìö REFERENCIAS OBLIGATORIAS:\n* Context7 ID: /supabase/supabase-py (Supabase Python SDK)\n* Context7 ID: /jd/tenacity (Retry library)\n* Context7 ID: /pydantic/pydantic (Data validation)\n* Context7 ID: /delgan/loguru (Logging framework)\n* Archivo: ../module_connector/requirements.txt (para versiones de referencia)\n* Archivo: ../module_scraper/requirements.txt (para versiones de referencia)\n* Documentaci√≥n: docs/08-configuracion-e-infraestructura.md (dependencias cr√≠ticas)\n* Patr√≥n: Consistencia de versiones entre m√≥dulos del ecosistema La M√°quina de Noticias\n\n‚öôÔ∏è IMPLEMENTACI√ìN GUIADA:\n1. Navegar a ../module_connector/ y examinar requirements.txt para extraer versiones de: supabase, tenacity, pydantic, loguru\n2. Navegar a ../module_scraper/ y examinar requirements.txt para las mismas librer√≠as\n3. Crear tabla comparativa documentando versiones encontradas en cada m√≥dulo\n4. Identificar conflictos potenciales o discrepancias entre versiones\n5. Seleccionar versi√≥n m√°s restrictiva/estable para cada librer√≠a compartida\n6. Documentar decisiones de versionado en archivo temporal version_analysis.md\n\n‚úÖ CRITERIOS DE ACEPTACI√ìN:\n* Versiones exactas identificadas para supabase, tenacity, pydantic, loguru en ambos m√≥dulos\n* Tabla comparativa creada mostrando versiones por m√≥dulo\n* Decisi√≥n documentada sobre versi√≥n final a usar para cada librer√≠a\n* No hay conflictos de compatibilidad identificados entre versiones seleccionadas\n* Verificaci√≥n de que las versiones son compatibles con Python 3.8+\n\n‚ö†Ô∏è NOTAS CR√çTICAS:\n* Priorizar versiones que mantengan compatibilidad con APIs ya implementadas en module_connector\n* Si hay discrepancias, elegir versi√≥n m√°s reciente que sea compatible con ambos m√≥dulos\n* Considerar que Supabase SDK debe ser compatible con RPCs existentes (insertar_articulo_completo, etc.)\n* Loguru debe mantener formato de logs consistente con el ecosistema",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 2
        },
        {
          "id": 2,
          "title": "Update requirements.txt with consistent versions",
          "description": "Modify requirements.txt to use the same versions of shared libraries (supabase, tenacity, pydantic, loguru) as in other modules.",
          "details": "üéØ OBJETIVO ESPEC√çFICO:\nActualizar requirements.txt del module_pipeline con las versiones exactas identificadas en la subtarea anterior, asegurando compatibilidad total con module_connector y module_scraper para evitar conflictos de versiones durante la integraci√≥n.\n\nüìö REFERENCIAS OBLIGATORIAS:\n* Context7 ID: /tiangolo/fastapi (FastAPI framework)\n* Context7 ID: /encode/uvicorn (ASGI server)\n* Context7 ID: /groq/groq-typescript (Groq SDK)\n* Archivo: version_analysis.md (resultado de subtarea 2.1)\n* Archivo: requirements.txt (archivo actual a modificar)\n* Documentaci√≥n: docs/08-configuracion-e-infraestructura.md (lista completa de dependencias)\n* Patr√≥n: Versionado sem√°ntico y lock de versiones en ecosistema Python\n\n‚öôÔ∏è IMPLEMENTACI√ìN GUIADA:\n1. Usar resultados de version_analysis.md para identificar versiones finales\n2. Actualizar requirements.txt manteniendo comentarios explicativos sobre Context7 IDs\n3. Agrupar dependencias por categor√≠a (Web Framework, IA/ML, Base de Datos, etc.)\n4. Mantener comentario IMPORTANTE sobre NumPy <2.0.0 para compatibilidad con spaCy\n5. Verificar que todas las versiones espec√≠ficas est√°n documentadas con Context7 IDs\n6. A√±adir notas de compatibilidad para dependencias cr√≠ticas\n\n‚úÖ CRITERIOS DE ACEPTACI√ìN:\n* requirements.txt actualizado con versiones exactas identificadas\n* Todas las librer√≠as compartidas usan mismas versiones que otros m√≥dulos\n* Comentarios Context7 IDs mantenidos para trazabilidad\n* Nota cr√≠tica sobre NumPy <2.0.0 preservada\n* Agrupaci√≥n por categor√≠as mantenida para legibilidad\n* No hay dependencias duplicadas o conflictivas\n\n‚ö†Ô∏è NOTAS CR√çTICAS:\n* NO cambiar versiones de fastapi, uvicorn, groq que ya est√°n especificadas correctamente\n* Mantener restricci√≥n numpy>=1.21.0,<2.0.0 para evitar conflictos con spaCy 3.8.7\n* Verificar que versiones actualizadas no introduzcan breaking changes en APIs ya utilizadas\n* Documentar cualquier cambio de versi√≥n que pueda afectar funcionalidad existente",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 2
        },
        {
          "id": 3,
          "title": "Install dependencies with version consistency",
          "description": "Install all dependencies ensuring version consistency with existing modules.",
          "details": "üéØ OBJETIVO ESPEC√çFICO:\nInstalar todas las dependencias del module_pipeline usando las versiones actualizadas, verificando que la instalaci√≥n es exitosa y que no hay conflictos de versiones que comprometan la funcionalidad del sistema o la integraci√≥n con otros m√≥dulos.\n\nüìö REFERENCIAS OBLIGATORIAS:\n* Context7 ID: /python/pip (Package installer)\n* Context7 ID: /explosion/spacy (NLP models download)\n* Archivo: requirements.txt (versiones finales)\n* Script: scripts/setup_env.py (para verificaci√≥n post-instalaci√≥n)\n* Script: scripts/test_connections.py (para verificar conectividad)\n* Documentaci√≥n: docs/08-configuracion-e-infraestructura.md (proceso de instalaci√≥n)\n* Patr√≥n: Instalaci√≥n en entorno virtual aislado\n\n‚öôÔ∏è IMPLEMENTACI√ìN GUIADA:\n1. Verificar que entorno virtual est√° activo usando scripts/setup_env.py\n2. Ejecutar pip install -r requirements.txt y capturar salida\n3. Verificar instalaci√≥n exitosa con pip freeze > installed_versions.txt\n4. Comparar versiones instaladas vs. requeridas para detectar discrepancias\n5. Instalar modelos spaCy si USE_SPACY_FILTER=true: python -m spacy download es_core_news_lg\n6. Ejecutar scripts/setup_env.py para verificaci√≥n completa post-instalaci√≥n\n7. Ejecutar import tests b√°sicos para librer√≠as cr√≠ticas\n\n‚úÖ CRITERIOS DE ACEPTACI√ìN:\n* Todas las dependencias instaladas sin errores de pip\n* Versiones instaladas coinciden exactamente con requirements.txt\n* No hay advertencias de conflictos de versiones en pip\n* scripts/setup_env.py pasa todas las verificaciones de dependencias\n* Imports exitosos de fastapi, groq, supabase, pydantic, loguru, tenacity\n* Modelos spaCy instalados si est√°n habilitados en configuraci√≥n\n* Archivo installed_versions.txt generado para auditor√≠a\n\n‚ö†Ô∏è NOTAS CR√çTICAS:\n* Si pip reporta conflictos de versiones, DETENER y revisar requirements.txt\n* NumPy debe instalarse como <2.0.0 para evitar problemas con spaCy\n* No instalar dependencias globalmente - usar siempre entorno virtual\n* Si falla instalaci√≥n de alguna dependencia, verificar compatibilidad con Python 3.8+\n* Documentar cualquier warning o mensaje durante instalaci√≥n para troubleshooting futuro",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 2
        }
      ]
    },
    {
      "id": 3,
      "title": "Create centralized configuration module",
      "description": "Create centralized configuration for the pipeline module following patterns from module_connector/src/config.py.",
      "details": "Create src/config/settings.py following the configuration patterns established in module_connector/src/config.py. Include environment variables for Groq API, Supabase, processing parameters, and pipeline-specific settings. Maintain consistency with other modules while adapting for pipeline needs: GROQ_API_KEY, MODEL_ID, SUPABASE_URL, SUPABASE_KEY, LOG_LEVEL, etc. Include validation and default values.",
      "testStrategy": "Test that configuration loads correctly from environment variables and provides proper defaults.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Implement Pydantic models for input data",
      "description": "Create Pydantic models for FragmentoProcesableItem in the models/entrada.py file, using ArticuloInItem from module_connector as a reference.",
      "details": "1. Create src/models/entrada.py\n2. Reference ArticuloInItem from module_connector/src/models.py as a pattern\n3. Implement FragmentoProcesableItem model for processing document fragments\n4. Adapt the model to meet specific pipeline needs while maintaining consistency with module_connector patterns\n5. Use Pydantic v2 features for optimal performance and validation\n6. Follow the same validation patterns and nomenclature as in module_connector\n7. Add appropriate field constraints and validators",
      "testStrategy": "Write unit tests to ensure models correctly validate sample input data and raise appropriate validation errors for invalid data. Verify compatibility with the existing ArticuloInItem model from module_connector.",
      "status": "done",
      "dependencies": [
        3
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Pydantic model structure",
          "description": "Create the basic structure of Pydantic models with appropriate class definitions and inheritance patterns",
          "dependencies": [],
          "details": "Define the base model classes and their relationships. Identify which models should inherit from others. Determine the overall structure of your data models, including naming conventions and organization. Include proper type annotations for all attributes.\n<info added on 2025-06-05T02:06:27.163Z>\n```python\n# src/models/entrada.py\n\nfrom typing import Dict, List, Optional, Any\nfrom pydantic import BaseModel\n\nclass FragmentoProcesableItem(BaseModel):\n    \"\"\"\n    Modelo Pydantic que representa un fragmento de documento procesable en el pipeline.\n    Sirve como contrato de datos para las primeras etapas del procesamiento.\n    \"\"\"\n    id_fragmento: str\n    texto_original: str\n    id_articulo_fuente: str\n    orden_en_articulo: Optional[int] = None\n    metadata_adicional: Optional[Dict[str, Any]] = None\n```\n</info added on 2025-06-05T02:06:27.163Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Implement field constraints",
          "description": "Add field-level constraints and validations to the Pydantic models",
          "dependencies": [
            1
          ],
          "details": "For each field in the models, define appropriate constraints such as min/max values, string patterns, length limits, etc. Use Pydantic's Field class to specify constraints. Add descriptive metadata like field descriptions and examples. Consider using Pydantic's specialized field types for common data formats.\n<info added on 2025-06-05T02:07:18.027Z>\nTo enrich the FragmentoProcesableItem model defined in src/models/entrada.py, modify the file to:\n\n1. Import the necessary components:\n```python\nfrom pydantic import BaseModel, Field, constr, conint\nfrom typing import Dict, Optional\n```\n\n2. Update the model definition with field-level constraints:\n```python\nclass FragmentoProcesableItem(BaseModel):\n    id_fragmento: constr(strip_whitespace=True, min_length=1, max_length=255) = Field(\n        ..., \n        description=\"Identificador √∫nico del fragmento, sin espacios al inicio/final y longitud entre 1 y 255 caracteres.\"\n    )\n    texto_original: constr(strip_whitespace=True, min_length=1) = Field(\n        ..., \n        description=\"Contenido textual original del fragmento, no debe estar vac√≠o.\"\n    )\n    id_articulo_fuente: constr(strip_whitespace=True, min_length=1) = Field(\n        ..., \n        description=\"Identificador √∫nico del art√≠culo fuente, sin espacios al inicio/final y no debe estar vac√≠o.\"\n    )\n    orden_en_articulo: Optional[conint(ge=0)] = Field(\n        default=None, \n        description=\"Posici√≥n ordinal del fragmento dentro del art√≠culo fuente, debe ser un entero no negativo si se provee.\"\n    )\n    metadata_adicional: Optional[Dict] = Field(\n        default_factory=dict, \n        description=\"Metadatos adicionales asociados al fragmento en formato de diccionario.\"\n    )\n```\n</info added on 2025-06-05T02:07:18.027Z>",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Create custom validation logic",
          "description": "Implement model-level validation methods for complex business rules",
          "dependencies": [
            1,
            2
          ],
          "details": "Develop validator methods using Pydantic's @validator or @root_validator decorators. Implement cross-field validations that depend on multiple attributes. Create custom error messages for validation failures. Add conditional validation logic based on business requirements. Test the validation logic with various input scenarios.\n<info added on 2025-06-05T02:08:14.033Z>\nImplement custom validation logic for the `FragmentoProcesableItem` model in `src/models/entrada.py` using Pydantic v2's `field_validator` and `model_validator` decorators. These validators will enforce complex business rules involving interdependent fields and conditional logic beyond the individual field constraints defined in subtask 4.2.\n\nKey implementation requirements:\n1. Import `field_validator`, `model_validator`, `ValidationInfo`, `Any`, and `Self` from Pydantic\n2. Create field validators with the signature: `def validator_name(cls, v, info: ValidationInfo): ...`\n3. Implement model validators with appropriate signatures:\n   - For `mode='before'`: `def model_validator_name(cls, data: Any) -> Any:`\n   - For `mode='after'`: `def model_validator_name(self) -> Self:`\n4. Raise `ValueError` with descriptive messages when validation fails\n5. Return the appropriate value from validators:\n   - Field validators: return the validated value\n   - Model validators (before): return the data dictionary\n   - Model validators (after): return the model instance\n\nApply validators to handle complex scenarios such as:\n- Conditional validation based on metadata fields\n- Cross-field validation where one field's validity depends on others\n- Business logic that requires access to multiple model attributes\n\nEnsure validators properly handle optional fields and provide clear, informative error messages that help identify validation failures.\n</info added on 2025-06-05T02:08:14.033Z>",
          "status": "done"
        }
      ]
    },
    {
      "id": 5,
      "title": "Implement Pydantic models for processing",
      "description": "Create Pydantic models for processing stages in the models/procesamiento.py file.",
      "details": "1. Create src/models/procesamiento.py\n2. Implement models: HechoBase, HechoProcesado, EntidadBase, EntidadProcesada, CitaTextual, DatosCuantitativos\n3. Implement result models: ResultadoFase1, ResultadoFase2, ResultadoFase3, ResultadoFase4\n4. Use Pydantic v2 features and add appropriate validators",
      "testStrategy": "Create unit tests to validate the structure and constraints of each model, ensuring they match the PRD specifications.",
      "status": "done",
      "dependencies": [
        4
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Definir Arquitectura de Modelos Base para el Pipeline de Procesamiento",
          "description": "Investigar y definir si se requiere un modelo Pydantic base personalizado (ej. PipelineBaseModel) para todos los modelos en procesamiento.py, o si heredar directamente de pydantic.BaseModel es suficiente. Documentar la decisi√≥n y, si aplica, definir la estructura de este modelo base com√∫n.",
          "details": "<info added on 2025-06-05T02:50:17.403Z>\nDespu√©s de analizar los requisitos del pipeline de procesamiento y los modelos Pydantic que se implementar√°n, he decidido crear una clase base personalizada `PipelineBaseModel` en `src/models/procesamiento.py` para centralizar la configuraci√≥n com√∫n y promover la consistencia entre todos los modelos.\n\n```python\nfrom datetime import datetime\nfrom typing import Optional, ClassVar, Dict, Any\nfrom pydantic import BaseModel, Field, model_validator\n\nclass PipelineBaseModel(BaseModel):\n    \"\"\"\n    Clase base para todos los modelos de procesamiento del pipeline.\n    \n    Esta clase centraliza:\n    - Configuraci√≥n com√∫n a trav√©s de model_config\n    - Campos de auditor√≠a (fecha_creacion, fecha_actualizacion)\n    - M√©todos de utilidad compartidos\n    \n    Justificaci√≥n: Se implementa esta clase base para:\n    1. Garantizar consistencia en la configuraci√≥n de todos los modelos\n    2. Evitar duplicaci√≥n de c√≥digo en campos comunes como timestamps\n    3. Centralizar l√≥gica de validaci√≥n y serializaci√≥n espec√≠fica del pipeline\n    4. Facilitar cambios futuros que afecten a todos los modelos\n    \"\"\"\n    \n    # Campos de auditor√≠a\n    fecha_creacion: datetime = Field(default_factory=datetime.utcnow)\n    fecha_actualizacion: Optional[datetime] = Field(default=None)\n    \n    # Configuraci√≥n com√∫n para todos los modelos del pipeline\n    model_config: ClassVar[Dict[str, Any]] = {\n        \"extra\": \"forbid\",        # Rechazar campos no definidos en el modelo\n        \"validate_assignment\": True,  # Validar asignaciones despu√©s de la inicializaci√≥n\n        \"populate_by_name\": True,     # Permitir poblaci√≥n por nombre de atributo\n        \"json_encoders\": {\n            datetime: lambda dt: dt.isoformat()  # Formato ISO para fechas\n        }\n    }\n    \n    @model_validator(mode='after')\n    def update_modification_timestamp(self):\n        \"\"\"Actualiza el timestamp de modificaci√≥n al validar el modelo.\"\"\"\n        if getattr(self, 'fecha_creacion', None) is not None:\n            self.fecha_actualizacion = datetime.utcnow()\n        return self\n    \n    def touch(self) -> None:\n        \"\"\"Actualiza manualmente el timestamp de modificaci√≥n.\"\"\"\n        self.fecha_actualizacion = datetime.utcnow()\n    \n    def to_supabase_json(self) -> Dict[str, Any]:\n        \"\"\"\n        Serializa el modelo para almacenamiento en Supabase.\n        \n        Convierte el modelo a un diccionario JSON compatible con Supabase,\n        aplicando transformaciones espec√≠ficas si son necesarias.\n        \"\"\"\n        data = self.model_dump(by_alias=True)\n        # Aqu√≠ se pueden aplicar transformaciones espec√≠ficas para Supabase\n        return data\n```\n\nEsta implementaci√≥n proporciona una base s√≥lida para todos los modelos de procesamiento, con campos de auditor√≠a, configuraci√≥n com√∫n y m√©todos de utilidad que ser√°n heredados por las clases derivadas como `HechoBase`, `EntidadBase`, etc.\n</info added on 2025-06-05T02:50:17.403Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 5
        },
        {
          "id": 2,
          "title": "Definir modelos Pydantic fundamentales: HechoBase y EntidadBase",
          "description": "Crear las estructuras para HechoBase y EntidadBase en src/models/procesamiento.py, heredando del modelo base definido en 5.1 (o pydantic.BaseModel).",
          "details": "<info added on 2025-06-05T02:51:12.580Z>\nImplementar las clases Pydantic `HechoBase` y `EntidadBase` en `src/models/procesamiento.py`. Estas clases servir√°n como modelos base para representar hechos y entidades extra√≠dos del contenido de noticias.\n\nPara `HechoBase`:\n- Incluir campos como `id_hecho` (UUID autogenerado), `texto_original_del_hecho`, `confianza_extraccion`, `offset_inicio_hecho`, `offset_fin_hecho` y `metadata_hecho`\n- Aplicar validaciones con `constr` para longitud m√≠nima de texto y `confloat` para rangos de confianza\n\nPara `EntidadBase`:\n- Implementar campos como `id_entidad` (UUID autogenerado), `texto_entidad`, `tipo_entidad`, `relevancia_entidad`, `offset_inicio_entidad`, `offset_fin_entidad` y `metadata_entidad`\n- Aplicar validaciones similares a `HechoBase`\n\nAmbas clases deben heredar de `PipelineBaseModel` (si fue creada en la Subtarea 5.1) o directamente de `pydantic.BaseModel`.\n\nImplementar validadores para garantizar que cuando se proporcionen ambos offsets, el offset_fin sea mayor o igual que el offset_inicio.\n\nAsegurar compatibilidad con serializaci√≥n/deserializaci√≥n y crear pruebas unitarias que verifiquen la creaci√≥n de instancias, valores por defecto, validaciones y manejo de errores.\n</info added on 2025-06-05T02:51:12.580Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 5
        },
        {
          "id": 3,
          "title": "Definir modelos Pydantic derivados: HechoProcesado y EntidadProcesada",
          "description": "Extender HechoBase y EntidadBase para crear HechoProcesado (que podr√≠a incluir campos como id_fragmento_origen, id_articulo_fuente) y EntidadProcesada (que podr√≠a incluir campos como id_entidad_normalizada, uri_wikidata).",
          "details": "<info added on 2025-06-05T02:52:00.987Z>\n# Implementaci√≥n de modelos Pydantic para procesamiento\n\n## Objetivo\nCrear los modelos Pydantic `HechoProcesado` y `EntidadProcesada` en `src/models/procesamiento.py` que extiendan `HechoBase` y `EntidadBase` respectivamente, a√±adiendo campos espec√≠ficos para el procesamiento.\n\n## Estructura de los modelos\n\n### HechoProcesado(HechoBase)\n- `id_fragmento_origen`: UUID del fragmento de origen\n- `id_articulo_fuente`: UUID opcional del art√≠culo original\n- `vinculado_a_entidades`: Lista de IDs de entidades relacionadas\n- `prompt_utilizado`: Prompt opcional usado para extraer el hecho\n- `respuesta_llm_bruta`: Respuesta completa del LLM (como Dict)\n\n### EntidadProcesada(EntidadBase)\n- `id_fragmento_origen`: UUID del fragmento de origen\n- `id_entidad_normalizada`: UUID opcional de la entidad can√≥nica\n- `nombre_entidad_normalizada`: Nombre opcional de la entidad can√≥nica\n- `uri_wikidata`: HttpUrl opcional para la entidad en Wikidata\n- `similitud_normalizacion`: Puntuaci√≥n de similitud (0.0 a 1.0)\n- `prompt_utilizado_normalizacion`: Prompt opcional usado para normalizaci√≥n\n\n## Consideraciones t√©cnicas\n- Importar tipos necesarios: UUID, Optional, List, Dict, Any, Field, HttpUrl\n- Asegurar la trazabilidad mediante el campo `id_fragmento_origen`\n- Campos de normalizaci√≥n opcionales para manejar casos donde no sea exitosa\n- Implementar validadores apropiados (ej. HttpUrl, rangos num√©ricos)\n- Mantener consistencia con la nomenclatura y estilo del proyecto\n\n## Criterios de aceptaci√≥n\n- Herencia correcta de las clases base\n- Definici√≥n completa de campos con tipos y restricciones\n- Compatibilidad con serializaci√≥n/deserializaci√≥n\n- Pruebas unitarias para validar herencia, campos y creaci√≥n de instancias\n</info added on 2025-06-05T02:52:00.987Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 5
        },
        {
          "id": 4,
          "title": "Definir modelos Pydantic de datos espec√≠ficos: CitaTextual y DatosCuantitativos",
          "description": "Crear los modelos CitaTextual (para almacenar citas directas y su contexto) y DatosCuantitativos (para cifras, estad√≠sticas, etc.).",
          "details": "<info added on 2025-06-05T02:52:49.200Z>\n```python\nimport uuid\nfrom uuid import UUID\nfrom typing import Optional, Dict, Any\nfrom pydantic import Field, constr, BaseModel\n\n# Usar PipelineBaseModel si existe, de lo contrario usar BaseModel\n# Asumimos que PipelineBaseModel ya est√° definido en el mismo archivo\n# Si no existe, se debe usar BaseModel directamente\n\nclass CitaTextual(PipelineBaseModel):  # o BaseModel si PipelineBaseModel no existe\n    id_cita: UUID = Field(default_factory=uuid.uuid4, description=\"Identificador √∫nico de la cita textual.\")\n    id_fragmento_origen: UUID = Field(..., description=\"ID del FragmentoProcesableItem del cual se extrajo esta cita.\")\n    texto_cita: constr(min_length=5) = Field(..., description=\"El contenido textual exacto de la cita.\")\n    persona_citada: Optional[str] = Field(default=None, description=\"Nombre de la persona o entidad que realiza la cita.\")\n    id_entidad_citada: Optional[UUID] = Field(default=None, description=\"ID de la EntidadProcesada (persona/organizaci√≥n) que realiza la cita, si est√° identificada.\")\n    offset_inicio_cita: Optional[int] = Field(default=None, description=\"Posici√≥n inicial de la cita en el texto original del fragmento.\", ge=0)\n    offset_fin_cita: Optional[int] = Field(default=None, description=\"Posici√≥n final de la cita en el texto original del fragmento.\", ge=0)\n    contexto_cita: Optional[str] = Field(default=None, description=\"Contexto breve que rodea la cita para mejor entendimiento.\")\n    metadata_cita: Dict[str, Any] = Field(default_factory=dict, description=\"Metadatos adicionales sobre la cita.\")\n\nclass DatosCuantitativos(PipelineBaseModel):  # o BaseModel si PipelineBaseModel no existe\n    id_dato_cuantitativo: UUID = Field(default_factory=uuid.uuid4, description=\"Identificador √∫nico del dato cuantitativo.\")\n    id_fragmento_origen: UUID = Field(..., description=\"ID del FragmentoProcesableItem del cual se extrajo este dato.\")\n    descripcion_dato: constr(min_length=3) = Field(..., description=\"Descripci√≥n del dato cuantitativo (ej: 'N√∫mero de empleados', 'Porcentaje de aumento').\")\n    valor_dato: float = Field(..., description=\"Valor num√©rico del dato.\")\n    unidad_dato: Optional[str] = Field(default=None, description=\"Unidad de medida del dato (ej: 'millones', '%', 'USD').\")\n    fecha_dato: Optional[str] = Field(default=None, description=\"Fecha o per√≠odo al que se refiere el dato (ej: '2023-Q4', 'anual').\")\n    fuente_especifica_dato: Optional[str] = Field(default=None, description=\"Fuente espec√≠fica mencionada para este dato dentro del texto, si la hay.\")\n    offset_inicio_dato: Optional[int] = Field(default=None, description=\"Posici√≥n inicial del dato en el texto original del fragmento.\", ge=0)\n    offset_fin_dato: Optional[int] = Field(default=None, description=\"Posici√≥n final del dato en el texto original del fragmento.\", ge=0)\n    metadata_dato: Dict[str, Any] = Field(default_factory=dict, description=\"Metadatos adicionales sobre el dato cuantitativo.\")\n```\n</info added on 2025-06-05T02:52:49.200Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 5
        },
        {
          "id": 5,
          "title": "Definir modelos Pydantic de resultado de fase: ResultadoFase1Triaje y ResultadoFase2Extraccion",
          "description": "Crear modelos para encapsular los resultados de las dos primeras fases del pipeline.",
          "details": "<info added on 2025-06-05T02:53:37.183Z>\n```python\nimport uuid\nfrom uuid import UUID\nfrom typing import Optional, List, Dict, Any\nfrom pydantic import BaseModel, Field\n\n# Importar modelos existentes (asumiendo que est√°n definidos)\n# Si PipelineBaseModel existe, usarlo como base, de lo contrario usar BaseModel\ntry:\n    from .procesamiento import PipelineBaseModel, HechoProcesado, EntidadProcesada\n    BaseModelClass = PipelineBaseModel\nexcept ImportError:\n    BaseModelClass = BaseModel\n    # Importar HechoProcesado y EntidadProcesada si est√°n en otro m√≥dulo\n    # from .otro_modulo import HechoProcesado, EntidadProcesada\n\nclass ResultadoFase1Triaje(BaseModelClass):\n    id_resultado_triaje: UUID = Field(default_factory=uuid.uuid4, description=\"ID √∫nico del resultado de esta fase de triaje.\")\n    id_fragmento: UUID = Field(..., description=\"ID del FragmentoProcesableItem que fue triado.\")\n    es_relevante: bool = Field(..., description=\"Indica si el fragmento fue considerado relevante por el LLM.\")\n    justificacion_triaje: Optional[str] = Field(default=None, description=\"Explicaci√≥n o justificaci√≥n proporcionada por el LLM para la decisi√≥n de relevancia.\")\n    categoria_principal: Optional[str] = Field(default=None, description=\"Categor√≠a principal asignada al fragmento durante el triaje.\")\n    palabras_clave_triaje: List[str] = Field(default_factory=list, description=\"Lista de palabras clave identificadas en el fragmento durante el triaje.\")\n    confianza_triaje: Optional[float] = Field(default=None, description=\"Nivel de confianza del LLM en la decisi√≥n de triaje (0.0 a 1.0).\", ge=0.0, le=1.0)\n    metadata_triaje: Dict[str, Any] = Field(default_factory=dict, description=\"Metadatos adicionales espec√≠ficos de la fase de triaje.\")\n\nclass ResultadoFase2Extraccion(BaseModelClass):\n    id_resultado_extraccion: UUID = Field(default_factory=uuid.uuid4, description=\"ID √∫nico del resultado de esta fase de extracci√≥n.\")\n    id_fragmento: UUID = Field(..., description=\"ID del FragmentoProcesableItem del cual se extrajeron datos.\")\n    hechos_extraidos: List[HechoProcesado] = Field(default_factory=list, description=\"Lista de hechos procesados extra√≠dos del fragmento.\")\n    entidades_extraidas: List[EntidadProcesada] = Field(default_factory=list, description=\"Lista de entidades procesadas extra√≠das del fragmento.\")\n    resumen_extraccion: Optional[str] = Field(default=None, description=\"Resumen generado por el LLM a partir de la informaci√≥n extra√≠da.\")\n    prompt_extraccion_usado: Optional[str] = Field(default=None, description=\"El prompt espec√≠fico utilizado para la fase de extracci√≥n.\")\n    advertencias_extraccion: List[str] = Field(default_factory=list, description=\"Posibles advertencias o problemas identificados durante la extracci√≥n.\")\n    metadata_extraccion: Dict[str, Any] = Field(default_factory=dict, description=\"Metadatos adicionales espec√≠ficos de la fase de extracci√≥n.\")\n```\n</info added on 2025-06-05T02:53:37.183Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 5
        },
        {
          "id": 6,
          "title": "Definir modelos Pydantic de resultado de fase: ResultadoFase3CitasDatos y ResultadoFase4Normalizacion",
          "description": "Crear modelos para encapsular los resultados de las dos √∫ltimas fases del pipeline.",
          "details": "<info added on 2025-06-05T02:54:20.348Z>\nimport uuid\nfrom typing import List, Dict, Any, Optional\nfrom uuid import UUID\nfrom pydantic import Field, BaseModel\n\n# Asumiendo que estas clases ya existen en el archivo\n# from src.models.procesamiento import PipelineBaseModel, CitaTextual, DatosCuantitativos, EntidadProcesada\n# from src.models.entrada import FragmentoProcesableItem\n\n# Si existe PipelineBaseModel, usar esa como base, de lo contrario usar BaseModel\n# Para este ejemplo, usar√© BaseModel directamente\n\nclass ResultadoFase3CitasDatos(BaseModel):\n    id_resultado_citas_datos: UUID = Field(default_factory=uuid.uuid4, description=\"ID √∫nico del resultado de esta fase de citas y datos.\")\n    id_fragmento: UUID = Field(..., description=\"ID del FragmentoProcesableItem procesado.\")\n    citas_textuales_extraidas: List[CitaTextual] = Field(default_factory=list, description=\"Lista de citas textuales identificadas en el fragmento.\")\n    datos_cuantitativos_extraidos: List[DatosCuantitativos] = Field(default_factory=list, description=\"Lista de datos cuantitativos identificados en el fragmento.\")\n    prompt_citas_datos_usado: Optional[str] = Field(default=None, description=\"Prompt espec√≠fico utilizado para la extracci√≥n de citas y datos.\")\n    advertencias_citas_datos: List[str] = Field(default_factory=list, description=\"Posibles advertencias durante la extracci√≥n de citas y datos.\")\n    metadata_citas_datos: Dict[str, Any] = Field(default_factory=dict, description=\"Metadatos adicionales de la fase de citas y datos.\")\n\nclass ResultadoFase4Normalizacion(BaseModel):\n    id_resultado_normalizacion: UUID = Field(default_factory=uuid.uuid4, description=\"ID √∫nico del resultado de esta fase de normalizaci√≥n.\")\n    id_fragmento: UUID = Field(..., description=\"ID del FragmentoProcesableItem cuyas entidades fueron normalizadas.\")\n    entidades_normalizadas: List[EntidadProcesada] = Field(default_factory=list, description=\"Lista de entidades procesadas que ahora incluyen informaci√≥n de normalizaci√≥n.\")\n    resumen_normalizacion: Optional[str] = Field(default=None, description=\"Resumen del proceso de normalizaci√≥n para este fragmento.\")\n    prompt_normalizacion_usado: Optional[str] = Field(default=None, description=\"Prompt espec√≠fico utilizado para la fase de normalizaci√≥n (si aplica).\")\n    estado_general_normalizacion: str = Field(..., description=\"Estado general del proceso de normalizaci√≥n (ej: 'Completo', 'Parcial', 'Fallido', 'No Requerido').\")\n    metadata_normalizacion: Dict[str, Any] = Field(default_factory=dict, description=\"Metadatos adicionales de la fase de normalizaci√≥n.\")\n</info added on 2025-06-05T02:54:20.348Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 5
        }
      ]
    },
    {
      "id": 6,
      "title": "Implement Pydantic models for persistence",
      "description": "Create Pydantic models for constructing JSONB payloads required by the existing Supabase RPCs.",
      "details": "1. Create src/models/persistencia.py\n2. Implement Pydantic models that represent the JSONB payload structures expected by the existing Supabase RPCs\n3. Focus on creating models for insertar_articulo_completo() and insertar_fragmento_completo() RPCs\n4. No need to map all database tables - just create the payload structures",
      "testStrategy": "Write unit tests to verify that the models can serialize and deserialize sample data correctly, matching the expected JSONB payload structure required by the Supabase RPCs.",
      "status": "done",
      "dependencies": [
        5
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Set up FastAPI application structure",
      "description": "Create the main FastAPI application structure in src/main.py following the architectural patterns observed in module_connector.",
      "details": "1. Create src/main.py following the structure observed in module_connector\n2. Import FastAPI and necessary dependencies\n3. Initialize FastAPI app with proper async/await patterns\n4. Set up CORS middleware if required\n5. Implement configuration handling by integrating with the centralized configuration from Task 3\n6. Include robust error handling and logging setup\n7. Implement the health check endpoint GET /health\n8. Ensure the application structure is consistent with project patterns\n9. Load environment variables and settings using the centralized configuration",
      "testStrategy": "1. Run the FastAPI application and test the health check endpoint using a tool like curl or Postman\n2. Verify that the application structure follows the patterns in module_connector\n3. Test async/await functionality works correctly\n4. Verify that the application correctly loads configuration from the centralized configuration system",
      "status": "done",
      "dependencies": [
        2,
        3
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Implement Groq API integration",
      "description": "Create the Groq API service integration in src/services/groq_service.py, using the centralized configuration for API keys and following the module_connector pattern.",
      "details": "1. Create src/services/groq_service.py\n2. Implement GroqService class using groq==0.26.0 SDK\n3. Use the centralized configuration (Task 3) to load GROQ_API_KEY and other settings\n4. Follow the module_connector pattern for HTTP/API integration\n5. Implement methods for initializing the client with API key and model ID\n6. Implement method for sending prompts and receiving responses\n7. Add retry logic using tenacity with exponential backoff\n8. Implement proper timeout handling following module_connector pattern\n9. Implement error handling for timeouts, rate limits, and malformed responses\n10. Use loguru for structured logging of API interactions",
      "testStrategy": "Write unit tests mocking the Groq API responses. Test error handling and retry logic with simulated failures. Ensure tests verify proper loading of configuration from the centralized config system.",
      "status": "done",
      "dependencies": [
        2,
        3
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Implement Supabase integration",
      "description": "Create a Supabase client service in src/services/supabase_service.py following the architectural pattern from module_scraper/scraper_core/utils/supabase_client.py to consume existing RPCs.",
      "details": "1. Create src/services/supabase_service.py\n2. Use module_scraper/scraper_core/utils/supabase_client.py as a reference model\n3. Implement a SupabaseService class using supabase==2.15.2 with Singleton pattern\n4. Add methods for initializing the client with URL and API key\n5. Implement client methods to call the following existing RPCs specific to the pipeline:\n   - insertar_articulo_completo()\n   - insertar_fragmento_completo()\n   - buscar_entidad_similar()\n6. Maintain the same error handling patterns as in the reference implementation\n7. Add basic retry logic using tenacity library\n8. Implement consistent logging approach\n9. Focus on architectural consistency with the reference implementation while adapting for pipeline-specific RPCs",
      "testStrategy": "Create integration tests with the production Supabase instance. Verify correct RPC calls, Singleton pattern implementation, retry logic, and error handling consistent with the reference implementation.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Implement error handling utilities",
      "description": "Create error handling utilities in src/utils/error_handling.py, following the patterns established in module_connector/src/main.py.",
      "details": "1. Create src/utils/error_handling.py\n2. Implement custom exception classes for different error types (e.g., APIError, DatabaseError, GroqAPIError, SupabaseRPCError)\n3. Create utility functions for standardized error responses\n4. Implement a global exception handler for FastAPI\n5. Implement retry logic using tenacity library with exponential backoff strategy\n6. Create decorators for common retry patterns (e.g., @retry_with_backoff)\n7. Add specific exception handling for Groq API calls\n8. Add specific exception handling for Supabase RPC calls\n9. Ensure architectural consistency with the module_connector container",
      "testStrategy": "Write unit tests for each custom exception and utility function. Test retry logic with mocked services that fail intermittently. Ensure the global exception handler catches and formats errors correctly. Verify that backoff strategies work as expected under various failure scenarios.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "medium",
      "subtasks": [
        {
          "id": 1,
          "title": "Create basic custom exception classes",
          "description": "Define the core custom exception classes needed for the pipeline following the documentation in docs/07-manejo-de-errores.md.",
          "dependencies": [],
          "details": "Implement BaseAppException as the root exception and derived exceptions like ValidationError, ExternalAPIError, DataProcessingError, etc. Each exception should have appropriate attributes for error codes, messages, and context data. Follow the simplicity principle by keeping the exception hierarchy flat and focused.",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Implement standardized error response utilities",
          "description": "Create utility functions that convert exceptions into standardized API responses.",
          "dependencies": [
            1
          ],
          "details": "Develop functions that transform exceptions into consistent JSON response objects with appropriate HTTP status codes, error messages, and error codes. Include utilities for both synchronous and asynchronous contexts. Ensure the response format follows the documentation guidelines with fields like 'error_code', 'message', and optional 'details'.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Implement retry decorators with tenacity",
          "description": "Create retry decorators using the tenacity library for handling transient failures.",
          "dependencies": [
            1
          ],
          "details": "Implement decorators that can be applied to functions interacting with external services. Configure appropriate retry strategies with exponential backoff, jitter, and maximum retry attempts. Create specific decorators for different types of operations (e.g., database operations, API calls) with appropriate timeout settings as specified in the documentation.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Implement global exception handler for FastAPI",
          "description": "Create a global exception handler that integrates with FastAPI's exception handling system.",
          "dependencies": [
            1,
            2
          ],
          "details": "Develop exception handlers that catch both custom exceptions and standard exceptions, converting them to appropriate HTTP responses. Register these handlers with FastAPI's exception_handler decorator. Ensure proper logging of exceptions and implement different handling strategies based on exception types.",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Implement pipeline-specific fallback functions",
          "description": "Create fallback mechanisms for each phase of the pipeline to handle specific failure scenarios.",
          "dependencies": [
            1,
            3
          ],
          "details": "Develop fallback functions for Groq API failures, Supabase operations, and other critical pipeline components. Implement graceful degradation strategies that allow the system to continue operation with reduced functionality when specific components fail. Follow the documentation guidelines for implementing simple but robust fallback mechanisms.",
          "status": "done"
        },
        {
          "id": 6,
          "title": "Create basic unit tests for error handling system",
          "description": "Develop unit tests to verify the functionality of the error handling components.",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Create test cases for custom exceptions, error response utilities, retry decorators, and fallback functions. Include tests for the global exception handler using FastAPI's TestClient. Verify that exceptions are properly caught, transformed into appropriate responses, and that retry logic works as expected under different failure scenarios.",
          "status": "done"
        }
      ]
    },
    {
      "id": 11,
      "title": "Set up logging configuration",
      "description": "Configure structured logging using loguru (already in requirements.txt) for pipeline-specific needs with focus on request traceability, LLM debugging, and performance monitoring, while drawing inspiration from the existing module_scraper/scraper_core/logging_config.py architecture.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "medium",
      "details": "Implement a robust logging system for the pipeline that enables request traceability, supports different environments, and facilitates debugging:\n\n1. Create src/utils/logging_config.py inspired by the architecture in module_scraper/scraper_core/logging_config.py but leveraging loguru's capabilities\n2. Configure environment-specific settings:\n   - DEBUG for development, INFO for staging, WARNING for production\n   - Appropriate output formats and destinations per environment\n3. Implement key pipeline logging features:\n   - Request correlation with unique request_id for end-to-end traceability\n   - Component and phase-specific logging contexts\n   - Basic performance metrics tracking\n4. Set up log rotation and retention with daily files and configurable retention periods\n5. Create utility functions for common logging patterns:\n   - Execution time tracking\n   - Sensitive data sanitization\n   - Error handling integration\n6. Ensure compatibility with FastAPI/Uvicorn and async operations",
      "testStrategy": "Test the logging system to verify it meets the key requirements:\n1. Verify request traceability across pipeline components\n2. Test environment-specific configurations (development, staging, production)\n3. Confirm log rotation and retention functionality\n4. Ensure sensitive data is properly sanitized\n5. Verify compatibility with FastAPI and async functions\n6. Test integration with error handling",
      "subtasks": [
        {
          "id": 1,
          "title": "Analyze existing logging_config.py",
          "description": "Review and analyze the existing logging_config.py in module_scraper to understand the current implementation and identify reusable components.",
          "dependencies": [],
          "details": "Document the current logging structure, configuration parameters, handlers, formatters, and any environment variable integrations. Identify strengths and limitations of the current implementation for pipeline use cases.\n<info added on 2025-06-06T02:41:00.507Z>\n# Logging System Analysis Results\n\n## Current Structure Analysis\nThe existing logging_config.py in module_scraper implements a modular architecture through the LoggingConfig class with static methods for configuration. The system supports multiple environments (development, staging, production, test) with environment-specific settings.\n\n## Configuration Features\n- **Multiple format support**: detailed, simple, json, and production formats controlled via LOG_FORMAT_TYPE\n- **Structured logging**: JSON formatting and custom format templates\n- **Component-specific configuration**: Dedicated mapping for different logger components\n- **Date-based log rotation**: Files organized as {component}_{date}.log\n- **Environment variable controls**: LOG_LEVEL, LOG_FORMAT_TYPE, LOG_TO_FILE, LOG_FILE_ENCODING\n\n## Reusable Components\n- Environment-based logging level system\n- Multiple format options with environment variable override capability\n- Organized log directory structure\n- Component-specific configuration framework\n- Configurable encoding settings\n\n## Required Adaptations for Pipeline Integration\n- Remove Scrapy-specific dependencies\n- Add pipeline phase-specific logging\n- Adapt component configuration for pipeline modules\n- Simplify configuration while maintaining flexibility\n</info added on 2025-06-06T02:41:00.507Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Implement basic logging setup with environment configuration",
          "description": "Create a basic logging configuration using loguru that can be adjusted based on environment variables for different deployment scenarios.",
          "dependencies": [
            1
          ],
          "details": "Create logging_config.py that configures loguru according to the environment:\n- Set up environment-specific log levels (DEBUG for development, INFO for staging, WARNING for production)\n- Configure appropriate output formats (console for development, structured files for production)\n- Integrate with existing environment variables\n- Ensure compatibility with FastAPI/Uvicorn\n- Create a setup_logging() function that initializes the logging system",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Develop pipeline-specific logging features with loguru",
          "description": "Implement logging features specific to pipeline phases and components using loguru's context binding to enable better tracking and debugging of pipeline execution.",
          "dependencies": [
            2
          ],
          "details": "Implement pipeline-specific logging features:\n- Request correlation mechanism to track requests across all pipeline components\n- Component and phase-specific logging contexts\n- Basic metrics tracking (timing, success/failure)\n- Support for LLM interaction logging\n- Supabase operation logging\n- Ensure each pipeline component has appropriate logging context",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Configure log rotation and retention with loguru",
          "description": "Implement log rotation and retention mechanisms using loguru's built-in capabilities to manage log file growth and storage requirements.",
          "dependencies": [
            2
          ],
          "details": "Set up log file management:\n- Configure daily log rotation using loguru's capabilities\n- Implement environment-specific retention periods\n- Organize logs by environment and component\n- Ensure proper file permissions and encoding",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Create utility functions for standardized logging with loguru",
          "description": "Develop utility functions that provide standardized logging patterns using loguru's features for common scenarios across the pipeline.",
          "dependencies": [
            3
          ],
          "details": "Create helper functions for common logging patterns:\n- Execution time tracking for functions and operations\n- Sensitive data sanitization to prevent logging API keys or personal information\n- Error handling integration\n- Context managers for phase logging\n- Ensure compatibility with async operations",
          "status": "done"
        },
        {
          "id": 6,
          "title": "Test loguru-based logging system functionality",
          "description": "Create and execute tests to verify the loguru-based logging system works correctly across different scenarios and configurations.",
          "dependencies": [
            2,
            3,
            4,
            5
          ],
          "details": "Test the logging system to verify key requirements:\n- Request traceability across pipeline components\n- Environment-specific configurations\n- Log rotation and retention functionality\n- Sensitive data sanitization\n- Compatibility with FastAPI and async functions\n- Integration with error handling",
          "status": "done"
        }
      ]
    },
    {
      "id": 12,
      "title": "Implement prompt loader utility",
      "description": "Create a utility for loading external prompt files in src/utils/prompt_loader.py following established project patterns for modularity, file handling, encoding, and error management.",
      "details": "1. Create src/utils/prompt_loader.py following the modular patterns observed in other project modules\n2. Implement a function to load prompt files from a specified directory, maintaining consistency with existing file handling patterns\n3. Use the same encoding standards for reading files as established in the project\n4. Implement error handling that follows project conventions for missing or unreadable prompt files\n5. Add caching mechanism to avoid repeated disk reads\n6. Ensure the module structure and API design aligns with other utility modules in the project\n7. Create utility functions for prompt templating if required, following project patterns",
      "testStrategy": "Write unit tests with sample prompt files. Test caching behavior and error handling for missing files. Ensure tests follow the same patterns as other utility module tests in the project.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Implement Entity Normalizer service",
      "description": "Create entity normalization service using the existing buscar_entidad_similar RPC.",
      "details": "Create src/services/entity_normalizer.py with NormalizadorEntidades class that uses the existing buscar_entidad_similar RPC to normalize entities during Phase 4. Include logic for matching thresholds, handling new vs existing entities, and preparing normalized entity data for persistence. This replaces the complex entity cache management that was planned.",
      "testStrategy": "Create integration tests with mock RPC responses to verify entity normalization logic, similarity thresholds, and proper handling of different matching scenarios.",
      "status": "done",
      "dependencies": [
        9
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 14,
      "title": "Implement PayloadBuilder service",
      "description": "Create a PayloadBuilder service to construct JSONB payloads for Supabase RPCs from pipeline results.",
      "details": "Create src/services/payload_builder.py with PayloadBuilder class that takes pipeline results and constructs the exact JSONB structure expected by insertar_articulo_completo() and insertar_fragmento_completo() RPCs. Include methods for serializing hechos, entidades, citas, datos, and relaciones. Add proper error handling and validation.",
      "testStrategy": "Create unit tests to verify correct payload construction for both article and fragment RPCs. Test serialization of different data types and proper reference handling between objects.",
      "status": "done",
      "dependencies": [
        6
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Implement Phase 1: Preprocessing and Triage",
      "description": "Create the first phase of the processing pipeline in src/pipeline/fase_1_triaje.py.",
      "details": "1. Create src/pipeline/fase_1_triaje.py\n2. Implement ejecutar_fase_1() function\n3. Use spaCy for text cleaning and language detection\n4. Integrate with Groq API using the prompt from 'Prompt_1_filtrado.md'\n5. Implement logic for relevance evaluation and translation if necessary\n6. Return ResultadoFase1 object with clean/translated text and initial evaluation",
      "testStrategy": "Write unit tests with various input texts. Test language detection, cleaning, and relevance evaluation. Mock Groq API calls for testing.",
      "status": "done",
      "dependencies": [
        5,
        8,
        12
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Create Basic Structure for fase_1_triaje.py",
          "description": "Set up the initial file structure for the Phase 1 triage module with necessary imports and the ResultadoFase1 class definition.",
          "dependencies": [],
          "details": "Create src/pipeline/fase_1_triaje.py file with imports for spaCy, Groq API client, and other required libraries. Define the ResultadoFase1 class with appropriate attributes: texto_limpio, idioma_detectado, es_relevante, puntuacion_relevancia, and texto_traducido.",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Implement Text Cleaning and Language Detection",
          "description": "Develop functions for text preprocessing and language detection using spaCy.",
          "dependencies": [
            1
          ],
          "details": "Create functions for text cleaning (removing unnecessary whitespace, special characters, etc.) and language detection. Use spaCy's language models to identify the text language. Ensure proper handling of edge cases like very short texts or mixed language content.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Integrate Groq API for Relevance Evaluation",
          "description": "Implement the connection to Groq API and use the prompt from Prompt_1_filtrado.md to evaluate text relevance.",
          "dependencies": [
            1
          ],
          "details": "Create a function to connect to the Groq API, send the cleaned text with the prompt from Prompt_1_filtrado.md, and parse the response. Implement error handling for API failures and response parsing issues. Extract relevance score and decision from the API response.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Implement Translation Logic",
          "description": "Add functionality to translate non-Spanish texts to Spanish when needed.",
          "dependencies": [
            2
          ],
          "details": "Create a translation function that checks if the detected language is not Spanish and translates the text accordingly. Consider using a translation API or library. Implement proper error handling and fallback mechanisms for translation failures.",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Implement ejecutar_fase_1() Function",
          "description": "Create the main function that orchestrates the entire Phase 1 process and returns a ResultadoFase1 object.",
          "dependencies": [
            2,
            3,
            4
          ],
          "details": "Implement the ejecutar_fase_1() function that takes input text, processes it through cleaning, language detection, relevance evaluation, and translation if necessary. Assemble and return a ResultadoFase1 object with all the required information. Include appropriate logging and error handling throughout the process.",
          "status": "done"
        }
      ]
    },
    {
      "id": 16,
      "title": "Implement Phase 2: Basic Element Extraction",
      "description": "Create the second phase of the processing pipeline in src/pipeline/fase_2_extraccion.py.",
      "details": "1. Create src/pipeline/fase_2_extraccion.py\n2. Implement ejecutar_fase_2() function\n3. Integrate with Groq API using the prompt from 'Prompt_2_elementos_basicos.md'\n4. Extract main facts and mentioned entities\n5. Assign temporary IDs to HechoBase and EntidadBase objects\n6. Return ResultadoFase2 object with lists of extracted facts and entities",
      "testStrategy": "Create unit tests with sample preprocessed text. Verify correct extraction of facts and entities. Test ID assignment logic.",
      "status": "done",
      "dependencies": [
        5,
        8,
        12
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Create basic structure and ResultadoFase2 class definition",
          "description": "Implement the basic structure of fase_2_extraccion.py following the pattern established in fase_1_triaje.py, and define the ResultadoFase2 Pydantic class.",
          "dependencies": [],
          "details": "Review fase_1_triaje.py for structural patterns. Create fase_2_extraccion.py with necessary imports. Define ResultadoFase2 Pydantic class with fields for extracted facts and entities. Include appropriate type hints and validation. Ensure the class structure aligns with the expected output format for Phase 2.\n<info added on 2025-06-06T03:02:43.995Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `src/pipeline/fase_1_triaje.py` - Para seguir el patr√≥n estructural establecido (imports, organizaci√≥n de funciones, manejo de errores)\n- `src/models/procesamiento.py` - Para entender `ResultadoFase2Extraccion` ya definido y sus campos\n- `prompts/Prompt_2_elementos_basicos.md` - Para entender el formato de salida JSON esperado\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Pydantic**: BaseModel inheritance patterns, field validation, model_config best practices\n- **Python typing**: Type hints con UUID, Optional, List, Dict\n- **Python pathlib**: Manejo de rutas de archivos para imports relativos\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Crear archivo `src/pipeline/fase_2_extraccion.py` siguiendo estructura de fase_1_triaje.py\n- Importar dependencias necesarias desde `..models.procesamiento`, servicios de Groq, utils de error handling\n- Definir funci√≥n `ejecutar_fase_2()` con signatura similar a `ejecutar_fase_1()`\n- Crear funciones helper privadas (_funci√≥n_privada) para cada etapa del proceso\n- NOTA: `ResultadoFase2Extraccion` ya existe en procesamiento.py, no crear duplicado\n</info added on 2025-06-06T03:02:43.995Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Implement Groq API integration with specific prompt",
          "description": "Set up the connection to Groq API and implement the function to send the existing prompt for fact and entity extraction.",
          "dependencies": [
            1
          ],
          "details": "Review existing Groq API integration code. Implement a function to connect to Groq API using the provided credentials. Set up the specific prompt for fact and entity extraction. Handle API errors and rate limiting appropriately. Ensure the prompt engineering is optimized for extracting structured data from unstructured text.\n<info added on 2025-06-06T03:03:09.752Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `src/services/groq_service.py` - Para entender el servicio Groq existente y c√≥mo usarlo\n- `src/utils/prompt_loader.py` - Para cargar plantillas de prompt desde archivos externos\n- `src/pipeline/fase_1_triaje.py` - Funciones `_llamar_groq_api_triaje()` y `_get_groq_config()` como referencia\n- `prompts/Prompt_2_elementos_basicos.md` - Para entender las variables a formatear en el prompt\n- `src/utils/error_handling.py` - Para usar decoradores de retry y manejo de errores\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Groq SDK**: Client initialization, authentication, error handling, rate limits, timeout configuration\n- **Python json module**: JSON parsing, handling malformed responses, json.loads() best practices\n- **Tenacity library**: Retry patterns con exponential backoff, stop conditions, wait strategies\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Crear funci√≥n `_llamar_groq_api_extraccion()` siguiendo patr√≥n de fase_1_triaje.py\n- Usar prompt_loader para cargar `Prompt_2_elementos_basicos.md`\n- Formatear prompt con variables: {{TITULO_O_DOCUMENTO}}, {{FUENTE_O_TIPO}}, {{PAIS_ORIGEN}}, {{FECHA_FUENTE}}, {{CONTENIDO}}\n- Implementar retry logic usando decorador `@retry_groq_api()` existente\n- Manejar errores espec√≠ficos: timeout, rate limit, API key missing, malformed response\n- Configurar par√°metros Groq: model_id, temperature, max_tokens apropiados para extracci√≥n\n</info added on 2025-06-06T03:03:09.752Z>",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Develop fact extraction logic",
          "description": "Implement the logic to parse and extract facts from the Groq API response and convert them to the required Pydantic model format.",
          "dependencies": [
            2
          ],
          "details": "Create a function to parse the JSON response from Groq API. Extract fact data according to the defined schema. Implement validation to ensure extracted facts meet the required format. Create mapping functions to convert raw API response to Pydantic models. Handle edge cases such as missing or malformed data in the API response.\n<info added on 2025-06-06T03:03:35.987Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `prompts/Prompt_2_elementos_basicos.md` - Para entender la estructura exacta de hechos en el JSON de respuesta\n- `src/models/procesamiento.py` - Modelos `HechoBase` y `HechoProcesado`, campos requeridos y validaciones\n- `src/pipeline/fase_1_triaje.py` - Funci√≥n `_parsear_respuesta_triaje()` como referencia de parsing\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Pydantic**: Model validation, field constraints, model creation from dict, validation errors\n- **Python datetime**: Parsing dates en diferentes formatos (YYYY-MM-DD), timezone handling\n- **Python json**: Safe JSON parsing, handling nested objects, KeyError management\n- **Python UUID**: uuid4() generation para IDs temporales\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Crear funci√≥n `_parsear_hechos_from_json(response_json, id_fragmento_origen)`\n- Mapear campos JSON a modelo Pydantic:\n  * `id` (del JSON) ‚Üí generar UUID temporal nuevo con uuid4()\n  * `contenido` ‚Üí `texto_original_del_hecho`\n  * `fecha.inicio`, `fecha.fin` ‚Üí almacenar en `metadata_hecho`\n  * `tipo_hecho`, `pais`, `region`, `ciudad`, `es_futuro`, `estado_programacion` ‚Üí `metadata_hecho`\n  * `precision_temporal` ‚Üí `metadata_hecho`\n- Asignar `confianza_extraccion` por defecto (ej: 0.8)\n- Crear instancias de `HechoProcesado` con `id_fragmento_origen` asignado\n- Manejar casos edge: hechos con campos faltantes, fechas malformadas, arrays vac√≠os\n</info added on 2025-06-06T03:03:35.987Z>",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Develop entity extraction logic",
          "description": "Implement the logic to parse and extract entities from the Groq API response and convert them to the required Pydantic model format.",
          "dependencies": [
            2
          ],
          "details": "Create a function to parse entities from the Groq API response. Implement entity categorization according to the defined schema. Ensure proper validation of extracted entities. Create mapping functions to convert raw entity data to Pydantic models. Handle edge cases such as duplicate entities or entities with missing attributes.\n<info added on 2025-06-06T03:04:09.764Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `prompts/Prompt_2_elementos_basicos.md` - Para entender la estructura exacta de entidades en el JSON de respuesta\n- `src/models/procesamiento.py` - Modelos `EntidadBase` y `EntidadProcesada`, campos requeridos y validaciones\n- Mapping de tipos: PERSONA, ORGANIZACION, INSTITUCION, LUGAR, EVENTO, NORMATIVA, CONCEPTO\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Pydantic**: String constraints (constr), field validation, model creation from dict\n- **Pydantic HttpUrl**: URL validation para campos como uri_wikidata\n- **Python json**: Handling arrays, optional fields, null values\n- **Python UUID**: uuid4() generation para IDs temporales\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Crear funci√≥n `_parsear_entidades_from_json(response_json, id_fragmento_origen)`\n- Mapear campos JSON a modelo Pydantic:\n  * `id` (del JSON) ‚Üí generar UUID temporal nuevo con uuid4()\n  * `nombre` ‚Üí `texto_entidad`\n  * `tipo` ‚Üí `tipo_entidad` (validar contra tipos permitidos)\n  * `descripcion`, `alias`, `fecha_nacimiento`, `fecha_disolucion` ‚Üí `metadata_entidad`\n- Asignar `relevancia_entidad` por defecto (ej: 0.7)\n- Crear instancias de `EntidadProcesada` con `id_fragmento_origen` asignado\n- Manejar casos edge: entidades duplicadas, tipos de entidad inv√°lidos, fechas en formato incorrecto\n- Validar que `alias` sea array de strings, manejar valores null correctamente\n- Procesar campo `descripcion` que viene con guiones separados\n</info added on 2025-06-06T03:04:09.764Z>",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Implement temporary ID assignment and result assembly",
          "description": "Develop the logic to assign temporary IDs to facts and entities, and assemble the final ResultadoFase2 object.",
          "dependencies": [
            3,
            4
          ],
          "details": "Create a function to assign unique temporary IDs to extracted facts and entities. Implement the main processing function that orchestrates the entire extraction process. Assemble the final ResultadoFase2 object with all extracted and processed data. Add basic tests to verify the correctness of the extraction process. Document the implementation with appropriate comments and docstrings.\n<info added on 2025-06-06T03:04:39.888Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `src/models/procesamiento.py` - Modelo `ResultadoFase2Extraccion` completo y sus campos requeridos\n- `src/pipeline/fase_1_triaje.py` - Funci√≥n `ejecutar_fase_1()` para patr√≥n de ensamblaje final y manejo de errores\n- `src/utils/error_handling.py` - Patterns de manejo de errores y fallbacks para fase 2\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Python UUID**: uuid4() generation, UUID serialization, uniqueness guarantees\n- **Loguru**: Structured logging patterns, log levels (info, error, debug), contextual information\n- **Python Exception handling**: Custom exceptions, error propagation, try-catch patterns\n- **Pydantic**: Model validation, .touch() method, timestamp handling\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Implementar funci√≥n principal `ejecutar_fase_2(resultado_fase_1: ResultadoFase1Triaje) -> ResultadoFase2Extraccion`\n- Usar `resultado_fase_1.texto_para_siguiente_fase` como input para Groq API\n- Generar UUIDs √∫nicos para hechos y entidades usando `uuid4()`\n- Ensamblar `ResultadoFase2Extraccion`:\n  * `id_fragmento` desde `resultado_fase_1.id_fragmento`\n  * `hechos_extraidos` y `entidades_extraidas` desde funciones parser\n  * `prompt_extraccion_usado` con prompt formateado usado\n  * `metadata_extraccion` con info t√©cnica (tokens, duraci√≥n, modelo usado)\n- Llamar `.touch()` para actualizar timestamps antes de retornar\n- Manejo de errores con fallbacks apropiados usando error_handling existente\n- Logging estructurado en puntos clave: inicio, √©xito, errores\n- Validar que todas las listas no sean None antes de asignar a resultado\n</info added on 2025-06-06T03:04:39.888Z>",
          "status": "done"
        }
      ]
    },
    {
      "id": 17,
      "title": "Implement Phase 3: Quote and Quantitative Data Extraction",
      "description": "Create the third phase of the processing pipeline in src/pipeline/fase_3_citas_datos.py.",
      "status": "done",
      "dependencies": [
        5,
        8,
        12
      ],
      "priority": "high",
      "details": "1. Create src/pipeline/fase_3_citas_datos.py\n2. Implement ejecutar_fase_3() function that receives a FragmentProcessor parameter\n3. Integrate with Groq API using the prompt from 'Prompt_3_citas_datos.md'\n4. Extract direct textual quotes and structured numerical data\n5. Create CitaTextual and DatosCuantitativos objects with references using MetadatosCita and MetadatosDato models\n6. Maintain sequential IDs from phase 2 (don't generate UUIDs)\n7. Return ResultadoFase3 object with lists of extracted quotes and quantitative data",
      "testStrategy": "Develop unit tests with text containing quotes and numerical data. Verify accurate extraction and reference assignment. Ensure IDs are maintained consistently between phases and cross-references are valid.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create basic structure and ResultadoFase3 class definition",
          "description": "Analyze inputs/outputs and create the basic structure for Phase 3 following the pattern of previous phases, including the ResultadoFase3CitasDatos class definition.",
          "dependencies": [],
          "details": "Review the structure of Phase 1 and 2 implementations to maintain consistency. Define the ResultadoFase3CitasDatos class with appropriate properties to store quotes and quantitative data. Include proper typing for all properties and implement necessary constructors. Ensure the class follows the same patterns as previous phase result classes for consistency.\n<info added on 2025-06-06T03:19:12.978Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `src/pipeline/fase_1_triaje.py` y `src/pipeline/fase_2_extraccion.py` - Para seguir el patr√≥n estructural establecido\n- `src/models/procesamiento.py` - Para entender `ResultadoFase3CitasDatos`, `CitaTextual` y `DatosCuantitativos` ya definidos\n- `prompts/Prompt_3_citas_datos.md` - Para entender el formato de input (JSON_PASO_1) y output esperado\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Pydantic**: BaseModel inheritance patterns, field validation, UUID handling\n- **Python typing**: Type hints con List, Optional, Dict para citas y datos cuantitativos\n- **Python pathlib**: Manejo de rutas para imports relativos\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Crear archivo `src/pipeline/fase_3_citas_datos.py` siguiendo estructura de fases anteriores\n- Importar dependencias necesarias: `CitaTextual`, `DatosCuantitativos`, `ResultadoFase3CitasDatos` desde models\n- Definir funci√≥n `ejecutar_fase_3(resultado_fase_2: ResultadoFase2Extraccion)` \n- IMPORTANTE: Esta fase necesita el JSON de la fase anterior como input para el prompt\n- Crear funciones helper privadas para cada etapa: `_formatear_json_paso_1()`, `_parsear_citas()`, `_parsear_datos_cuantitativos()`\n- NOTA: `ResultadoFase3CitasDatos`, `CitaTextual` y `DatosCuantitativos` ya est√°n definidos en procesamiento.py\n</info added on 2025-06-06T03:19:12.978Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Implement Groq API integration with specific prompt",
          "description": "Integrate with Groq API using the Prompt_3_citas_datos.md file that requires JSON_PASO_1 as input.",
          "dependencies": [
            1
          ],
          "details": "Create a service method to call Groq API with the specific prompt from Prompt_3_citas_datos.md. Ensure the method accepts JSON_PASO_1 as input and properly formats it for the API call. Handle API responses and errors appropriately. Document the API integration process and any configuration requirements.\n<info added on 2025-06-06T03:19:42.284Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `prompts/Prompt_3_citas_datos.md` - Para entender el formato del prompt que requiere JSON_PASO_1 como input\n- `src/pipeline/fase_1_triaje.py` y `fase_2_extraccion.py` - Funciones `_llamar_groq_api_*()` como referencia\n- `src/utils/prompt_loader.py` - Para cargar plantillas de prompt desde archivos externos  \n- `src/utils/error_handling.py` - Para usar decoradores de retry y manejo de errores\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Groq SDK**: Client configuration, chat completions, error handling para prompts complejos\n- **Python json**: JSON serialization para formatear JSON_PASO_1 en el prompt\n- **Tenacity library**: Retry patterns espec√≠ficos para APIs con prompts largos\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Crear funci√≥n `_llamar_groq_api_citas_datos()` siguiendo patr√≥n de fases anteriores\n- CR√çTICO: Formatear JSON_PASO_1 desde `resultado_fase_2` (hechos y entidades extra√≠dos)\n- Variables del prompt: {{TITULO_O_DOCUMENTO}}, {{FUENTE_O_TIPO}}, {{FECHA_FUENTE}}, {{CONTENIDO}}, {{JSON_PASO_1}}\n- Configurar par√°metros Groq apropiados para an√°lisis de texto complejo (mayor max_tokens)\n- Implementar retry logic usando decorador `@retry_groq_api()` existente\n- Manejar respuestas JSON complejas que incluyen arrays de citas y datos cuantitativos\n- IMPORTANTE: Validar que JSON_PASO_1 tenga el formato correcto antes de enviar a Groq\n</info added on 2025-06-06T03:19:42.284Z>",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Implement quote extraction logic",
          "description": "Develop logic to extract textual quotes from the API response and map them to CitaTextual objects using MetadatosCita model.",
          "dependencies": [
            1,
            2
          ],
          "details": "Create a parser to identify and extract quotes from the API response. Implement the CitaTextual class with appropriate properties (text, source reference, context, etc.) using MetadatosCita for structured metadata. Develop mapping logic to convert raw quote data into properly structured CitaTextual objects. Include validation to ensure quotes are properly formatted and contain required information. Maintain sequential IDs from phase 2 instead of generating UUIDs.\n<info added on 2025-06-06T03:20:14.421Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `prompts/Prompt_3_citas_datos.md` - Para entender la estructura exacta de citas en el JSON de respuesta\n- `src/models/procesamiento.py` - Modelo `CitaTextual` con campos requeridos y validaciones\n- Ejemplo en prompt: estructura de cita con id, cita, entidad_id, hecho_id, fecha, contexto, relevancia\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Pydantic**: Model validation, field constraints (constr min_length), handling optional fields\n- **Python datetime**: Date parsing y validation para fechas de citas\n- **Python json**: Parsing arrays, handling missing fields en estructura de citas\n- **Python UUID**: uuid4() generation para IDs √∫nicos de citas\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Crear funci√≥n `_parsear_citas_textuales_from_json(response_json, id_fragmento_origen)`\n- Mapear campos JSON a modelo `CitaTextual`:\n  * `id` (del JSON) ‚Üí generar UUID temporal nuevo con uuid4()\n  * `cita` ‚Üí `texto_cita`\n  * `entidad_id` ‚Üí `id_entidad_citada` (mapear desde ID temporal de fase 2)\n  * `hecho_id` ‚Üí almacenar en `metadata_cita` \n  * `fecha`, `contexto`, `relevancia` ‚Üí `metadata_cita`\n- Validar que `texto_cita` cumpla constr(min_length=5)\n- Asignar `id_fragmento_origen` desde resultado de fase anterior\n- Manejar casos edge: citas sin entidad_id, fechas en formato incorrecto, contextos largos\n- CR√çTICO: Los entidad_id y hecho_id deben corresponder a IDs reales de la fase 2\n</info added on 2025-06-06T03:20:14.421Z>\n<info added on 2025-06-06T13:45:22.727Z>\nModificar la implementaci√≥n para usar IDs secuenciales en lugar de UUIDs. En la funci√≥n `_parsear_citas_textuales_from_json`, mantener los IDs originales de la fase 2 tanto para las citas como para las referencias a entidades y hechos. Utilizar la clase MetadatosCita para estructurar los campos de metadatos (fecha, contexto, relevancia, hecho_id) de manera organizada. Integrar el processor existente para garantizar la coherencia de IDs entre fases, asegurando que cada CitaTextual mantenga su relaci√≥n con las entidades y hechos previamente identificados. Verificar que los entidad_id y hecho_id correspondan exactamente a los IDs generados en la fase 2, sin crear nuevos identificadores.\n</info added on 2025-06-06T13:45:22.727Z>",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Implement quantitative data extraction logic",
          "description": "Develop logic to extract quantitative data from the API response and map them to DatosCuantitativos objects using MetadatosDato model.",
          "dependencies": [
            1,
            2
          ],
          "details": "Create a parser to identify and extract numerical data from the API response. Implement the DatosCuantitativos class with appropriate properties (value, unit, context, source reference, etc.) using MetadatosDato for structured metadata. Develop mapping logic to convert raw numerical data into properly structured DatosCuantitativos objects. Include validation to ensure numerical data is properly formatted and contains required information. Maintain sequential IDs from phase 2 instead of generating UUIDs.\n<info added on 2025-06-06T03:20:44.987Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `prompts/Prompt_3_citas_datos.md` - Para entender la estructura de datos cuantitativos en JSON de respuesta\n- `src/models/procesamiento.py` - Modelo `DatosCuantitativos` con campos requeridos y validaciones\n- Ejemplo en prompt: estructura compleja con hecho_id, indicador, categoria, valor, unidad, periodo, variaciones\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Pydantic**: Float validation, string constraints (constr min_length), nested object handling\n- **Python datetime**: Date range parsing para campos periodo (inicio/fin)\n- **Python json**: Parsing complex nested objects, handling arrays y null values\n- **Python UUID**: uuid4() generation para IDs √∫nicos de datos cuantitativos\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Crear funci√≥n `_parsear_datos_cuantitativos_from_json(response_json, id_fragmento_origen)`\n- Mapear campos JSON a modelo `DatosCuantitativos`:\n  * `id` (del JSON) ‚Üí generar UUID temporal nuevo con uuid4()\n  * `indicador` ‚Üí `descripcion_dato`\n  * `valor` ‚Üí `valor_dato` (validar como float)\n  * `unidad` ‚Üí `unidad_dato`\n  * `categoria`, `ambito_geografico`, `tipo_periodo` ‚Üí `metadata_dato`\n  * `periodo.inicio`, `periodo.fin` ‚Üí `fecha_dato` (formatear como rango)\n  * `valor_anterior`, `variacion_absoluta`, `variacion_porcentual`, `tendencia` ‚Üí `metadata_dato`\n- Validar que `descripcion_dato` cumpla constr(min_length=3)\n- Asignar `id_fragmento_origen` desde resultado de fase anterior\n- Manejar casos edge: valores no num√©ricos, fechas malformadas, arrays de √°mbito geogr√°fico\n- CR√çTICO: Validar categor√≠as permitidas (econ√≥mico, demogr√°fico, electoral, etc.)\n</info added on 2025-06-06T03:20:44.987Z>\n<info added on 2025-06-06T13:45:44.952Z>\nImplement sequential ID handling for quantitative data extraction by using the integer IDs directly from the JSON response instead of generating UUIDs. Ensure the DatosCuantitativos objects maintain the same ID sequence from the previous phase. Use the MetadatosDato structure to store all specific fields including categoria, ambito_geografico, tipo_periodo, valor_anterior, variacion_absoluta, variacion_porcentual, and tendencia. Configure the processor to maintain ID coherence between processing phases, preserving the relationship between fragments and their extracted quantitative data. When mapping JSON to DatosCuantitativos objects, extract the 'id' field directly from the source JSON and convert it to an integer type if needed.\n</info added on 2025-06-06T13:45:44.952Z>",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Implement reference assignment and result assembly",
          "description": "Develop logic to assign references to extracted data and assemble the final ResultadoFase3CitasDatos object.",
          "dependencies": [
            1,
            3,
            4
          ],
          "details": "Create logic to maintain references between extracted quotes/data and their original sources. Implement the final assembly process to combine all extracted information into a complete ResultadoFase3CitasDatos object. Ensure proper validation of the final result. Add comprehensive documentation explaining the structure and relationships in the final output. Implement any necessary utility methods for accessing or filtering the assembled data.\n<info added on 2025-06-06T03:21:16.420Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `src/models/procesamiento.py` - Modelo `ResultadoFase3CitasDatos` completo y sus campos requeridos\n- `src/pipeline/fase_1_triaje.py` y `fase_2_extraccion.py` - Patrones de ensamblaje final y manejo de errores\n- `src/utils/error_handling.py` - Patterns de manejo de errores espec√≠ficos para fase 3\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Python UUID**: uuid4() generation, UUID uniqueness en referencias cruzadas\n- **Loguru**: Structured logging para fase de extracci√≥n de citas y datos\n- **Python Exception handling**: Error handling para procesos de m√∫ltiples componentes\n- **Pydantic**: Model validation, .touch() method, relationship handling entre modelos\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Implementar funci√≥n principal `ejecutar_fase_3(resultado_fase_2: ResultadoFase2Extraccion) -> ResultadoFase3CitasDatos`\n- CR√çTICO: Formatear JSON_PASO_1 desde `resultado_fase_2.hechos_extraidos` y `entidades_extraidas`\n- Generar UUIDs √∫nicos para citas y datos cuantitativos usando `uuid4()`\n- Ensamblar `ResultadoFase3CitasDatos`:\n  * `id_fragmento` desde `resultado_fase_2.id_fragmento`\n  * `citas_textuales_extraidas` desde parser de citas\n  * `datos_cuantitativos_extraidos` desde parser de datos\n  * `prompt_citas_datos_usado` con prompt formateado usado\n  * `metadata_citas_datos` con info t√©cnica (tokens, duraci√≥n, modelo)\n- Validar referencias cruzadas: que entidad_id y hecho_id en citas/datos existan en fase 2\n- Llamar `.touch()` para actualizar timestamps antes de retornar\n- Manejo de errores con fallbacks apropiados\n- Logging detallado para debugging de referencias entre fases\n</info added on 2025-06-06T03:21:16.420Z>\n<info added on 2025-06-06T13:46:55.395Z>\n## ACTUALIZACI√ìN DE IMPLEMENTACI√ìN:\n- Modificar la funci√≥n principal a `ejecutar_fase_3(resultado_fase_2: ResultadoFase2Extraccion, processor: FragmentProcessor) -> ResultadoFase3CitasDatos`\n- NO generar UUIDs para citas y datos cuantitativos\n- Utilizar IDs secuenciales del JSON para citas y datos cuantitativos\n- Mantener los IDs secuenciales de fase 2 en JSON_PASO_1\n- Asegurar que el formateo de JSON_PASO_1 preserve los IDs originales de `resultado_fase_2.hechos_extraidos` y `entidades_extraidas`\n- Implementar l√≥gica de mapeo entre IDs secuenciales y referencias en el ensamblaje final\n- Adaptar la validaci√≥n de referencias cruzadas para trabajar con IDs secuenciales en lugar de UUIDs\n- Integrar el par√°metro processor: FragmentProcessor en el flujo de procesamiento\n</info added on 2025-06-06T13:46:55.395Z>",
          "status": "done"
        },
        {
          "id": 6,
          "title": "Implement FragmentProcessor integration",
          "description": "Update ejecutar_fase_3 function to receive and use FragmentProcessor for maintaining sequential IDs.",
          "dependencies": [
            1
          ],
          "details": "Modify the ejecutar_fase_3 function signature to accept processor: FragmentProcessor as a parameter. Use the processor to maintain sequential IDs that are coherent with previous phases. Update all ID generation logic to use the processor instead of generating UUIDs. Ensure that the processor is properly used throughout the implementation to maintain ID consistency across all phases of processing.\n<info added on 2025-06-06T13:47:13.863Z>\nEl processor debe utilizarse para generar IDs secuenciales para citas y datos cuantitativos mediante los m√©todos processor.next_cita_id() y processor.next_dato_id() respectivamente. Esto garantiza que los identificadores sean coherentes con los generados en las fases anteriores del procesamiento. Aseg√∫rate de reemplazar cualquier generaci√≥n de UUID o ID personalizada con estas llamadas al processor para mantener la trazabilidad de los elementos a lo largo de todo el flujo de procesamiento.\n</info added on 2025-06-06T13:47:13.863Z>",
          "status": "done"
        },
        {
          "id": 7,
          "title": "Implement structured metadata models",
          "description": "Use MetadatosCita and MetadatosDato models instead of Dict[str, Any] for metadata fields.",
          "dependencies": [
            1,
            3,
            4
          ],
          "details": "Update the implementation to use the structured MetadatosCita and MetadatosDato models for storing metadata instead of generic dictionaries. Ensure all metadata fields are properly mapped to the appropriate fields in these models. Update validation logic to take advantage of the structured models. Document the structure and usage of these metadata models in the code.",
          "status": "done"
        },
        {
          "id": 8,
          "title": "Implement JSON_PASO_1 formatting with sequential IDs",
          "description": "Format JSON_PASO_1 from resultado_fase_2 while maintaining sequential IDs.",
          "dependencies": [
            2,
            6
          ],
          "details": "Develop the _formatear_json_paso_1 function to properly format the JSON input for the Groq API from resultado_fase_2. Ensure that all IDs from phase 2 are maintained in the JSON_PASO_1 format. Validate that the JSON structure matches the expected input format for the prompt. Implement proper error handling for cases where the input data doesn't match the expected structure.\n<info added on 2025-06-06T13:48:11.268Z>\nEs crucial que la funci√≥n _formatear_json_paso_1 preserve los IDs secuenciales exactos provenientes de fase 2, sin realizar ninguna modificaci√≥n o conversi√≥n. Los identificadores num√©ricos originales de hechos_extraidos y entidades_extraidas deben mantenerse como los mismos valores enteros en el formato JSON_PASO_1, garantizando as√≠ la trazabilidad completa entre las fases del procesamiento. Esta preservaci√≥n de IDs es esencial para mantener la integridad referencial a lo largo de todo el pipeline de extracci√≥n.\n</info added on 2025-06-06T13:48:11.268Z>",
          "status": "done"
        },
        {
          "id": 9,
          "title": "Implement cross-reference validation",
          "description": "Ensure entidad_id and hecho_id references correspond to actual IDs from phase 2.",
          "dependencies": [
            3,
            4,
            8
          ],
          "details": "Develop validation logic to ensure that all entidad_id and hecho_id references in quotes and quantitative data correspond to actual IDs from phase 2. Implement error handling for cases where references don't match. Add logging to track reference validation. Create utility functions to help with reference validation if needed.",
          "status": "done"
        }
      ]
    },
    {
      "id": 18,
      "title": "Implement Phase 4: Normalization, Linking, and Relationships",
      "description": "Create the fourth phase of the processing pipeline in src/pipeline/fase_4_normalizacion.py.",
      "status": "done",
      "dependencies": [
        5,
        8,
        9,
        12,
        13
      ],
      "priority": "high",
      "details": "1. Create src/pipeline/fase_4_normalizacion.py\n2. Implement ejecutar_fase_4() function\n3. Integrate with Groq API using the prompt from 'Prompt_4_relaciones.md'\n4. Use EntityNormalizer service (from Task 13) for entity normalization and linking\n5. Extract relationships between facts and entities\n6. Normalize and consolidate all extracted information\n7. Return ResultadoFase4 object with fully processed and linked data",
      "testStrategy": "Create comprehensive unit tests for normalization, linking, and relationship extraction. Mock Supabase calls for testing entity linking. Test integration with EntityNormalizer service. Ensure tests verify that sequential IDs are maintained throughout the normalization process.",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement ResultadoFase4 class and basic structure",
          "description": "Create the ResultadoFase4 class with all required fields and methods to store the final processed data including normalized entities and relationships.",
          "dependencies": [],
          "details": "1. Define ResultadoFase4 class with fields for normalized entities, relationships, and metadata\n2. Implement constructor, getters, setters, and validation methods\n3. Create data structures for storing entity relationships\n4. Add error handling and logging mechanisms\n5. Implement toString() and toJson() methods for debugging and serialization\n6. Add documentation following project standards\n<info added on 2025-06-06T08:43:34.234Z>\n## Implementation Details for ResultadoFase4 and Basic Structure\n\n### File Structure\n- Create `src/pipeline/fase_4_normalizacion.py` following the pattern established in previous phases\n- Follow the same architectural approach as in phases 1-3 for consistency\n\n### Key Components\n1. Main function: `ejecutar_fase_4(resultado_fase_3: ResultadoFase3CitasDatos) -> ResultadoFase4Normalizacion`\n2. Helper functions:\n   - `_formatear_elementos_normalizados()` - Prepares normalized elements for processing\n   - `_normalizar_entidades()` - Handles entity normalization using EntityNormalizer service\n   - `_extraer_relaciones()` - Processes relationships between entities\n\n### ResultadoFase4Normalizacion Implementation\n- Use existing class from `src/models/procesamiento.py`\n- Ensure proper field validation with Pydantic\n- Implement relationship structures using complex type hints (List, Dict, Optional)\n- Generate unique UUIDs for relationships and final result\n\n### Integration Points\n- EntityNormalizer service integration for standardizing entities\n- Input: Process ELEMENTOS_BASICOS_NORMALIZADOS from previous phases\n- Output: Follow JSON format specified in `prompts/Prompt_4_relaciones.md`\n\n### Technical Requirements\n- Implement proper error handling for normalization failures\n- Add comprehensive logging throughout the process\n- Ensure type safety with Python typing annotations\n- Follow project documentation standards for all methods and classes\n</info added on 2025-06-06T08:43:34.234Z>\n<info added on 2025-06-06T08:45:29.645Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `src/pipeline/fase_1_triaje.py`, `fase_2_extraccion.py`, `fase_3_citas_datos.py` - Para seguir el patr√≥n estructural establecido en fases anteriores\n- `src/models/procesamiento.py` - Para entender `ResultadoFase4Normalizacion` ya definido y sus campos requeridos\n- `prompts/Prompt_4_relaciones.md` - Para entender formato de entrada (ELEMENTOS_BASICOS_NORMALIZADOS) y salida JSON\n- `src/services/entity_normalizer.py` - Para entender integraci√≥n con servicio de normalizaci√≥n existente\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Pydantic**: BaseModel inheritance patterns, field validation, nested models para estructuras de relaciones\n- **Python typing**: Type hints complejos con List, Dict, Optional para relaciones hecho-entidad\n- **Python pathlib**: Manejo de rutas para imports relativos desde pipeline hacia servicios\n- **UUID generation**: uuid4() para IDs √∫nicos de relaciones y resultado final\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Crear archivo `src/pipeline/fase_4_normalizacion.py` siguiendo estructura de fases anteriores\n- Importar dependencias: `ResultadoFase4Normalizacion` desde models, entity_normalizer, groq_service\n- Definir funci√≥n `ejecutar_fase_4(resultado_fase_1, resultado_fase_2, resultado_fase_3)` como entry point\n- CR√çTICO: Esta fase necesita TODOS los resultados de fases anteriores para formatear ELEMENTOS_BASICOS_NORMALIZADOS\n- Crear funciones helper privadas: `_formatear_elementos_para_prompt()`, `_normalizar_entidades_extraidas()`, `_parsear_relaciones_json()`\n- NOTA: `ResultadoFase4Normalizacion` ya existe en procesamiento.py, usar directamente sin modificar\n- Las relaciones deben mapear exactamente al JSON schema del Prompt_4_relaciones.md: hecho_entidad, hecho_relacionado, entidad_relacion, contradicciones\n</info added on 2025-06-06T08:45:29.645Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Implement EntityNormalizer service integration",
          "description": "Integrate with the existing EntityNormalizer service to normalize and deduplicate entities extracted in previous phases.",
          "dependencies": [
            1
          ],
          "details": "1. Create an EntityNormalizerClient class to interface with the service\n2. Implement methods to send entities for normalization\n3. Process normalization responses and handle errors\n4. Add caching mechanism to avoid redundant normalization requests\n5. Implement entity mapping to maintain references between original and normalized entities\n6. Add comprehensive logging for debugging normalization issues\n<info added on 2025-06-06T08:45:50.379Z>\n7. Review existing EntityNormalizer service in src/services/entity_normalizer.py\n8. Implement _normalizar_entidades_extraidas() function to process extracted entities\n9. Update EntidadProcesada objects with normalization fields (id_entidad_normalizada, nombre_entidad_normalizada, similitud_normalizacion, uri_wikidata)\n10. Apply similarity threshold logic (>0.8 for automatic matching, <0.8 for new entity)\n11. Implement batch processing for optimizing multiple RPC calls\n12. Create temporary cache during phase execution to avoid redundant normalization requests\n13. Add structured logging with Loguru for tracking successful/failed normalizations\n14. Integrate retry mechanisms using Tenacity library with exponential backoff\n15. Implement async/await patterns for parallel normalization of multiple entities\n16. Preserve original entity list integrity while returning modified list with normalization data\n</info added on 2025-06-06T08:45:50.379Z>\n<info added on 2025-06-07T10:12:30.123Z>\n17. Enriquecer objetos EntidadProcesada existentes en lugar de crear nuevos\n18. Mantener id_entidad (int secuencial) intacto durante todo el proceso\n19. Solo a√±adir id_entidad_normalizada (UUID de DB) como campo adicional\n20. Implementar l√≥gica para preservar IDs secuenciales internos durante normalizaci√≥n\n21. Asegurar que las referencias entre entidades y hechos mantengan coherencia de IDs\n22. Documentar claramente la diferencia entre IDs internos (secuenciales) e IDs de DB (UUID)\n</info added on 2025-06-07T10:12:30.123Z>",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Implement Groq API integration for relationship extraction",
          "description": "Integrate with Groq API using the relationship prompt to extract relationships between facts and entities.",
          "dependencies": [
            1
          ],
          "details": "1. Create a RelationshipExtractor class using the GroqClient\n2. Implement prompt construction using Prompt_4_relaciones.md template\n3. Process API responses to extract relationship data\n4. Implement retry logic and error handling for API failures\n5. Add rate limiting and request batching for efficiency\n6. Create data structures to store extracted relationships\n7. Implement validation for relationship data\n<info added on 2025-06-06T08:46:17.050Z>\n## DOCUMENTOS DEL REPOSITORIO A REVISAR:\n- `prompts/Prompt_4_relaciones.md` - Para entender las variables del prompt: {{ELEMENTOS_BASICOS_NORMALIZADOS}}, {{ELEMENTOS_COMPLEMENTARIOS}}\n- `src/pipeline/fase_1_triaje.py`, `fase_2_extraccion.py` - Funciones `_llamar_groq_api_*()` como referencia de integraci√≥n Groq\n- `src/services/groq_service.py` - Para entender el servicio Groq existente y configuraci√≥n de par√°metros\n- `src/utils/prompt_loader.py` - Para cargar plantilla de prompt desde archivo externo\n- `src/utils/error_handling.py` - Para decoradores de retry espec√≠ficos para APIs LLM\n\n## CONTEXT7 DOCUMENTATION A CONSULTAR:\n- **Groq SDK**: Client configuration para prompts complejos, chat completions con JSON output\n- **Python json**: JSON parsing para respuesta compleja con arrays anidados (hecho_entidad, entidad_relacion)\n- **Tenacity library**: Retry patterns espec√≠ficos para prompts largos con mayor timeout\n- **Pydantic**: Model validation para parsear estructuras JSON complejas de relaciones\n\n## IMPLEMENTACI√ìN ESPEC√çFICA:\n- Crear funci√≥n `_llamar_groq_api_relaciones()` siguiendo patr√≥n de fases anteriores\n- CR√çTICO: Formatear {{ELEMENTOS_BASICOS_NORMALIZADOS}} desde hechos y entidades normalizadas de fases 2\n- CR√çTICO: Formatear {{ELEMENTOS_COMPLEMENTARIOS}} desde citas y datos de fase 3  \n- Variables del prompt: {{TITULO_O_DOCUMENTO}}, {{FUENTE_O_TIPO}}, {{PAIS_ORIGEN}}, {{FECHA_FUENTE}} desde metadata original\n- Configurar par√°metros Groq para an√°lisis complejo: mayor max_tokens, temperature baja para consistencia\n- Implementar parsing robusto para JSON de respuesta con 4 secciones: hecho_entidad, hecho_relacionado, entidad_relacion, contradicciones\n- Manejo de errores espec√≠ficos: JSON malformado, relaciones con IDs inexistentes, arrays vac√≠os\n- Usar decorador `@retry_groq_api()` existente con timeout extendido para prompts complejos\n</info added on 2025-06-06T08:46:17.050Z>\n<info added on 2025-06-07T10:13:45.456Z>\n8. Asegurar que las relaciones extra√≠das mantengan los IDs secuenciales internos\n9. No generar nuevos IDs para entidades o hechos durante el proceso de extracci√≥n\n10. Mantener coherencia con los IDs secuenciales utilizados en fases anteriores\n11. Implementar validaci√≥n para verificar que todos los IDs referenciados existen\n12. Documentar claramente el formato de IDs esperado en las relaciones extra√≠das\n</info added on 2025-06-07T10:13:45.456Z>",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Implement relationship processing and linking logic",
          "description": "Develop logic to process extracted relationships and link them to normalized entities.",
          "dependencies": [
            2,
            3
          ],
          "details": "1. Create a RelationshipProcessor class to handle relationship data\n2. Implement methods to link relationships to normalized entities\n3. Add validation and conflict resolution for contradictory relationships\n4. Implement confidence scoring for relationships\n5. Create data structures for efficient relationship querying\n6. Add methods to filter and prioritize relationships based on confidence\n7. Implement logging for relationship processing steps\n<info added on 2025-06-06T08:46:50.310Z>\n8. Review repository documents for relationship structure understanding:\n   - `prompts/Prompt_4_relaciones.md` for JSON relationship structure\n   - `src/models/procesamiento.py` for normalized ID fields\n   - `src/pipeline/fase_2_extraccion.py` and `fase_3_citas_datos.py` for JSON parsers\n   - `src/utils/error_handling.py` for relationship validation errors\n\n9. Implement specific parser functions:\n   - `_parsear_relaciones_hecho_entidad()`\n   - `_parsear_relaciones_hecho_hecho()`\n   - `_parsear_relaciones_entidad_entidad()`\n   - `_parsear_contradicciones()`\n\n10. Validate relationship types according to defined mappings:\n    - hecho_entidad: protagonista, mencionado, afectado, declarante, ubicacion, etc.\n    - hecho_relacionado: causa, consecuencia, contexto_historico, respuesta_a, etc.\n    - entidad_relacion: miembro_de, subsidiaria_de, aliado_con, opositor_a, etc.\n\n11. Implement numerical validation for relationship attributes:\n    - relevancia_en_hecho (1-10)\n    - fuerza_relacion (1-10)\n    - grado_contradiccion (1-5)\n\n12. Create indexed data structures for efficient relationship querying\n\n13. Implement circular relationship detection and logical contradiction identification\n\n14. Ensure all referenced IDs in relationships exist in extracted facts/entities\n</info added on 2025-06-06T08:46:50.310Z>\n<info added on 2025-06-07T10:15:00.789Z>\n15. Mantener coherencia de IDs secuenciales en todas las relaciones procesadas\n16. Verificar que las relaciones solo referencien IDs existentes en el conjunto de datos\n17. Implementar validaci√≥n cruzada entre IDs de entidades y hechos referenciados\n18. Documentar claramente la estructura de datos para mantener trazabilidad de IDs\n19. Asegurar que el procesamiento de relaciones no altere los IDs secuenciales originales\n</info added on 2025-06-07T10:15:00.789Z>",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Implement final data consolidation and PayloadBuilder integration",
          "description": "Consolidate all processed data and implement integration with PayloadBuilder for final output.",
          "dependencies": [
            4
          ],
          "details": "1. Create a DataConsolidator class to assemble all processed information\n2. Implement methods to merge data from all previous phases\n3. Add validation for the final consolidated data structure\n4. Create PayloadBuilderAdapter to format data for PayloadBuilder\n5. Implement error handling and fallback mechanisms\n6. Add comprehensive logging for the consolidation process\n7. Create unit and integration tests for the entire Phase 4 implementation\n<info added on 2025-06-06T08:47:32.661Z>\nImplement the `ejecutar_fase_4()` function that consolidates results from all previous phases into a unified `ResultadoFase4Normalizacion` structure. The function should:\n\n1. Extract `id_fragmento` from `resultado_fase_1`\n2. Use EntityNormalizer to create normalized entities\n3. Generate automatic normalization summary with process statistics\n4. Include the formatted prompt used in Groq\n5. Set normalization status (Complete, Partial, Failed) based on component success/failure\n6. Add technical metrics in metadata (tokens, duration, processed entities, extracted relationships)\n7. Validate referential integrity ensuring all relationships point to existing entities/facts\n8. Call `.touch()` to update timestamps before returning\n9. Prepare data for PayloadBuilder by serializing UUIDs, formatting dates, validating JSONB structure\n10. Implement robust error handling with fallbacks to return unnormalized data if normalization fails\n11. Add comprehensive logging for final metrics, normalized vs. new entities, processed relationships, and total pipeline time\n\nReview repository documents:\n- `src/models/procesamiento.py` for complete `ResultadoFase4Normalizacion` structure\n- `src/services/payload_builder.py` for JSONB payload building methods\n- Phase 1-3 implementation files for final assembly pattern and error handling\n- `src/utils/error_handling.py` for error handling patterns and fallbacks\n\nConsult documentation for:\n- Python UUID generation and serialization\n- Loguru structured logging for completeness metrics\n- Comprehensive exception handling for multi-component processes\n- Pydantic model validation, .touch() method, and JSON serialization\n</info added on 2025-06-06T08:47:32.661Z>\n<info added on 2025-06-07T10:16:30.234Z>\n12. Modificar la firma de la funci√≥n a `ejecutar_fase_4(processor: FragmentProcessor, resultado_fase_1, resultado_fase_2, resultado_fase_3)`\n13. Utilizar el mismo processor recibido como par√°metro para mantener coherencia con fases anteriores\n14. No realizar conversi√≥n final a UUIDs en esta fase, eso ocurre en persistencia\n15. Mantener los IDs secuenciales internos durante todo el proceso\n16. Documentar claramente que solo id_entidad_normalizada (UUID de DB) es externo\n17. Asegurar que el mismo processor usado en fases anteriores se utilice aqu√≠\n18. Implementar validaci√≥n para verificar coherencia de IDs entre fases\n19. A√±adir logging espec√≠fico para seguimiento de IDs durante el proceso\n</info added on 2025-06-07T10:16:30.234Z>",
          "status": "done"
        },
        {
          "id": 6,
          "title": "Update ejecutar_fase_4 function signature for FragmentProcessor",
          "description": "Modify the function signature to receive FragmentProcessor as parameter to maintain sequential ID coherence across all phases.",
          "dependencies": [
            1
          ],
          "details": "1. Update function signature to `ejecutar_fase_4(processor: FragmentProcessor, resultado_fase_1, resultado_fase_2, resultado_fase_3)`\n2. Use the processor parameter to maintain ID sequence consistency with previous phases\n3. Document the importance of using the same processor instance across all phases\n4. Implement validation to ensure processor state is consistent with previous phase results\n5. Add logging for processor state at beginning and end of phase execution\n6. Update unit tests to include processor parameter in function calls\n7. Create helper methods that utilize processor for ID management if needed\n<info added on 2025-06-06T13:48:29.821Z>\nEspecificar que la funci√≥n debe recibir resultado_fase_3: ResultadoFase3CitasDatos y processor: FragmentProcessor. El processor debe ser el mismo usado en fases anteriores para mantener consistencia de IDs a trav√©s de todo el pipeline. Verificar que el processor contenga el estado acumulado de las fases anteriores antes de proceder con la normalizaci√≥n y vinculaci√≥n de entidades.\n</info added on 2025-06-06T13:48:29.821Z>",
          "status": "done"
        },
        {
          "id": 7,
          "title": "Implement ID preservation strategy",
          "description": "Ensure that sequential internal IDs are maintained throughout the normalization process while properly handling external database UUIDs.",
          "dependencies": [
            2,
            3
          ],
          "details": "1. Document the distinction between internal sequential IDs and external database UUIDs\n2. Implement logic to enrich existing EntidadProcesada objects rather than creating new ones\n3. Maintain id_entidad (int sequential) intact throughout the entire process\n4. Only add id_entidad_normalizada (DB UUID) as an additional field\n5. Ensure all entity references in relationships use the original sequential IDs\n6. Add validation to verify ID integrity across all processed data\n7. Implement logging specifically for ID tracking during normalization\n8. Create unit tests that verify ID preservation across the entire phase\n<info added on 2025-06-06T13:48:45.730Z>\nIMPORTANTE: La fase de normalizaci√≥n NO debe crear nuevos objetos EntidadProcesada bajo ninguna circunstancia. El proceso debe limitarse exclusivamente a enriquecer los objetos existentes a√±adiendo los campos de normalizaci√≥n correspondientes. Los identificadores secuenciales internos (id_entidad) deben mantenerse intactos durante todo el proceso y nunca ser modificados o reemplazados. La conversi√≥n de identificadores secuenciales a UUIDs es responsabilidad exclusiva del PayloadBuilder y no debe realizarse en esta fase. Cualquier implementaci√≥n que cree nuevos objetos o modifique los IDs secuenciales existentes debe considerarse err√≥nea.\n</info added on 2025-06-06T13:48:45.730Z>",
          "status": "done"
        }
      ]
    },
    {
      "id": 19,
      "title": "Implement main processing controller",
      "description": "Create the main processing controller in src/controller.py to orchestrate the pipeline phases, using existing services and a simplified approach.",
      "status": "done",
      "dependencies": [
        15,
        16,
        17,
        18,
        13,
        14
      ],
      "priority": "high",
      "details": "Implement a simplified controller that focuses on core functionality without unnecessary complexity:\n\n1. Create src/controller.py with basic initialization\n2. Implement process_fragment() as the core processing method\n3. Implement process_article() as a simple wrapper\n4. Add minimal persistence integration\n5. Use existing services and models\n6. Focus on functional implementation first, with minimal error handling\n\nNote: The controller.py is already implemented but requires specific improvements as detailed in the subtasks.",
      "testStrategy": "Write integration tests that run through the pipeline with sample articles and fragments. Focus on testing the happy path first, then add tests for error handling. Verify correct phase execution and persistence. Create a dedicated test_controller_integration.py file to validate the complete controller functionality.",
      "subtasks": [
        {
          "id": "19.1",
          "title": "Optimizar integraci√≥n entre fases",
          "status": "done",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/controller.py` - Revisar m√©todo process_fragment() actual\n  - `/src/utils/fragment_processor.py` - Sistema de IDs secuenciales\n  - `/src/pipeline/fase_*.py` - Interfaces de cada fase\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 5.3: \"Flujo de Datos entre Fases\"\n  - Secci√≥n 11.4: \"Optimizaci√≥n de Pipeline\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Integrar FragmentProcessor en el controller para gesti√≥n coherente de IDs\n  - Pasar resultado_fase_1 a fase_3 para acceso al texto original\n  - Mejorar el paso de contexto entre fases\n  - Validar que todas las fases reciban los datos necesarios\n  - Sin cambios en la l√≥gica de negocio, solo mejoras en el flujo"
        },
        {
          "id": "19.2",
          "title": "Mejorar manejo de errores y fallbacks",
          "status": "done",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/controller.py` - Bloques try/except actuales\n  - `/src/utils/error_handling.py` - Sistema de manejo de errores\n  - `/docs/07-manejo-de-errores.md` - Especificaci√≥n de errores\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 11.2: \"Manejo de Errores en Pipeline\"\n  - Secci√≥n 11.3: \"Pol√≠ticas de Fallback\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Envolver cada llamada a fase en try/except espec√≠fico\n  - Implementar fallbacks parciales cuando una fase falle\n  - Asegurar que el pipeline contin√∫e aunque una fase falle\n  - Registrar todos los errores pero permitir procesamiento parcial\n  - Agregar campo \"advertencias\" al resultado final"
        },
        {
          "id": "19.3",
          "title": "Completar persistencia de relaciones",
          "status": "done",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/pipeline/fase_4_normalizacion.py` - Extracci√≥n de relaciones\n  - `/src/services/payload_builder.py` - Construcci√≥n de payloads\n  - `/docs/Persistencia/*.md` - Especificaci√≥n de persistencia\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 9.2: \"Estructuras de Payload para Relaciones\"\n  - Secci√≥n 9.3: \"RPCs de Persistencia Completa\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Extraer relaciones del metadata de fase 4\n  - Adaptar PayloadBuilder para incluir relaciones\n  - Mapear relaciones a formato esperado por RPC\n  - Incluir en el payload: relaciones_hechos, relaciones_entidades, contradicciones\n  - Actualizar logging para mostrar relaciones persistidas"
        },
        {
          "id": "19.4",
          "title": "A√±adir m√©tricas y logging estructurado",
          "status": "done",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/controller.py` - Logging actual con loguru\n  - `/src/utils/logging_config.py` - Configuraci√≥n de logging\n  - `/docs/sistema_logging_implementado.md` - Sistema de logging\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 12.1: \"M√©tricas de Pipeline\"\n  - Secci√≥n 12.2: \"Logging Estructurado\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Medir tiempo de ejecuci√≥n por fase\n  - Contar elementos procesados en cada fase\n  - Usar logger.bind() para contexto estructurado\n  - Agregar m√©tricas al resultado final: tiempos, conteos, tasas de √©xito\n  - Log de inicio y fin de cada fase con m√©tricas"
        },
        {
          "id": "19.5",
          "title": "Test de verificaci√≥n del controller completo",
          "status": "done",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/controller.py` - Controller implementado\n  - `/tests/test_pipeline/*.py` - Tests existentes\n  - `/tests/conftest.py` - Fixtures de pytest\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 13.1: \"Testing de Integraci√≥n\"\n  - Secci√≥n 13.2: \"Fixtures y Mocks\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Crear test_controller_integration.py\n  - Test con art√≠culo completo (process_article)\n  - Test con fragmento (process_fragment)\n  - Verificar flujo completo con mocks de servicios\n  - Validar m√©tricas y manejo de errores\n  - Ejecutable con: pytest tests/test_controller_integration.py -v"
        }
      ]
    },
    {
      "id": 20,
      "title": "Implement POST /procesar_articulo endpoint",
      "description": "Create the API endpoint for processing complete articles in src/main.py.",
      "status": "done",
      "dependencies": [
        4,
        7,
        19
      ],
      "priority": "high",
      "details": "1. In src/main.py, implement the POST /procesar_articulo endpoint\n2. Use ArticuloInItem Pydantic model for request validation\n3. Call the process_article() function from the controller\n4. Implement proper error handling and response formatting\n5. Use FastAPI's background tasks for asynchronous processing if required",
      "testStrategy": "Create API tests using TestClient. Test with valid and invalid article inputs. Verify correct response formats and error handling.",
      "subtasks": [
        {
          "id": "20.1",
          "title": "Importar dependencias y configurar endpoint b√°sico",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` - Buscar la funci√≥n `procesar_articulo()` ya esqueletizada\n  - `/src/controller.py` - Revisar la firma de `process_article()`\n  - `/src/module_connector/src/models.py` - Copiar import path de ArticuloInItem\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 6.1: \"Definici√≥n de Endpoints FastAPI\"\n  - Secci√≥n 6.2: \"Modelos de Request/Response\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Importar ArticuloInItem desde module_connector\n  - Importar el controller ya instanciado (pipeline_controller)\n  - Remover el raise NotImplementedError\n  - Definir el endpoint con el modelo correcto como par√°metro\n  - Agregar documentaci√≥n en el docstring",
          "status": "done"
        },
        {
          "id": "20.2",
          "title": "Implementar flujo de procesamiento s√≠ncrono",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/controller.py` - M√©todo `process_article()` l√≠nea ~200\n  - `/src/utils/error_handling.py` - Funci√≥n `generate_request_id()`\n  - `/src/utils/logging_config.py` - Funci√≥n `get_logger()`\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 7.1: \"Flujo de Procesamiento de Art√≠culos\"\n  - Secci√≥n 7.2: \"Contratos de Datos entre M√≥dulos\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Generar request_id √∫nico para el procesamiento\n  - Crear logger con contexto del request_id\n  - Convertir ArticuloInItem a diccionario para el controller\n  - Llamar await process_article() del controller\n  - Retornar el resultado directamente",
          "status": "done"
        },
        {
          "id": "20.3",
          "title": "Agregar validaci√≥n y preparaci√≥n de datos",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/module_connector/src/models.py` - M√©todo `validate_required_fields()`\n  - `/src/models/entrada.py` - Validadores de FragmentoProcesableItem\n  - `/docs/07-manejo-de-errores.md` - Secci√≥n de validaci√≥n de entrada\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 6.3: \"Validaci√≥n de Datos de Entrada\"\n  - Secci√≥n 11.1: \"Validaci√≥n en FastAPI\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Verificar campos requeridos del art√≠culo\n  - Preparar campos faltantes con valores por defecto\n  - Validar longitud m√≠nima del contenido_texto\n  - Registrar en logs los datos recibidos (sin contenido completo)\n  - Lanzar HTTPException 400 si faltan datos cr√≠ticos",
          "status": "done"
        },
        {
          "id": "20.4",
          "title": "Implementar respuesta estructurada y manejo de errores",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` - Manejadores de excepciones ya configurados\n  - `/src/utils/error_handling.py` - Funci√≥n `create_error_response()`\n  - `/docs/07-manejo-de-errores.md` - Formatos de respuesta de error\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 11.2: \"Manejo de Errores en Pipeline\"\n  - Secci√≥n 11.3: \"Respuestas HTTP Estructuradas\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Envolver la llamada al controller en try/except\n  - Capturar PipelineException y dejar que el handler global la maneje\n  - Para errores de validaci√≥n, usar ValidationError personalizada\n  - Estructurar respuesta exitosa con campos est√°ndar\n  - Incluir siempre request_id en la respuesta\n  - Agregar timestamp y versi√≥n del API",
          "status": "done"
        },
        {
          "id": "20.5",
          "title": "Optimizar para art√≠culos largos (opcional background)",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` - Importar BackgroundTasks de FastAPI\n  - `/src/controller.py` - M√©tricas de tiempo de procesamiento\n  - `/docs/arquitectura_general.md` - Consideraciones de performance\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 8.1: \"Procesamiento As√≠ncrono en FastAPI\"\n  - Secci√≥n 8.2: \"Background Tasks vs Workers\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Medir longitud del contenido_texto\n  - Si supera 10,000 caracteres, considerar background task\n  - Por ahora, mantener s√≠ncrono pero agregar comentario TODO\n  - Documentar el threshold en el c√≥digo\n  - Preparar estructura para futura implementaci√≥n as√≠ncrona",
          "status": "done"
        },
        {
          "id": "20.6",
          "title": "Test de Verificaci√≥n Simple",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/tests/test_api/` - Crear test_procesar_articulo.py aqu√≠\n  - `/tests/conftest.py` - Fixtures compartidas de pytest\n  - `/src/module_connector/src/models.py` - Ejemplo de ArticuloInItem\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 13.3: \"Testing de Endpoints con TestClient\"\n  - Secci√≥n 13.4: \"Fixtures para Testing de API\"\n- **üîß Especificaciones de implementaci√≥n**:\n  - Crear test con FastAPI TestClient\n  - Test con art√≠culo v√°lido completo\n  - Test con art√≠culo sin campos requeridos\n  - Test con contenido vac√≠o\n  - Verificar estructura de respuesta exitosa\n  - Verificar c√≥digos de estado HTTP correctos\n  - Ejecutable con: pytest tests/test_api/test_procesar_articulo.py -v",
          "status": "done"
        }
      ]
    },
    {
      "id": 21,
      "title": "Implement POST /procesar_fragmento endpoint",
      "description": "Create the API endpoint for processing document fragments in src/main.py.",
      "status": "done",
      "dependencies": [
        4,
        7,
        19
      ],
      "priority": "high",
      "details": "1. In src/main.py, implement the POST /procesar_fragmento endpoint\n2. Use FragmentoProcesableItem Pydantic model for request validation\n3. Call the process_fragment() function from the controller\n4. Implement proper error handling and response formatting\n5. Use FastAPI's background tasks for asynchronous processing if required",
      "testStrategy": "Create API tests using TestClient. Test with valid and invalid fragment inputs. Verify correct response formats and error handling.",
      "subtasks": [
        {
          "id": "21.1",
          "title": "Implementar endpoint b√°sico con validaci√≥n Pydantic",
          "status": "done",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` - Reemplazar el NotImplementedError actual del endpoint\n  - `/src/models/entrada.py` - Importar y usar FragmentoProcesableItem\n  - `/src/main.py` l√≠neas 370-406 - Usar endpoint /procesar_articulo como referencia\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 7.1: \"Endpoints del Pipeline\" - Estructura de endpoints FastAPI\n  - Secci√≥n 7.2: \"Validaci√≥n de Entrada\" - Uso de modelos Pydantic\n- **üîß Especificaciones de implementaci√≥n**:\n  - Reemplazar la funci√≥n `procesar_fragmento()` actual que lanza NotImplementedError\n  - Importar FragmentoProcesableItem desde models.entrada\n  - Definir par√°metro de entrada con tipo FragmentoProcesableItem\n  - Generar request_id √∫nico para tracking\n  - Crear logger con contexto del request_id\n  - Log inicial con informaci√≥n b√°sica del fragmento recibido"
        },
        {
          "id": "21.2",
          "title": "Integrar llamada al controller y manejo de respuesta",
          "status": "done",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/controller.py` m√©todo `process_fragment()` - Entender firma y respuesta esperada\n  - `/src/main.py` l√≠neas 470-530 - Copiar patr√≥n de llamada al controller de /procesar_articulo\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 8.1: \"Integraci√≥n con Controller\" - Patrones de llamada\n  - Secci√≥n 11.1: \"Respuestas Estructuradas\" - Formato de respuesta API\n- **üîß Especificaciones de implementaci√≥n**:\n  - Convertir FragmentoProcesableItem a diccionario con model_dump()\n  - A√±adir request_id al diccionario de datos\n  - Llamar await pipeline_controller.process_fragment(fragmento_dict)\n  - Estructurar respuesta con campos: success, request_id, timestamp, api_version, data\n  - Log de √©xito con m√©tricas b√°sicas del procesamiento"
        },
        {
          "id": "21.3",
          "title": "Implementar manejo robusto de errores",
          "status": "done",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` l√≠neas 531-575 - Copiar estructura try/except de /procesar_articulo\n  - `/src/utils/error_handling.py` - Entender tipos de excepciones del pipeline\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 11.2: \"Manejo de Errores en API\" - Patrones de error handling\n  - Secci√≥n 11.3: \"Logging de Errores\" - Estructura de logs de error\n- **üîß Especificaciones de implementaci√≥n**:\n  - Envolver llamada al controller en bloque try/except\n  - Capturar ValidationError, PipelineException, PydanticValidationError espec√≠ficamente\n  - Re-lanzar excepciones para que los handlers globales las procesen\n  - Para errores inesperados, log detallado con exc_info=True\n  - Asegurar que todas las excepciones incluyan el request_id para tracking"
        },
        {
          "id": "21.4",
          "title": "Optimizar para fragmentos peque√±os y logging detallado",
          "status": "done",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` l√≠neas 420-440 - Referencia de logging estructurado\n  - `/src/utils/logging_config.py` - Sistema de logging con contexto\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 12.1: \"M√©tricas de Performance\" - Qu√© m√©tricas registrar\n  - Secci√≥n 12.2: \"Logging Estructurado\" - Mejores pr√°cticas de logging\n- **üîß Especificaciones de implementaci√≥n**:\n  - Detectar fragmentos muy peque√±os (<100 caracteres) y loguear advertencia\n  - A√±adir campos al log inicial: longitud_texto, tiene_metadata, orden_en_articulo\n  - En respuesta exitosa, incluir m√©tricas del resultado en el log\n  - Documentar en docstring que fragmentos <50 chars fallar√°n por validaci√≥n\n  - No implementar cach√© ni optimizaciones complejas (KISS)"
        },
        {
          "id": "21.5",
          "title": "Test de Verificaci√≥n Simple",
          "status": "done",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` - Endpoint implementado en subtareas anteriores\n  - `/tests/` - Directorio de tests existentes para referencia\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n 13.1: \"Testing de Endpoints\" - Uso de TestClient de FastAPI\n  - Secci√≥n 13.2: \"Casos de Prueba API\" - Qu√© escenarios probar\n- **üîß Especificaciones de implementaci√≥n**:\n  - Crear archivo `test_endpoint_fragmento.py` en ra√≠z del proyecto\n  - Test 1: Fragmento v√°lido - verificar respuesta exitosa y estructura\n  - Test 2: Fragmento sin campos requeridos - verificar error 400\n  - Test 3: Fragmento con texto muy corto - verificar error de validaci√≥n\n  - Usar httpx o TestClient para las pruebas\n  - El test debe ejecutarse con: `python test_endpoint_fragmento.py`\n  - Incluir print statements claros de √©xito/fallo"
        }
      ]
    },
    {
      "id": 22,
      "title": "Implement GET /status/{job_id} endpoint",
      "description": "Create the API endpoint for querying processing status in src/main.py, with a robust job tracking service to manage job states throughout the processing pipeline.",
      "status": "pending",
      "dependencies": [
        7
      ],
      "priority": "medium",
      "details": "1. In src/main.py, implement the GET /status/{job_id} endpoint\n2. Create a job tracking service to manage job status (in-memory store with thread safety)\n3. Implement logic to retrieve and return job status with appropriate formatting\n4. Handle cases for non-existent job IDs with proper error responses\n5. Ensure proper error handling, response formatting, and automatic cleanup of old jobs\n6. Integrate job tracking with existing processing endpoints",
      "testStrategy": "Write API tests to check status retrieval for existing and non-existing job IDs. Verify correct status updates through the processing pipeline. Create a comprehensive test script to validate the entire job tracking flow from creation to completion or failure.",
      "subtasks": [
        {
          "id": "22.1",
          "title": "Crear servicio de tracking de jobs",
          "description": "Implementar un servicio para el seguimiento y gesti√≥n del estado de los trabajos de procesamiento.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/services/groq_service.py` - [Revisar estructura de servicio para mantener consistencia]\n  - `/src/services/supabase_service.py` - [Revisar patr√≥n singleton si aplica]\n  - `/src/utils/logging_config.py` - [Entender sistema de logging estructurado]\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n de patrones de dise√±o de servicios\n  - Mejores pr√°cticas para almacenamiento in-memory\n- **üîß Especificaciones de implementaci√≥n**:\n  - Crear archivo `/src/services/job_tracker_service.py`\n  - Implementar clase `JobTrackerService` con patr√≥n singleton\n  - Definir estados: `pending`, `processing`, `completed`, `failed`\n  - Almacenar: job_id, status, created_at, updated_at, result (opcional), error (opcional)\n  - M√©todos: `create_job()`, `update_status()`, `get_status()`, `delete_job()`\n  - Usar threading.Lock para thread-safety\n  - Logger estructurado con contexto de job_id",
          "status": "pending"
        },
        {
          "id": "22.2",
          "title": "Integrar tracking con endpoints existentes",
          "description": "Modificar los endpoints de procesamiento existentes para crear y actualizar jobs en el servicio de tracking.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` - [Endpoints procesar_articulo y procesar_fragmento]\n  - `/src/controller.py` - [M√©todos process_article y process_fragment]\n  - `/src/utils/error_handling.py` - [Manejo de excepciones]\n- **üìö Documentaci√≥n Context7**:\n  - Patrones de integraci√≥n de servicios\n  - Manejo de estados as√≠ncronos\n- **üîß Especificaciones de implementaci√≥n**:\n  - Modificar `procesar_articulo` para crear job con request_id\n  - Actualizar estado a `processing` antes de llamar al controller\n  - Actualizar a `completed` con resultado al finalizar\n  - Actualizar a `failed` con error en caso de excepci√≥n\n  - Mantener compatibilidad con respuesta s√≠ncrona actual\n  - A√±adir campo `job_id` en respuesta para tracking opcional\n  - Aplicar mismo patr√≥n a `procesar_fragmento`",
          "status": "pending"
        },
        {
          "id": "22.3",
          "title": "Implementar endpoint GET /status/{job_id}",
          "description": "Desarrollar el endpoint para consultar el estado de un job espec√≠fico por su ID.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` - [Endpoint placeholder get_status]\n  - `/src/utils/error_handling.py` - [Funciones create_error_response]\n  - Otros endpoints para referencia de formato de respuesta\n- **üìö Documentaci√≥n Context7**:\n  - Formato est√°ndar de respuestas API\n  - Manejo de errores HTTP\n- **üîß Especificaciones de implementaci√≥n**:\n  - Reemplazar NotImplementedError con implementaci√≥n real\n  - Obtener instancia de JobTrackerService\n  - Consultar estado del job_id\n  - Si no existe: HTTPException 404 con mensaje descriptivo\n  - Si existe: retornar estado con formato consistente\n  - Incluir: status, created_at, updated_at, progress (opcional)\n  - Si completed: incluir resumen del resultado\n  - Si failed: incluir mensaje de error\n  - Logging estructurado de consultas",
          "status": "pending"
        },
        {
          "id": "22.4",
          "title": "A√±adir limpieza autom√°tica de jobs antiguos",
          "description": "Implementar un mecanismo para eliminar autom√°ticamente los jobs que superen un tiempo de retenci√≥n configurable.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/services/job_tracker_service.py` - [Servicio creado en 22.1]\n  - `/src/main.py` - [Eventos startup/shutdown]\n  - `/src/config.py` - [Configuraciones del sistema]\n- **üìö Documentaci√≥n Context7**:\n  - Gesti√≥n de memoria en aplicaciones Python\n  - Mejores pr√°cticas para background tasks\n- **üîß Especificaciones de implementaci√≥n**:\n  - A√±adir configuraci√≥n JOB_RETENTION_MINUTES (default: 60)\n  - Implementar m√©todo `cleanup_old_jobs()` en JobTrackerService\n  - Eliminar jobs m√°s antiguos que el tiempo de retenci√≥n\n  - Crear tarea background con asyncio que ejecute limpieza cada 5 minutos\n  - Iniciar tarea en evento startup de FastAPI\n  - Detener tarea en evento shutdown\n  - Logging de jobs eliminados\n  - Considerar l√≠mite m√°ximo de jobs almacenados",
          "status": "pending"
        },
        {
          "id": "22.5",
          "title": "Test de verificaci√≥n simple",
          "description": "Crear un script de prueba para verificar el funcionamiento correcto del sistema de tracking de jobs.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - Todos los archivos implementados en subtareas anteriores\n  - `/src/main.py` - [Endpoints modificados]\n  - `/src/services/job_tracker_service.py` - [Servicio de tracking]\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n de testing de APIs\n  - Verificaci√≥n de integraci√≥n\n- **üîß Especificaciones de implementaci√≥n**:\n  - Crear script `/tests/test_job_tracking.py`\n  - Test 1: Crear job y verificar estado inicial\n  - Test 2: Procesar art√≠culo y verificar job_id en respuesta\n  - Test 3: Consultar estado durante procesamiento\n  - Test 4: Verificar estado completed despu√©s de procesamiento\n  - Test 5: Consultar job_id inexistente (esperar 404)\n  - Test 6: Simular error y verificar estado failed\n  - Usar httpx o requests para llamadas HTTP\n  - Imprimir resultados claros de cada test",
          "status": "pending"
        },
        {
          "id": 23.5,
          "title": "Subtarea 22.2 COMPLETADA - Integrar tracking con endpoints existentes",
          "description": "‚úÖ COMPLETADA: Se integr√≥ exitosamente el tracking de jobs con ambos endpoints (procesar_articulo y procesar_fragmento). Los endpoints ahora crean jobs, actualizan estados durante el procesamiento y a√±aden job_id a las respuestas.",
          "details": "",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 22
        }
      ]
    },
    {
      "id": 23,
      "title": "Implement asynchronous processing",
      "description": "Modify the controller and API endpoints to support asynchronous processing of articles and fragments.",
      "status": "pending",
      "dependencies": [
        19,
        20,
        21
      ],
      "priority": "medium",
      "details": "1. Update controller.py to use async/await syntax\n2. Modify process_article() and process_fragment() to be asynchronous\n3. Update API endpoints to use background tasks for processing\n4. Implement a job queue system (e.g., using Redis or RabbitMQ) if required\n5. Ensure proper handling of concurrent requests",
      "testStrategy": "Create tests that simulate concurrent API calls. Verify that processing occurs asynchronously and that the system handles multiple simultaneous requests correctly.",
      "subtasks": [
        {
          "id": "23.1",
          "title": "Crear m√©todos de procesamiento en background en el controller",
          "description": "Implementar m√©todos de procesamiento en background para art√≠culos y fragmentos en el controller.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/controller.py` - A√±adir m√©todos _process_article_background y _process_fragment_background\n  - `/src/services/job_tracker_service.py` - Entender c√≥mo actualizar estados del job\n- **üìö Documentaci√≥n Context7**:\n  - No aplica espec√≠ficamente - usar patrones del c√≥digo existente\n- **üîß Especificaciones de implementaci√≥n**:\n  - Crear m√©todo `_process_article_background(self, articulo_data: Dict, job_id: str)`\n  - Crear m√©todo `_process_fragment_background(self, fragmento_data: Dict, job_id: str)`\n  - Ambos m√©todos deben actualizar el job tracker al inicio, durante y al final\n  - Manejar errores y actualizar job status a FAILED si algo falla\n  - Usar el mismo flujo que los m√©todos s√≠ncronos existentes",
          "status": "pending"
        },
        {
          "id": "23.2",
          "title": "Implementar l√≥gica de decisi√≥n para procesamiento as√≠ncrono",
          "description": "Definir cu√°ndo usar procesamiento as√≠ncrono basado en el tama√±o del contenido.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` - Modificar endpoint `/procesar_articulo` (l√≠neas ~380-400 tienen ejemplo)\n  - `/src/utils/config.py` - Verificar si hay configuraci√≥n para thresholds\n- **üìö Documentaci√≥n Context7**:\n  - Secci√≥n de optimizaci√≥n para art√≠culos largos mencionada en comentarios\n- **üîß Especificaciones de implementaci√≥n**:\n  - Definir constante `ASYNC_PROCESSING_THRESHOLD = 10_000` caracteres\n  - En endpoint `procesar_articulo`: si len(contenido) > threshold, usar BackgroundTasks\n  - En endpoint `procesar_fragmento`: aplicar misma l√≥gica\n  - Retornar respuesta inmediata con job_id y endpoint de status\n  - Mantener procesamiento s√≠ncrono para contenido peque√±o",
          "status": "pending"
        },
        {
          "id": "23.3",
          "title": "Actualizar endpoints para usar BackgroundTasks",
          "description": "Modificar los endpoints para soportar procesamiento en segundo plano con FastAPI BackgroundTasks.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/main.py` - Modificar endpoints procesar_articulo y procesar_fragmento\n  - `/src/main.py` - Seguir el patr√≥n del ejemplo comentado en l√≠neas 380-400\n- **üìö Documentaci√≥n Context7**:\n  - FastAPI BackgroundTasks documentation pattern\n- **üîß Especificaciones de implementaci√≥n**:\n  - Usar `background_tasks.add_task()` cuando se detecte contenido largo\n  - Pasar el m√©todo del controller apropiado como task\n  - Incluir job_id y todos los datos necesarios\n  - Retornar respuesta con estructura: success, job_id, status=\"processing\", tracking info\n  - Estimar tiempo de procesamiento basado en longitud (ejemplo: longitud/100 segundos)",
          "status": "pending"
        },
        {
          "id": "23.4",
          "title": "Asegurar manejo robusto de concurrencia",
          "description": "Verificar y mejorar el manejo de concurrencia en el sistema para evitar problemas con m√∫ltiples solicitudes simult√°neas.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/controller.py` - Revisar si hay estado compartido que pueda causar problemas\n  - `/src/services/job_tracker_service.py` - Verificar thread-safety (ya implementado)\n- **üìö Documentaci√≥n Context7**:\n  - Patrones de concurrencia y thread-safety\n- **üîß Especificaciones de implementaci√≥n**:\n  - Verificar que GroqService y SupabaseService manejen concurrencia correctamente\n  - Asegurar que cada procesamiento tenga su propio contexto (request_id √∫nico)\n  - No compartir estado mutable entre procesamiento de diferentes requests\n  - Documentar cualquier limitaci√≥n de concurrencia encontrada",
          "status": "pending"
        },
        {
          "id": "23.5",
          "title": "Test de Verificaci√≥n Simple",
          "description": "Crear pruebas para verificar el funcionamiento correcto del procesamiento as√≠ncrono.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/tests/` - Crear script test_async_processing.py\n  - `/src/main.py` - Endpoints a probar\n  - `/src/controller.py` - M√©todos background a verificar\n- **üìö Documentaci√≥n Context7**:\n  - Patrones de testing existentes en el proyecto\n- **üîß Especificaciones de implementaci√≥n**:\n  - Crear script que env√≠e un art√≠culo largo (>10k caracteres) v√≠a POST\n  - Verificar que retorna inmediatamente con status=\"processing\"\n  - Consultar endpoint /status/{job_id} peri√≥dicamente\n  - Verificar que el job pasa por estados: pending -> processing -> completed\n  - Crear segundo test con m√∫ltiples requests concurrentes\n  - Verificar que el sistema maneja correctamente m√∫ltiples jobs\n  - Output claro: \"‚úÖ Test as√≠ncrono exitoso\" o \"‚ùå Error: [detalles]\"",
          "status": "completed"
        },
        {
          "id": "23.6",
          "title": "Revisar resultados de pruebas y ajustar implementaci√≥n",
          "description": "Analizar los resultados de las pruebas completadas y realizar ajustes necesarios en la implementaci√≥n.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/tests/test_async_processing.py` - Revisar resultados de pruebas completadas\n  - `/src/controller.py` - Ajustar implementaci√≥n seg√∫n resultados\n  - `/src/main.py` - Refinar endpoints seg√∫n feedback de pruebas\n- **üìö Documentaci√≥n Context7**:\n  - Patrones de optimizaci√≥n basados en resultados de pruebas\n- **üîß Especificaciones de implementaci√≥n**:\n  - Revisar los resultados de las pruebas completadas en test_async_processing.py\n  - Verificar que el procesamiento as√≠ncrono funciona correctamente para art√≠culos largos\n  - Confirmar que el procesamiento s√≠ncrono se mantiene para art√≠culos peque√±os\n  - Asegurar que el manejo de m√∫ltiples requests concurrentes es robusto\n  - Realizar ajustes en la implementaci√≥n basados en cualquier problema identificado\n  - Documentar cualquier optimizaci√≥n adicional realizada",
          "status": "pending"
        }
      ]
    },
    {
      "id": 24,
      "title": "Implement comprehensive error handling",
      "description": "Enhance error handling throughout the application for improved reliability and debugging.",
      "status": "pending",
      "dependencies": [
        10,
        11
      ],
      "priority": "high",
      "details": "1. Review and update all error handling in the application\n2. Implement custom exception classes for specific error scenarios\n3. Ensure all external API calls (Groq, Supabase) have proper error handling\n4. Implement graceful degradation for non-critical failures\n5. Enhance logging for all error scenarios with appropriate context",
      "testStrategy": "Create a test suite that simulates various error conditions (API failures, database errors, etc.). Verify correct error handling, logging, and system recovery. The comprehensive test script will be implemented as `test_error_handling_integral.py` to validate the entire error handling system.",
      "subtasks": [
        {
          "id": 24.1,
          "title": "Auditor√≠a de Manejo de Errores en Servicios Externos",
          "description": "Realizar una auditor√≠a completa del manejo de errores en todos los servicios externos de la aplicaci√≥n.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/services/groq_service.py` - [Identificar puntos sin manejo de errores apropiado]\n  - `/src/services/supabase_service.py` - [Verificar uso de excepciones personalizadas]\n  - `/src/services/entity_normalizer.py` - [Revisar manejo de errores actual]\n  - `/src/services/job_tracker_service.py` - [Evaluar robustez del tracking]\n- **üìö Documentaci√≥n Context7**:\n  - No aplicable para esta subtarea de auditor√≠a\n- **üîß Especificaciones de implementaci√≥n**:\n  - Crear checklist de puntos cr√≠ticos sin manejo de errores\n  - Identificar usos de logging est√°ndar que deber√≠an usar loguru\n  - Documentar lugares donde faltan decoradores de retry\n  - Verificar que todos los try-except tengan logging apropiado",
          "status": "done"
        },
        {
          "id": 24.2,
          "title": "Integrar Excepciones Personalizadas en Servicios",
          "description": "Implementar y utilizar excepciones personalizadas en todos los servicios para mejorar la especificidad del manejo de errores.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/utils/error_handling.py` - [Usar GroqAPIError, SupabaseRPCError, etc.]\n  - `/src/services/groq_service.py` - [Reemplazar excepciones gen√©ricas]\n  - `/src/services/supabase_service.py` - [Migrar a excepciones personalizadas]\n- **üìö Documentaci√≥n Context7**:\n  - Buscar patrones de manejo de errores en servicios similares\n- **üîß Especificaciones de implementaci√≥n**:\n  - Importar excepciones personalizadas en cada servicio\n  - Reemplazar `raise Exception` por excepciones espec√≠ficas\n  - Usar decoradores `@retry_groq_api` y `@retry_supabase_rpc`\n  - Mantener retrocompatibilidad con c√≥digo existente\n  - Revisar el documento de auditor√≠a (docs/audit_error_handling_24_1.md) para implementar las recomendaciones priorizadas",
          "status": "done"
        },
        {
          "id": 24.3,
          "title": "Implementar Fallbacks en Fases del Pipeline",
          "description": "Desarrollar mecanismos de fallback para cada fase del pipeline que permitan una degradaci√≥n elegante ante fallos.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/docs/07-manejo-de-errores.md` - [Seguir estrategias de fallback por fase]\n  - `/src/pipeline/fase_1_triaje.py` - [Aplicar fallbacks fase 1]\n  - `/src/pipeline/fase_2_extraccion.py` - [Aplicar fallbacks fase 2]\n  - `/src/pipeline/fase_3_citas_datos.py` - [Aplicar fallbacks fase 3]\n  - `/src/pipeline/fase_4_normalizacion.py` - [Aplicar fallbacks fase 4]\n  - `/src/utils/error_handling.py` - [Usar handlers de fallback existentes]\n- **üìö Documentaci√≥n Context7**:\n  - Revisar patrones de degradaci√≥n elegante en sistemas distribuidos\n- **üîß Especificaciones de implementaci√≥n**:\n  - Implementar try-except con fallbacks espec√≠ficos por fase\n  - Usar funciones handle_* de error_handling.py\n  - Loggear claramente cuando se usa un fallback\n  - Asegurar que el pipeline contin√∫a con datos m√≠nimos\n  - Integrar con las excepciones personalizadas ya implementadas (ValidationError, GroqAPIError, SupabaseRPCError, ProcessingError)",
          "status": "done"
        },
        {
          "id": 24.4,
          "title": "Mejorar Logging con Contexto Enriquecido",
          "description": "Optimizar el sistema de logging para incluir contexto enriquecido que facilite el diagn√≥stico de problemas.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/utils/logging_config.py` - [Usar LogContext y log_phase]\n  - `/src/pipeline/pipeline_coordinator.py` - [Ejemplo de buen uso de logging]\n  - `/src/services/*.py` - [A√±adir contexto a todos los logs]\n- **üìö Documentaci√≥n Context7**:\n  - Buscar mejores pr√°cticas de structured logging\n- **üîß Especificaciones de implementaci√≥n**:\n  - Usar logger.bind() para a√±adir contexto (article_id, phase, etc.)\n  - Implementar format_error_for_logging() en puntos cr√≠ticos\n  - Asegurar que cada error incluye support_code para debugging\n  - A√±adir m√©tricas de tiempo de ejecuci√≥n en operaciones costosas\n  - Extender el uso de loguru a otros servicios siguiendo el patr√≥n implementado en supabase_service.py\n  - Asegurar que los logs de excepciones personalizadas (ValidationError, GroqAPIError, SupabaseRPCError, ProcessingError) incluyen toda la informaci√≥n relevante",
          "status": "done"
        },
        {
          "id": 24.5,
          "title": "Manejo de Errores en Controller y API",
          "description": "Implementar manejo de errores consistente en las capas de controller y API para garantizar respuestas adecuadas al cliente.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/controller.py` - [Integrar global exception handler]\n  - `/src/main.py` - [Configurar FastAPI exception handlers]\n  - `/src/utils/error_handling.py` - [Usar create_error_response()]\n- **üìö Documentaci√≥n Context7**:\n  - Revisar patrones de manejo de errores en FastAPI\n- **üîß Especificaciones de implementaci√≥n**:\n  - Registrar exception handlers globales en FastAPI\n  - Usar create_error_response() para respuestas consistentes\n  - Implementar request_id tracking end-to-end\n  - Validar formato de respuestas seg√∫n secci√≥n 11 de docs\n  - Configurar handlers espec√≠ficos para las excepciones personalizadas ya implementadas (ValidationError, GroqAPIError, SupabaseRPCError, ProcessingError)",
          "status": "done"
        },
        {
          "id": 24.6,
          "title": "Test de Verificaci√≥n Integral de Errores",
          "description": "Desarrollar un test integral que verifique el correcto funcionamiento de todo el sistema de manejo de errores.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/tests/test_utils/test_error_handling.py` - [Tests existentes]\n  - `/tests/demo_error_handling.py` - [Demo existente]\n  - Todos los archivos modificados en subtareas anteriores\n- **üìö Documentaci√≥n Context7**:\n  - Patrones de testing para manejo de errores\n- **üîß Especificaciones de implementaci√≥n**:\n  - Crear script `test_error_handling_integral.py`\n  - Simular fallos en cada servicio externo\n  - Verificar que los fallbacks funcionan correctamente\n  - Comprobar que los logs contienen informaci√≥n suficiente\n  - Validar respuestas de error de la API\n  - El test debe ser ejecutable con: `python tests/test_error_handling_integral.py`\n  - Incluir pruebas espec√≠ficas para las excepciones personalizadas implementadas (ValidationError, GroqAPIError, SupabaseRPCError, ProcessingError)\n  - Verificar el funcionamiento de los decoradores @retry_groq_api y @retry_supabase_rpc",
          "status": "done"
        },
        {
          "id": 24.7,
          "title": "Verificar Implementaci√≥n de Decoradores de Retry",
          "description": "Verificar la correcta implementaci√≥n de los decoradores de retry en todos los servicios y realizar ajustes si es necesario.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/src/utils/error_handling.py` - [Revisar implementaci√≥n de decoradores existentes]\n  - `/src/services/groq_service.py` - [Verificar @retry_groq_api]\n  - `/src/services/supabase_service.py` - [Verificar @retry_supabase_rpc]\n  - `/src/services/entity_normalizer.py` - [Verificar @retry_supabase_rpc]\n  - `/src/services/job_tracker_service.py` - [Aplicar decoradores si es necesario]\n- **üìö Documentaci√≥n Context7**:\n  - Patrones de retry en sistemas distribuidos\n- **üîß Especificaciones de implementaci√≥n**:\n  - Verificar que los decoradores est√©n aplicados correctamente en todos los servicios\n  - Ajustar par√°metros de retry (max_retries, backoff_factor) seg√∫n criticidad\n  - Asegurar que los decoradores loguean intentos y fallos adecuadamente\n  - Comprobar que los decoradores funcionan correctamente con las excepciones personalizadas\n  - Aplicar decoradores en job_tracker_service.py si es necesario",
          "status": "done"
        },
        {
          "id": 24.8,
          "title": "Documentar Sistema de Manejo de Errores",
          "description": "Crear documentaci√≥n completa sobre el sistema de manejo de errores implementado.",
          "details": "- **üìÅ Documentos del repositorio**:\n  - `/docs/07-manejo-de-errores.md` - [Actualizar con nuevas implementaciones]\n  - `/docs/excepciones-personalizadas.md` - [Crear nuevo documento]\n- **üìö Documentaci√≥n Context7**:\n  - Ejemplos de documentaci√≥n de sistemas de manejo de errores\n- **üîß Especificaciones de implementaci√≥n**:\n  - Documentar todas las excepciones personalizadas implementadas (ValidationError, GroqAPIError, SupabaseRPCError, ProcessingError)\n  - Explicar el funcionamiento de los decoradores @retry_groq_api y @retry_supabase_rpc\n  - Detallar las estrategias de fallback por fase del pipeline\n  - Incluir ejemplos de uso de las excepciones y decoradores\n  - Crear gu√≠a de troubleshooting para desarrolladores",
          "status": "done"
        }
      ]
    },
    {
      "id": 25,
      "title": "Implement data validation and sanitization",
      "description": "Enhance input data validation and sanitization throughout the pipeline, using existing validation patterns as a model.",
      "details": "1. Use module_scraper/scraper_core/pipelines/validation.py as a reference model for implementing pipeline-specific data validation\n2. Review all Pydantic models for comprehensive validation rules\n3. Implement custom validators for complex data types, adapting existing validation patterns\n4. Add data sanitization for user inputs (e.g., HTML escaping)\n5. Ensure all pipeline phases validate their inputs and outputs consistently with the architectural patterns\n6. Implement data integrity checks before database persistence\n7. Maintain architectural consistency between containers by following established validation patterns",
      "testStrategy": "Create a comprehensive test suite with various input data scenarios, including edge cases and malformed inputs. Verify that validation and sanitization work correctly at all stages. Ensure tests validate that the implementation follows the same architectural patterns as module_scraper/scraper_core/pipelines/validation.py.",
      "status": "pending",
      "dependencies": [
        4,
        5,
        6
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 26,
      "title": "Implement monitoring and observability",
      "description": "Add monitoring and observability features to the application for better operational insights.",
      "details": "1. Implement application metrics collection (e.g., using Prometheus)\n2. Add tracing for request flows through the pipeline (e.g., using OpenTelemetry)\n3. Enhance logging with correlation IDs for request tracking\n4. Create dashboards for key performance indicators\n5. Implement alerts for critical errors and performance degradation",
      "testStrategy": "Set up a test environment with monitoring tools. Verify correct metric collection, log correlation, and alert triggering under various scenarios.",
      "status": "pending",
      "dependencies": [
        11
      ],
      "priority": "medium",
      "subtasks": []
    }
  ]
}