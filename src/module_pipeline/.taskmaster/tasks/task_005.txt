# Task ID: 5
# Title: Implement Pydantic models for processing
# Status: done
# Dependencies: 4
# Priority: high
# Description: Create Pydantic models for processing stages in the models/procesamiento.py file.
# Details:
1. Create src/models/procesamiento.py
2. Implement models: HechoBase, HechoProcesado, EntidadBase, EntidadProcesada, CitaTextual, DatosCuantitativos
3. Implement result models: ResultadoFase1, ResultadoFase2, ResultadoFase3, ResultadoFase4
4. Use Pydantic v2 features and add appropriate validators

# Test Strategy:
Create unit tests to validate the structure and constraints of each model, ensuring they match the PRD specifications.

# Subtasks:
## 1. Definir Arquitectura de Modelos Base para el Pipeline de Procesamiento [done]
### Dependencies: None
### Description: Investigar y definir si se requiere un modelo Pydantic base personalizado (ej. PipelineBaseModel) para todos los modelos en procesamiento.py, o si heredar directamente de pydantic.BaseModel es suficiente. Documentar la decisión y, si aplica, definir la estructura de este modelo base común.
### Details:
<info added on 2025-06-05T02:50:17.403Z>
Después de analizar los requisitos del pipeline de procesamiento y los modelos Pydantic que se implementarán, he decidido crear una clase base personalizada `PipelineBaseModel` en `src/models/procesamiento.py` para centralizar la configuración común y promover la consistencia entre todos los modelos.

```python
from datetime import datetime
from typing import Optional, ClassVar, Dict, Any
from pydantic import BaseModel, Field, model_validator

class PipelineBaseModel(BaseModel):
    """
    Clase base para todos los modelos de procesamiento del pipeline.
    
    Esta clase centraliza:
    - Configuración común a través de model_config
    - Campos de auditoría (fecha_creacion, fecha_actualizacion)
    - Métodos de utilidad compartidos
    
    Justificación: Se implementa esta clase base para:
    1. Garantizar consistencia en la configuración de todos los modelos
    2. Evitar duplicación de código en campos comunes como timestamps
    3. Centralizar lógica de validación y serialización específica del pipeline
    4. Facilitar cambios futuros que afecten a todos los modelos
    """
    
    # Campos de auditoría
    fecha_creacion: datetime = Field(default_factory=datetime.utcnow)
    fecha_actualizacion: Optional[datetime] = Field(default=None)
    
    # Configuración común para todos los modelos del pipeline
    model_config: ClassVar[Dict[str, Any]] = {
        "extra": "forbid",        # Rechazar campos no definidos en el modelo
        "validate_assignment": True,  # Validar asignaciones después de la inicialización
        "populate_by_name": True,     # Permitir población por nombre de atributo
        "json_encoders": {
            datetime: lambda dt: dt.isoformat()  # Formato ISO para fechas
        }
    }
    
    @model_validator(mode='after')
    def update_modification_timestamp(self):
        """Actualiza el timestamp de modificación al validar el modelo."""
        if getattr(self, 'fecha_creacion', None) is not None:
            self.fecha_actualizacion = datetime.utcnow()
        return self
    
    def touch(self) -> None:
        """Actualiza manualmente el timestamp de modificación."""
        self.fecha_actualizacion = datetime.utcnow()
    
    def to_supabase_json(self) -> Dict[str, Any]:
        """
        Serializa el modelo para almacenamiento en Supabase.
        
        Convierte el modelo a un diccionario JSON compatible con Supabase,
        aplicando transformaciones específicas si son necesarias.
        """
        data = self.model_dump(by_alias=True)
        # Aquí se pueden aplicar transformaciones específicas para Supabase
        return data
```

Esta implementación proporciona una base sólida para todos los modelos de procesamiento, con campos de auditoría, configuración común y métodos de utilidad que serán heredados por las clases derivadas como `HechoBase`, `EntidadBase`, etc.
</info added on 2025-06-05T02:50:17.403Z>

## 2. Definir modelos Pydantic fundamentales: HechoBase y EntidadBase [done]
### Dependencies: None
### Description: Crear las estructuras para HechoBase y EntidadBase en src/models/procesamiento.py, heredando del modelo base definido en 5.1 (o pydantic.BaseModel).
### Details:
<info added on 2025-06-05T02:51:12.580Z>
Implementar las clases Pydantic `HechoBase` y `EntidadBase` en `src/models/procesamiento.py`. Estas clases servirán como modelos base para representar hechos y entidades extraídos del contenido de noticias.

Para `HechoBase`:
- Incluir campos como `id_hecho` (UUID autogenerado), `texto_original_del_hecho`, `confianza_extraccion`, `offset_inicio_hecho`, `offset_fin_hecho` y `metadata_hecho`
- Aplicar validaciones con `constr` para longitud mínima de texto y `confloat` para rangos de confianza

Para `EntidadBase`:
- Implementar campos como `id_entidad` (UUID autogenerado), `texto_entidad`, `tipo_entidad`, `relevancia_entidad`, `offset_inicio_entidad`, `offset_fin_entidad` y `metadata_entidad`
- Aplicar validaciones similares a `HechoBase`

Ambas clases deben heredar de `PipelineBaseModel` (si fue creada en la Subtarea 5.1) o directamente de `pydantic.BaseModel`.

Implementar validadores para garantizar que cuando se proporcionen ambos offsets, el offset_fin sea mayor o igual que el offset_inicio.

Asegurar compatibilidad con serialización/deserialización y crear pruebas unitarias que verifiquen la creación de instancias, valores por defecto, validaciones y manejo de errores.
</info added on 2025-06-05T02:51:12.580Z>

## 3. Definir modelos Pydantic derivados: HechoProcesado y EntidadProcesada [done]
### Dependencies: None
### Description: Extender HechoBase y EntidadBase para crear HechoProcesado (que podría incluir campos como id_fragmento_origen, id_articulo_fuente) y EntidadProcesada (que podría incluir campos como id_entidad_normalizada, uri_wikidata).
### Details:
<info added on 2025-06-05T02:52:00.987Z>
# Implementación de modelos Pydantic para procesamiento

## Objetivo
Crear los modelos Pydantic `HechoProcesado` y `EntidadProcesada` en `src/models/procesamiento.py` que extiendan `HechoBase` y `EntidadBase` respectivamente, añadiendo campos específicos para el procesamiento.

## Estructura de los modelos

### HechoProcesado(HechoBase)
- `id_fragmento_origen`: UUID del fragmento de origen
- `id_articulo_fuente`: UUID opcional del artículo original
- `vinculado_a_entidades`: Lista de IDs de entidades relacionadas
- `prompt_utilizado`: Prompt opcional usado para extraer el hecho
- `respuesta_llm_bruta`: Respuesta completa del LLM (como Dict)

### EntidadProcesada(EntidadBase)
- `id_fragmento_origen`: UUID del fragmento de origen
- `id_entidad_normalizada`: UUID opcional de la entidad canónica
- `nombre_entidad_normalizada`: Nombre opcional de la entidad canónica
- `uri_wikidata`: HttpUrl opcional para la entidad en Wikidata
- `similitud_normalizacion`: Puntuación de similitud (0.0 a 1.0)
- `prompt_utilizado_normalizacion`: Prompt opcional usado para normalización

## Consideraciones técnicas
- Importar tipos necesarios: UUID, Optional, List, Dict, Any, Field, HttpUrl
- Asegurar la trazabilidad mediante el campo `id_fragmento_origen`
- Campos de normalización opcionales para manejar casos donde no sea exitosa
- Implementar validadores apropiados (ej. HttpUrl, rangos numéricos)
- Mantener consistencia con la nomenclatura y estilo del proyecto

## Criterios de aceptación
- Herencia correcta de las clases base
- Definición completa de campos con tipos y restricciones
- Compatibilidad con serialización/deserialización
- Pruebas unitarias para validar herencia, campos y creación de instancias
</info added on 2025-06-05T02:52:00.987Z>

## 4. Definir modelos Pydantic de datos específicos: CitaTextual y DatosCuantitativos [done]
### Dependencies: None
### Description: Crear los modelos CitaTextual (para almacenar citas directas y su contexto) y DatosCuantitativos (para cifras, estadísticas, etc.).
### Details:
<info added on 2025-06-05T02:52:49.200Z>
```python
import uuid
from uuid import UUID
from typing import Optional, Dict, Any
from pydantic import Field, constr, BaseModel

# Usar PipelineBaseModel si existe, de lo contrario usar BaseModel
# Asumimos que PipelineBaseModel ya está definido en el mismo archivo
# Si no existe, se debe usar BaseModel directamente

class CitaTextual(PipelineBaseModel):  # o BaseModel si PipelineBaseModel no existe
    id_cita: UUID = Field(default_factory=uuid.uuid4, description="Identificador único de la cita textual.")
    id_fragmento_origen: UUID = Field(..., description="ID del FragmentoProcesableItem del cual se extrajo esta cita.")
    texto_cita: constr(min_length=5) = Field(..., description="El contenido textual exacto de la cita.")
    persona_citada: Optional[str] = Field(default=None, description="Nombre de la persona o entidad que realiza la cita.")
    id_entidad_citada: Optional[UUID] = Field(default=None, description="ID de la EntidadProcesada (persona/organización) que realiza la cita, si está identificada.")
    offset_inicio_cita: Optional[int] = Field(default=None, description="Posición inicial de la cita en el texto original del fragmento.", ge=0)
    offset_fin_cita: Optional[int] = Field(default=None, description="Posición final de la cita en el texto original del fragmento.", ge=0)
    contexto_cita: Optional[str] = Field(default=None, description="Contexto breve que rodea la cita para mejor entendimiento.")
    metadata_cita: Dict[str, Any] = Field(default_factory=dict, description="Metadatos adicionales sobre la cita.")

class DatosCuantitativos(PipelineBaseModel):  # o BaseModel si PipelineBaseModel no existe
    id_dato_cuantitativo: UUID = Field(default_factory=uuid.uuid4, description="Identificador único del dato cuantitativo.")
    id_fragmento_origen: UUID = Field(..., description="ID del FragmentoProcesableItem del cual se extrajo este dato.")
    descripcion_dato: constr(min_length=3) = Field(..., description="Descripción del dato cuantitativo (ej: 'Número de empleados', 'Porcentaje de aumento').")
    valor_dato: float = Field(..., description="Valor numérico del dato.")
    unidad_dato: Optional[str] = Field(default=None, description="Unidad de medida del dato (ej: 'millones', '%', 'USD').")
    fecha_dato: Optional[str] = Field(default=None, description="Fecha o período al que se refiere el dato (ej: '2023-Q4', 'anual').")
    fuente_especifica_dato: Optional[str] = Field(default=None, description="Fuente específica mencionada para este dato dentro del texto, si la hay.")
    offset_inicio_dato: Optional[int] = Field(default=None, description="Posición inicial del dato en el texto original del fragmento.", ge=0)
    offset_fin_dato: Optional[int] = Field(default=None, description="Posición final del dato en el texto original del fragmento.", ge=0)
    metadata_dato: Dict[str, Any] = Field(default_factory=dict, description="Metadatos adicionales sobre el dato cuantitativo.")
```
</info added on 2025-06-05T02:52:49.200Z>

## 5. Definir modelos Pydantic de resultado de fase: ResultadoFase1Triaje y ResultadoFase2Extraccion [done]
### Dependencies: None
### Description: Crear modelos para encapsular los resultados de las dos primeras fases del pipeline.
### Details:
<info added on 2025-06-05T02:53:37.183Z>
```python
import uuid
from uuid import UUID
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field

# Importar modelos existentes (asumiendo que están definidos)
# Si PipelineBaseModel existe, usarlo como base, de lo contrario usar BaseModel
try:
    from .procesamiento import PipelineBaseModel, HechoProcesado, EntidadProcesada
    BaseModelClass = PipelineBaseModel
except ImportError:
    BaseModelClass = BaseModel
    # Importar HechoProcesado y EntidadProcesada si están en otro módulo
    # from .otro_modulo import HechoProcesado, EntidadProcesada

class ResultadoFase1Triaje(BaseModelClass):
    id_resultado_triaje: UUID = Field(default_factory=uuid.uuid4, description="ID único del resultado de esta fase de triaje.")
    id_fragmento: UUID = Field(..., description="ID del FragmentoProcesableItem que fue triado.")
    es_relevante: bool = Field(..., description="Indica si el fragmento fue considerado relevante por el LLM.")
    justificacion_triaje: Optional[str] = Field(default=None, description="Explicación o justificación proporcionada por el LLM para la decisión de relevancia.")
    categoria_principal: Optional[str] = Field(default=None, description="Categoría principal asignada al fragmento durante el triaje.")
    palabras_clave_triaje: List[str] = Field(default_factory=list, description="Lista de palabras clave identificadas en el fragmento durante el triaje.")
    confianza_triaje: Optional[float] = Field(default=None, description="Nivel de confianza del LLM en la decisión de triaje (0.0 a 1.0).", ge=0.0, le=1.0)
    metadata_triaje: Dict[str, Any] = Field(default_factory=dict, description="Metadatos adicionales específicos de la fase de triaje.")

class ResultadoFase2Extraccion(BaseModelClass):
    id_resultado_extraccion: UUID = Field(default_factory=uuid.uuid4, description="ID único del resultado de esta fase de extracción.")
    id_fragmento: UUID = Field(..., description="ID del FragmentoProcesableItem del cual se extrajeron datos.")
    hechos_extraidos: List[HechoProcesado] = Field(default_factory=list, description="Lista de hechos procesados extraídos del fragmento.")
    entidades_extraidas: List[EntidadProcesada] = Field(default_factory=list, description="Lista de entidades procesadas extraídas del fragmento.")
    resumen_extraccion: Optional[str] = Field(default=None, description="Resumen generado por el LLM a partir de la información extraída.")
    prompt_extraccion_usado: Optional[str] = Field(default=None, description="El prompt específico utilizado para la fase de extracción.")
    advertencias_extraccion: List[str] = Field(default_factory=list, description="Posibles advertencias o problemas identificados durante la extracción.")
    metadata_extraccion: Dict[str, Any] = Field(default_factory=dict, description="Metadatos adicionales específicos de la fase de extracción.")
```
</info added on 2025-06-05T02:53:37.183Z>

## 6. Definir modelos Pydantic de resultado de fase: ResultadoFase3CitasDatos y ResultadoFase4Normalizacion [done]
### Dependencies: None
### Description: Crear modelos para encapsular los resultados de las dos últimas fases del pipeline.
### Details:
<info added on 2025-06-05T02:54:20.348Z>
import uuid
from typing import List, Dict, Any, Optional
from uuid import UUID
from pydantic import Field, BaseModel

# Asumiendo que estas clases ya existen en el archivo
# from src.models.procesamiento import PipelineBaseModel, CitaTextual, DatosCuantitativos, EntidadProcesada
# from src.models.entrada import FragmentoProcesableItem

# Si existe PipelineBaseModel, usar esa como base, de lo contrario usar BaseModel
# Para este ejemplo, usaré BaseModel directamente

class ResultadoFase3CitasDatos(BaseModel):
    id_resultado_citas_datos: UUID = Field(default_factory=uuid.uuid4, description="ID único del resultado de esta fase de citas y datos.")
    id_fragmento: UUID = Field(..., description="ID del FragmentoProcesableItem procesado.")
    citas_textuales_extraidas: List[CitaTextual] = Field(default_factory=list, description="Lista de citas textuales identificadas en el fragmento.")
    datos_cuantitativos_extraidos: List[DatosCuantitativos] = Field(default_factory=list, description="Lista de datos cuantitativos identificados en el fragmento.")
    prompt_citas_datos_usado: Optional[str] = Field(default=None, description="Prompt específico utilizado para la extracción de citas y datos.")
    advertencias_citas_datos: List[str] = Field(default_factory=list, description="Posibles advertencias durante la extracción de citas y datos.")
    metadata_citas_datos: Dict[str, Any] = Field(default_factory=dict, description="Metadatos adicionales de la fase de citas y datos.")

class ResultadoFase4Normalizacion(BaseModel):
    id_resultado_normalizacion: UUID = Field(default_factory=uuid.uuid4, description="ID único del resultado de esta fase de normalización.")
    id_fragmento: UUID = Field(..., description="ID del FragmentoProcesableItem cuyas entidades fueron normalizadas.")
    entidades_normalizadas: List[EntidadProcesada] = Field(default_factory=list, description="Lista de entidades procesadas que ahora incluyen información de normalización.")
    resumen_normalizacion: Optional[str] = Field(default=None, description="Resumen del proceso de normalización para este fragmento.")
    prompt_normalizacion_usado: Optional[str] = Field(default=None, description="Prompt específico utilizado para la fase de normalización (si aplica).")
    estado_general_normalizacion: str = Field(..., description="Estado general del proceso de normalización (ej: 'Completo', 'Parcial', 'Fallido', 'No Requerido').")
    metadata_normalizacion: Dict[str, Any] = Field(default_factory=dict, description="Metadatos adicionales de la fase de normalización.")
</info added on 2025-06-05T02:54:20.348Z>

