"""
Test de Concurrencia del Pipeline
=================================

Tests exhaustivos que verifican el comportamiento del sistema bajo condiciones
de alta concurrencia, incluyendo:
- Race conditions en m√©tricas compartidas
- Procesamiento paralelo masivo
- L√≠mites de concurrencia del sistema
- Integridad de datos bajo concurrencia

Ejecutar con: python -m pytest tests/test_concurrency.py -v -s

ADVERTENCIA: Estos tests generan alta carga en el sistema.
"""

import pytest
import asyncio
import aiohttp
import threading
import time
import random
import uuid
from typing import List, Dict, Any, Set
from dataclasses import dataclass, field
from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing
from pathlib import Path

# Configuraci√≥n del sistema
import sys
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.config import API_HOST, API_PORT
from src.controller import PipelineController
from src.services.job_tracker_service import get_job_tracker_service, JobStatus
from src.monitoring.metrics_collector import get_metrics_collector


@dataclass
class ConcurrencyTestResult:
    """Resultado de un test de concurrencia."""
    test_name: str
    total_operations: int = 0
    successful_operations: int = 0
    failed_operations: int = 0
    race_conditions_detected: int = 0
    data_inconsistencies: List[str] = field(default_factory=list)
    duplicate_ids: Set[str] = field(default_factory=set)
    missing_data: List[str] = field(default_factory=list)
    response_times: List[float] = field(default_factory=list)
    errors: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    
    def add_error(self, error_type: str):
        """Registra un tipo de error."""
        self.errors[error_type] += 1
    
    def calculate_success_rate(self) -> float:
        """Calcula la tasa de √©xito."""
        if self.total_operations == 0:
            return 0.0
        return (self.successful_operations / self.total_operations) * 100
    
    def print_summary(self):
        """Imprime un resumen del resultado."""
        print(f"\n{'='*60}")
        print(f"RESUMEN: {self.test_name}")
        print(f"{'='*60}")
        print(f"Total operaciones:      {self.total_operations}")
        print(f"Exitosas:              {self.successful_operations} ({self.calculate_success_rate():.1f}%)")
        print(f"Fallidas:              {self.failed_operations}")
        print(f"Race conditions:       {self.race_conditions_detected}")
        print(f"Inconsistencias:       {len(self.data_inconsistencies)}")
        print(f"IDs duplicados:        {len(self.duplicate_ids)}")
        print(f"Datos faltantes:       {len(self.missing_data)}")
        
        if self.errors:
            print(f"\nErrores por tipo:")
            for error_type, count in self.errors.items():
                print(f"  {error_type}: {count}")
        
        if self.response_times:
            avg_time = sum(self.response_times) / len(self.response_times)
            print(f"\nTiempo respuesta promedio: {avg_time*1000:.1f}ms")
        
        print(f"{'='*60}")


class TestConcurrency:
    """Suite de tests de concurrencia del pipeline."""
    
    @pytest.fixture(scope="class")
    def api_base_url(self):
        """URL base del API."""
        return f"http://{API_HOST}:{API_PORT}"
    
    @pytest.fixture(scope="class")
    def sample_article_factory(self):
        """Factory para generar art√≠culos √∫nicos."""
        def create_article(article_id: str, size: int = 5000) -> Dict[str, Any]:
            content = f"Art√≠culo de prueba #{article_id}. " * (size // 30)
            return {
                "medio": f"Test News {article_id}",
                "pais_publicacion": "Espa√±a",
                "tipo_medio": "Digital",
                "titular": f"Test Concurrencia - Art√≠culo {article_id}",
                "fecha_publicacion": "2024-01-15T10:00:00Z",
                "contenido_texto": content[:size],
                "idioma": "es",
                "autor": f"Tester {article_id}",
                "url": f"https://test.example.com/concurrent-{article_id}",
                "seccion": "Testing",
                "es_opinion": False,
                "es_oficial": True,
                "metadata": {
                    "test_id": article_id,
                    "test_type": "concurrency"
                }
            }
        return create_article
    
    @pytest.fixture(scope="class")
    def sample_fragment_factory(self):
        """Factory para generar fragmentos √∫nicos."""
        def create_fragment(fragment_id: str, size: int = 1000) -> Dict[str, Any]:
            content = f"Fragmento de prueba #{fragment_id}. " * (size // 30)
            return {
                "id_fragmento": f"test_concurrent_{fragment_id}",
                "texto_original": content[:size],
                "id_articulo_fuente": f"concurrent_article_{fragment_id}",
                "orden_en_articulo": 1,
                "metadata_adicional": {
                    "test_id": fragment_id,
                    "test_type": "concurrency"
                }
            }
        return create_fragment
    
    async def _send_request(self, session: aiohttp.ClientSession, endpoint: str, data: Dict[str, Any]):
        """Env√≠a una request y retorna (response_time, status_code, response_data)."""
        start_time = time.time()
        try:
            async with session.post(endpoint, json=data, timeout=aiohttp.ClientTimeout(total=60)) as response:
                response_time = time.time() - start_time
                response_data = await response.json()
                return response_time, response.status, response_data
        except asyncio.TimeoutError:
            return time.time() - start_time, 504, {"error": "timeout"}
        except Exception as e:
            return time.time() - start_time, 500, {"error": str(e)}
    
    async def _send_article(self, session: aiohttp.ClientSession, endpoint: str, article: Dict[str, Any]):
        """Env√≠a un art√≠culo y retorna la respuesta."""
        return await self._send_request(session, endpoint, article)
    
    async def _get_metrics(self, base_url: str) -> str:
        """Obtiene las m√©tricas del sistema."""
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{base_url}/metrics") as response:
                return await response.text()
    
    def _extract_metric_value(self, metrics_text: str, metric_name: str) -> float:
        """Extrae el valor de una m√©trica espec√≠fica del texto Prometheus."""
        lines = metrics_text.split('\n')
        for line in lines:
            if line.startswith(metric_name) and not line.startswith('#'):
                parts = line.split()
                if len(parts) >= 2:
                    try:
                        return float(parts[1])
                    except ValueError:
                        pass
        return 0.0
    
    @pytest.mark.asyncio
    async def test_race_condition_metrics(self, api_base_url, sample_article_factory):
        """
        Test 1: Detecta race conditions en el sistema de m√©tricas.
        
        Seg√∫n CONCURRENCY_ANALYSIS.md, hay un problema identificado en
        PipelineController.metrics que no est√° sincronizado.
        """
        print("\n" + "="*80)
        print("TEST 1: RACE CONDITIONS EN M√âTRICAS")
        print("="*80)
        
        result = ConcurrencyTestResult("Race Conditions en M√©tricas")
        
        # Obtener m√©tricas iniciales
        initial_metrics_response = await self._get_metrics(api_base_url)
        initial_articles = self._extract_metric_value(initial_metrics_response, "pipeline_articles_processed_total")
        
        print(f"üìä M√©tricas iniciales: {initial_articles} art√≠culos procesados")
        
        # Enviar m√∫ltiples requests concurrentemente
        num_concurrent_requests = 50
        articles = [sample_article_factory(str(i)) for i in range(num_concurrent_requests)]
        
        print(f"\nüöÄ Enviando {num_concurrent_requests} requests concurrentes...")
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for article in articles:
                task = self._send_article(session, f"{api_base_url}/procesar_articulo", article)
                tasks.append(task)
            
            # Ejecutar todas las tareas concurrentemente
            responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Analizar respuestas
        successful_requests = 0
        for i, response in enumerate(responses):
            result.total_operations += 1
            
            if isinstance(response, Exception):
                result.failed_operations += 1
                result.add_error(type(response).__name__)
            elif response[1] == 200:  # status_code
                result.successful_operations += 1
                successful_requests += 1
            else:
                result.failed_operations += 1
                result.add_error(f"HTTP_{response[1]}")
        
        # Esperar un momento para que se actualicen las m√©tricas
        await asyncio.sleep(2)
        
        # Obtener m√©tricas finales
        final_metrics_response = await self._get_metrics(api_base_url)
        final_articles = self._extract_metric_value(final_metrics_response, "pipeline_articles_processed_total")
        
        # Calcular diferencia
        articles_processed = final_articles - initial_articles
        
        print(f"\nüìä M√©tricas finales: {final_articles} art√≠culos procesados")
        print(f"üìä Diferencia: {articles_processed} art√≠culos")
        print(f"‚úÖ Requests exitosas: {successful_requests}")
        
        # Detectar race condition
        if articles_processed != successful_requests and successful_requests > 0:
            result.race_conditions_detected = abs(articles_processed - successful_requests)
            result.data_inconsistencies.append(
                f"M√©tricas reportan {articles_processed} art√≠culos pero hubo {successful_requests} exitosos"
            )
            print(f"\n‚ö†Ô∏è  RACE CONDITION DETECTADA!")
            print(f"   Diferencia: {result.race_conditions_detected} art√≠culos")
        else:
            print(f"\n‚úÖ No se detectaron race conditions en m√©tricas")
        
        # Verificar tambi√©n las m√©tricas de fragmentos y errores
        total_errors = self._extract_metric_value(final_metrics_response, "pipeline_errors_total")
        if total_errors < result.failed_operations:
            result.data_inconsistencies.append(
                f"M√©tricas reportan {total_errors} errores pero hubo {result.failed_operations} fallos"
            )
        
        result.print_summary()
        
        # Assertion suave - esperamos encontrar la race condition documentada
        if result.race_conditions_detected > 0:
            print("\n‚ö†Ô∏è  Race condition confirmada (esperada seg√∫n documentaci√≥n)")
        
        assert result.successful_operations > 0, "No se proces√≥ ning√∫n art√≠culo exitosamente"
    
    @pytest.mark.asyncio
    async def test_procesamiento_paralelo_masivo(self, api_base_url, sample_article_factory, sample_fragment_factory):
        """
        Test 2: Procesamiento paralelo masivo con art√≠culos y fragmentos.
        
        Eval√∫a la capacidad del sistema para manejar cientos de requests
        simult√°neas sin perder datos o generar duplicados.
        """
        print("\n" + "="*80)
        print("TEST 2: PROCESAMIENTO PARALELO MASIVO")
        print("="*80)
        
        result = ConcurrencyTestResult("Procesamiento Paralelo Masivo")
        
        # Configuraci√≥n del test
        num_articles = 100
        num_fragments = 100
        max_concurrent = 50  # Limitar concurrencia para no saturar
        
        print(f"üìã Configuraci√≥n:")
        print(f"   - Art√≠culos: {num_articles}")
        print(f"   - Fragmentos: {num_fragments}")
        print(f"   - Concurrencia m√°xima: {max_concurrent}")
        
        # Generar datos √∫nicos
        articles = [sample_article_factory(f"massive_{i}") for i in range(num_articles)]
        fragments = [sample_fragment_factory(f"massive_{i}") for i in range(num_fragments)]
        
        # Rastrear IDs esperados
        expected_article_ids = set()
        expected_fragment_ids = {f"test_concurrent_massive_{i}" for i in range(num_fragments)}
        
        # Mezclar art√≠culos y fragmentos
        all_requests = []
        for i, article in enumerate(articles):
            all_requests.append(("article", article, i))
        for i, fragment in enumerate(fragments):
            all_requests.append(("fragment", fragment, i))
        
        # Aleatorizar orden
        random.shuffle(all_requests)
        
        print(f"\nüöÄ Iniciando procesamiento masivo...")
        start_time = time.time()
        
        # Procesar con l√≠mite de concurrencia
        semaphore = asyncio.Semaphore(max_concurrent)
        received_ids = set()
        received_fragment_ids = set()
        
        async def process_item(session, item_type, data, index):
            async with semaphore:
                if item_type == "article":
                    endpoint = f"{api_base_url}/procesar_articulo"
                else:
                    endpoint = f"{api_base_url}/procesar_fragmento"
                
                response_time, status_code, response_data = await self._send_request(
                    session, endpoint, data
                )
                
                return item_type, index, response_time, status_code, response_data
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for item_type, data, index in all_requests:
                task = process_item(session, item_type, data, index)
                tasks.append(task)
            
            # Procesar todas las tareas
            responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_time = time.time() - start_time
        
        # Analizar respuestas
        for response in responses:
            if isinstance(response, Exception):
                result.failed_operations += 1
                result.add_error(type(response).__name__)
                continue
            
            item_type, index, response_time, status_code, response_data = response
            result.total_operations += 1
            result.response_times.append(response_time)
            
            if status_code == 200:
                result.successful_operations += 1
                
                # Rastrear IDs recibidos
                if item_type == "fragment" and "data" in response_data:
                    frag_id = response_data["data"].get("fragmento_id")
                    if frag_id:
                        if frag_id in received_fragment_ids:
                            result.duplicate_ids.add(frag_id)
                        received_fragment_ids.add(frag_id)
            else:
                result.failed_operations += 1
                result.add_error(f"HTTP_{status_code}")
        
        # Verificar integridad de datos
        missing_fragments = expected_fragment_ids - received_fragment_ids
        for missing_id in missing_fragments:
            result.missing_data.append(f"Fragment ID: {missing_id}")
        
        # Estad√≠sticas
        print(f"\nüìä Procesamiento completado en {total_time:.2f} segundos")
        print(f"üìä Throughput: {result.total_operations/total_time:.2f} ops/segundo")
        
        if result.duplicate_ids:
            print(f"\n‚ö†Ô∏è  Se encontraron {len(result.duplicate_ids)} IDs duplicados:")
            for dup_id in list(result.duplicate_ids)[:5]:  # Mostrar solo primeros 5
                print(f"   - {dup_id}")
        
        if result.missing_data:
            print(f"\n‚ö†Ô∏è  Faltan {len(result.missing_data)} elementos:")
            for missing in result.missing_data[:5]:  # Mostrar solo primeros 5
                print(f"   - {missing}")
        
        result.print_summary()
        
        # Assertions
        assert result.calculate_success_rate() > 90, f"Tasa de √©xito muy baja: {result.calculate_success_rate():.1f}%"
        assert len(result.duplicate_ids) == 0, f"Se encontraron {len(result.duplicate_ids)} IDs duplicados"
    
    @pytest.mark.asyncio
    async def test_limites_concurrencia_sistema(self, api_base_url, sample_article_factory):
        """
        Test 3: Encuentra los l√≠mites de concurrencia del sistema.
        
        Incrementa gradualmente la concurrencia hasta encontrar el punto
        donde el sistema comienza a rechazar conexiones o degradarse.
        """
        print("\n" + "="*80)
        print("TEST 3: L√çMITES DE CONCURRENCIA DEL SISTEMA")
        print("="*80)
        
        result = ConcurrencyTestResult("L√≠mites de Concurrencia")
        
        # Configuraci√≥n
        initial_concurrency = 10
        max_concurrency = 200
        increment = 10
        test_duration = 10  # segundos por nivel
        
        print(f"üìã Configuraci√≥n:")
        print(f"   - Concurrencia inicial: {initial_concurrency}")
        print(f"   - Concurrencia m√°xima: {max_concurrency}")
        print(f"   - Incremento: {increment}")
        
        current_concurrency = initial_concurrency
        concurrency_results = []
        
        while current_concurrency <= max_concurrency:
            print(f"\nüîÑ Probando con {current_concurrency} conexiones concurrentes...")
            
            level_result = {
                "concurrency": current_concurrency,
                "successful": 0,
                "failed": 0,
                "errors": defaultdict(int),
                "avg_response_time": 0
            }
            
            # Crear workers concurrentes
            async def worker(worker_id: int, session: aiohttp.ClientSession):
                worker_successful = 0
                worker_failed = 0
                worker_times = []
                
                end_time = time.time() + test_duration
                
                while time.time() < end_time:
                    article = sample_article_factory(f"limit_{worker_id}_{int(time.time())}")
                    
                    try:
                        response_time, status_code, _ = await self._send_request(
                            session, f"{api_base_url}/procesar_articulo", article
                        )
                        
                        worker_times.append(response_time)
                        
                        if status_code == 200:
                            worker_successful += 1
                        else:
                            worker_failed += 1
                            level_result["errors"][f"HTTP_{status_code}"] += 1
                    except Exception as e:
                        worker_failed += 1
                        level_result["errors"][type(e).__name__] += 1
                    
                    # Peque√±a pausa entre requests
                    await asyncio.sleep(0.1)
                
                return worker_successful, worker_failed, worker_times
            
            # Ejecutar workers
            connector = aiohttp.TCPConnector(limit=current_concurrency)
            async with aiohttp.ClientSession(connector=connector) as session:
                workers = [worker(i, session) for i in range(current_concurrency)]
                results = await asyncio.gather(*workers, return_exceptions=True)
            
            # Analizar resultados del nivel
            all_response_times = []
            for worker_result in results:
                if isinstance(worker_result, Exception):
                    level_result["errors"][type(worker_result).__name__] += 1
                else:
                    successful, failed, times = worker_result
                    level_result["successful"] += successful
                    level_result["failed"] += failed
                    all_response_times.extend(times)
            
            if all_response_times:
                level_result["avg_response_time"] = sum(all_response_times) / len(all_response_times)
            
            # Calcular tasa de √©xito
            total = level_result["successful"] + level_result["failed"]
            success_rate = (level_result["successful"] / total * 100) if total > 0 else 0
            
            print(f"  ‚úÖ Exitosas: {level_result['successful']}")
            print(f"  ‚ùå Fallidas: {level_result['failed']}")
            print(f"  üìä Tasa √©xito: {success_rate:.1f}%")
            print(f"  ‚è±Ô∏è  Tiempo respuesta promedio: {level_result['avg_response_time']*1000:.1f}ms")
            
            concurrency_results.append(level_result)
            
            # Si la tasa de √©xito cae por debajo del 80%, hemos encontrado el l√≠mite
            if success_rate < 80:
                print(f"\n‚ö†Ô∏è  L√≠mite encontrado en {current_concurrency} conexiones concurrentes")
                break
            
            current_concurrency += increment
        
        # An√°lisis de resultados
        print("\nüìä RESUMEN DE L√çMITES DE CONCURRENCIA:")
        print("-" * 70)
        print(f"{'Concurrencia':<15} {'Exitosas':<10} {'Fallidas':<10} {'Tasa √âxito':<15} {'Tiempo Avg':<15}")
        print("-" * 70)
        
        optimal_concurrency = initial_concurrency
        for level in concurrency_results:
            total = level["successful"] + level["failed"]
            success_rate = (level["successful"] / total * 100) if total > 0 else 0
            
            print(f"{level['concurrency']:<15} {level['successful']:<10} {level['failed']:<10} "
                  f"{success_rate:<15.1f}% {level['avg_response_time']*1000:<15.1f}ms")
            
            # Encontrar concurrencia √≥ptima (√∫ltima con >95% √©xito)
            if success_rate >= 95:
                optimal_concurrency = level["concurrency"]
        
        print("-" * 70)
        print(f"\nüéØ Concurrencia √≥ptima recomendada: {optimal_concurrency} conexiones")
        
        result.print_summary()
        
        assert optimal_concurrency >= 20, f"Sistema soporta muy poca concurrencia: {optimal_concurrency}"
    
    @pytest.mark.asyncio
    async def test_integridad_datos_concurrencia(self, api_base_url, sample_fragment_factory):
        """
        Test 4: Verifica la integridad de datos bajo alta concurrencia.
        
        Env√≠a fragmentos con IDs secuenciales y verifica que todos sean
        procesados correctamente sin p√©rdidas o duplicaciones.
        """
        print("\n" + "="*80)
        print("TEST 4: INTEGRIDAD DE DATOS BAJO CONCURRENCIA")
        print("="*80)
        
        result = ConcurrencyTestResult("Integridad de Datos")
        
        # Configuraci√≥n
        num_fragments = 200
        batch_size = 50
        
        print(f"üìã Configuraci√≥n:")
        print(f"   - Total fragmentos: {num_fragments}")
        print(f"   - Tama√±o de batch: {batch_size}")
        
        # Generar fragmentos con IDs secuenciales
        fragments = []
        for i in range(num_fragments):
            fragment = sample_fragment_factory(f"integrity_{i:04d}", size=500)
            fragments.append(fragment)
        
        # Dividir en batches
        batches = [fragments[i:i+batch_size] for i in range(0, len(fragments), batch_size)]
        
        # Procesar cada batch concurrentemente
        all_received_ids = set()
        all_job_ids = set()
        
        for batch_num, batch in enumerate(batches):
            print(f"\nüîÑ Procesando batch {batch_num + 1}/{len(batches)}...")
            
            async with aiohttp.ClientSession() as session:
                tasks = []
                for fragment in batch:
                    endpoint = f"{api_base_url}/procesar_fragmento"
                    task = self._send_request(session, endpoint, fragment)
                    tasks.append(task)
                
                responses = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Analizar respuestas del batch
            for i, response in enumerate(responses):
                if isinstance(response, Exception):
                    result.failed_operations += 1
                    result.add_error(type(response).__name__)
                    continue
                
                response_time, status_code, response_data = response
                result.total_operations += 1
                result.response_times.append(response_time)
                
                if status_code == 200:
                    result.successful_operations += 1
                    
                    # Extraer IDs
                    if "data" in response_data:
                        data = response_data["data"]
                        
                        # Verificar fragmento_id
                        frag_id = data.get("fragmento_id")
                        if frag_id:
                            if frag_id in all_received_ids:
                                result.duplicate_ids.add(frag_id)
                            all_received_ids.add(frag_id)
                        
                        # Verificar job_id
                        job_id = response_data.get("job_id")
                        if job_id:
                            if job_id in all_job_ids:
                                result.data_inconsistencies.append(f"Job ID duplicado: {job_id}")
                            all_job_ids.add(job_id)
                else:
                    result.failed_operations += 1
                    result.add_error(f"HTTP_{status_code}")
            
            # Peque√±a pausa entre batches
            await asyncio.sleep(1)
        
        # Verificar integridad
        expected_ids = {f"test_concurrent_integrity_{i:04d}" for i in range(num_fragments)}
        missing_ids = expected_ids - all_received_ids
        unexpected_ids = all_received_ids - expected_ids
        
        print(f"\nüìä An√°lisis de integridad:")
        print(f"   - IDs esperados: {len(expected_ids)}")
        print(f"   - IDs recibidos: {len(all_received_ids)}")
        print(f"   - IDs faltantes: {len(missing_ids)}")
        print(f"   - IDs inesperados: {len(unexpected_ids)}")
        print(f"   - IDs duplicados: {len(result.duplicate_ids)}")
        print(f"   - Jobs √∫nicos: {len(all_job_ids)}")
        
        if missing_ids:
            print(f"\n‚ö†Ô∏è  IDs faltantes (primeros 10):")
            for missing_id in list(missing_ids)[:10]:
                print(f"   - {missing_id}")
                result.missing_data.append(f"ID: {missing_id}")
        
        if unexpected_ids:
            print(f"\n‚ö†Ô∏è  IDs inesperados (primeros 10):")
            for unexpected_id in list(unexpected_ids)[:10]:
                print(f"   - {unexpected_id}")
                result.data_inconsistencies.append(f"ID inesperado: {unexpected_id}")
        
        result.print_summary()
        
        # Assertions estrictas para integridad
        assert len(result.duplicate_ids) == 0, f"Se encontraron {len(result.duplicate_ids)} IDs duplicados"
        assert len(missing_ids) == 0, f"Faltan {len(missing_ids)} IDs"
        assert len(unexpected_ids) == 0, f"Se encontraron {len(unexpected_ids)} IDs inesperados"
        assert result.calculate_success_rate() > 95, f"Tasa de √©xito muy baja: {result.calculate_success_rate():.1f}%"
    
    @pytest.mark.asyncio
    async def test_job_tracker_concurrency(self, api_base_url, sample_article_factory):
        """
        Test 5: Verifica que el JobTrackerService maneja correctamente la concurrencia.
        
        Seg√∫n el an√°lisis, JobTrackerService es thread-safe, pero verificamos
        que funcione correctamente bajo alta carga.
        """
        print("\n" + "="*80)
        print("TEST 5: CONCURRENCIA EN JOB TRACKER SERVICE")
        print("="*80)
        
        result = ConcurrencyTestResult("Job Tracker Concurrency")
        
        # Enviar art√≠culos grandes para procesamiento as√≠ncrono
        num_async_articles = 30
        articles = []
        
        for i in range(num_async_articles):
            # Crear art√≠culos grandes (>10KB) para forzar procesamiento as√≠ncrono
            article = sample_article_factory(f"async_{i}", size=15000)
            articles.append(article)
        
        print(f"üìã Enviando {num_async_articles} art√≠culos grandes para procesamiento as√≠ncrono...")
        
        # Enviar todos los art√≠culos concurrentemente
        job_ids = []
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for article in articles:
                endpoint = f"{api_base_url}/procesar_articulo"
                task = self._send_request(session, endpoint, article)
                tasks.append(task)
            
            responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Recolectar job_ids
        for response in responses:
            if not isinstance(response, Exception):
                _, status_code, response_data = response
                if status_code == 200 and "job_id" in response_data:
                    job_ids.append(response_data["job_id"])
        
        print(f"‚úÖ Se crearon {len(job_ids)} jobs as√≠ncronos")
        
        # Ahora consultar todos los jobs concurrentemente
        print(f"\nüîÑ Consultando estado de todos los jobs concurrentemente...")
        
        async def check_job_status(session, job_id):
            endpoint = f"{api_base_url}/status/{job_id}"
            try:
                async with session.get(endpoint) as response:
                    if response.status == 200:
                        data = await response.json()
                        return job_id, "found", data
                    else:
                        return job_id, "not_found", None
            except Exception as e:
                return job_id, "error", str(e)
        
        # Consultar todos los jobs m√∫ltiples veces
        for round_num in range(3):
            print(f"\n  Ronda {round_num + 1}/3 de consultas...")
            
            async with aiohttp.ClientSession() as session:
                tasks = []
                for job_id in job_ids:
                    task = check_job_status(session, job_id)
                    tasks.append(task)
                
                results = await asyncio.gather(*tasks)
            
            # Analizar resultados
            found_count = 0
            status_counts = defaultdict(int)
            
            for job_id, status, data in results:
                if status == "found":
                    found_count += 1
                    if data and "data" in data:
                        job_status = data["data"].get("status", "unknown")
                        status_counts[job_status] += 1
                else:
                    result.data_inconsistencies.append(f"Job {job_id} status: {status}")
            
            print(f"  ‚úÖ Jobs encontrados: {found_count}/{len(job_ids)}")
            print(f"  üìä Estados: {dict(status_counts)}")
            
            # Peque√±a pausa entre rondas
            await asyncio.sleep(2)
        
        # Verificar que no hay duplicados o p√©rdidas en el tracking
        print(f"\nüìä Verificaci√≥n final del Job Tracker:")
        print(f"   - Jobs creados: {len(job_ids)}")
        print(f"   - Jobs √∫nicos: {len(set(job_ids))}")
        print(f"   - Inconsistencias: {len(result.data_inconsistencies)}")
        
        result.total_operations = len(job_ids) * 3  # 3 rondas de consultas
        result.successful_operations = found_count
        
        result.print_summary()
        
        # Assertions
        assert len(job_ids) == len(set(job_ids)), "Se generaron job_ids duplicados"
        assert found_count > 0, "No se encontraron jobs en el tracker"


def run_concurrency_stress_test():
    """
    Ejecuta un test de estr√©s de concurrencia adicional.
    
    Este test es m√°s agresivo y puede causar fallos en el sistema.
    Se ejecuta por separado de los tests normales.
    """
    import asyncio
    
    async def stress_test():
        print("\n" + "="*80)
        print("TEST DE ESTR√âS DE CONCURRENCIA (OPCIONAL)")
        print("="*80)
        print("‚ö†Ô∏è  ADVERTENCIA: Este test puede causar fallos en el sistema")
        
        base_url = f"http://{API_HOST}:{API_PORT}"
        
        # Configuraci√≥n agresiva
        num_workers = 100
        requests_per_worker = 50
        
        print(f"\nüìã Configuraci√≥n de estr√©s:")
        print(f"   - Workers: {num_workers}")
        print(f"   - Requests por worker: {requests_per_worker}")
        print(f"   - Total requests: {num_workers * requests_per_worker}")
        
        async def stress_worker(worker_id: int):
            successes = 0
            failures = 0
            
            async with aiohttp.ClientSession() as session:
                for i in range(requests_per_worker):
                    article = {
                        "medio": f"Stress Test {worker_id}",
                        "pais_publicacion": "Test",
                        "tipo_medio": "Digital",
                        "titular": f"Stress test article {worker_id}-{i}",
                        "fecha_publicacion": "2024-01-15T10:00:00Z",
                        "contenido_texto": f"Contenido de estr√©s " * 100,
                        "idioma": "es"
                    }
                    
                    try:
                        async with session.post(
                            f"{base_url}/procesar_articulo",
                            json=article,
                            timeout=aiohttp.ClientTimeout(total=5)
                        ) as response:
                            if response.status == 200:
                                successes += 1
                            else:
                                failures += 1
                    except:
                        failures += 1
            
            return successes, failures
        
        print("\nüöÄ Iniciando test de estr√©s...")
        start_time = time.time()
        
        # Lanzar todos los workers
        workers = [stress_worker(i) for i in range(num_workers)]
        results = await asyncio.gather(*workers, return_exceptions=True)
        
        total_time = time.time() - start_time
        
        # Analizar resultados
        total_successes = 0
        total_failures = 0
        
        for result in results:
            if isinstance(result, tuple):
                successes, failures = result
                total_successes += successes
                total_failures += failures
            else:
                total_failures += requests_per_worker
        
        total_requests = total_successes + total_failures
        success_rate = (total_successes / total_requests * 100) if total_requests > 0 else 0
        
        print(f"\nüìä Resultados del test de estr√©s:")
        print(f"   - Duraci√≥n: {total_time:.2f} segundos")
        print(f"   - Total requests: {total_requests}")
        print(f"   - Exitosas: {total_successes}")
        print(f"   - Fallidas: {total_failures}")
        print(f"   - Tasa de √©xito: {success_rate:.1f}%")
        print(f"   - Throughput: {total_requests/total_time:.2f} req/s")
        
        if success_rate < 50:
            print("\n‚ö†Ô∏è  El sistema mostr√≥ degradaci√≥n severa bajo estr√©s extremo")
        else:
            print("\n‚úÖ El sistema manej√≥ el estr√©s razonablemente bien")
    
    # Ejecutar el test de estr√©s
    asyncio.run(stress_test())


if __name__ == "__main__":
    # Permitir ejecuci√≥n directa del archivo
    import subprocess
    import sys
    
    print("Ejecutando tests de concurrencia...")
    cmd = [sys.executable, "-m", "pytest", __file__, "-v", "-s"]
    result = subprocess.run(cmd)
    
    if result.returncode == 0:
        print("\n‚úÖ Tests de concurrencia completados exitosamente")
        
        # Preguntar si ejecutar test de estr√©s
        response = input("\n¬øEjecutar test de estr√©s adicional? (s/n): ")
        if response.lower() == 's':
            run_concurrency_stress_test()
    else:
        print("\n‚ùå Tests de concurrencia fallaron")
    
    sys.exit(result.returncode)
