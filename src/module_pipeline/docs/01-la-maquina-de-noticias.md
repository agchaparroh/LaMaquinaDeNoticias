#  La M谩quina de Noticias

# Contexto del Sistema "La M谩quina de Noticias" para el Desarrollador del `module_pipeline`

## 1. Visi贸n General del Sistema y Rol del `module_pipeline`

**La M谩quina de Noticias** es un sistema integral dise帽ado para procesar y analizar grandes vol煤menes de informaci贸n period铆stica y documentos extensos. Utiliza inteligencia artificial, espec铆ficamente modelos de lenguaje grandes (LLM), para transformar texto no estructurado en datos estructurados y conectados, facilitando as铆 la investigaci贸n period铆stica.

El `module_pipeline` constituye el **n煤cleo de an谩lisis e inteligencia** de este sistema. Su responsabilidad principal es recibir contenido textual (art铆culos de noticias o fragmentos de documentos) y, mediante una secuencia de fases de procesamiento con LLMs, extraer, estructurar, validar y normalizar la informaci贸n. Los resultados de este procesamiento se persisten en una base de datos central para su posterior consulta y utilizaci贸n por otros componentes del sistema.

Este documento proporciona el contexto general del sistema para facilitar la comprensi贸n de c贸mo el `module_pipeline` se integra y colabora con otros componentes de "La M谩quina de Noticias".

## 2. Flujo General de Datos y Funcionamiento del Sistema

El sistema opera mediante un flujo de datos que involucra varios componentes conceptuales:

### 2.1. Fuentes y Recopilaci贸n de Informaci贸n

El sistema se alimenta de dos fuentes principales de texto:

- **Art铆culos de noticias:** Recopilados autom谩ticamente de diversos medios de comunicaci贸n mediante un componente de web scraping (conceptualizado como `module_scraper`).
- **Documentos extensos:** Informes, libros, leyes, etc., que pueden ingresarse manualmente al sistema a trav茅s de un motor de ingesta (conceptualizado como `module_ingestion_engine`), el cual tambi茅n puede segmentarlos cuando son muy extensos.

Estos textos originales se almacenan para referencia en lo que denominamos "Memoria Profunda".

### 2.2. Conexi贸n y Entrega de Datos al Pipeline

Los art铆culos recopilados y los fragmentos de documentos son preparados y enviados al `module_pipeline` para su procesamiento. Un componente conector (conceptualizado como `module_connector`) es responsable de formalizar los datos del scraper y entregarlos al pipeline.

### 2.3. Procesamiento Inteligente en el `module_pipeline`

Aqu铆 se centra el trabajo principal. El `module_pipeline` recibe el texto y lo procesa en varias fases:

- **Triaje y Preprocesamiento:** Evaluaci贸n inicial, limpieza y posible traducci贸n.
- **Extracci贸n de Informaci贸n:** Utilizaci贸n de LLMs para identificar y extraer elementos clave como hechos, entidades (personas, organizaciones, lugares), citas textuales y datos cuantitativos.
- **Normalizaci贸n y Vinculaci贸n:** Estandarizaci贸n de la informaci贸n extra铆da y vinculaci贸n de entidades a una base de conocimiento existente.
- **Evaluaci贸n y Enriquecimiento:** Asignaci贸n de metadatos adicionales, como la importancia contextual de los hechos.

La configuraci贸n de c贸mo el LLM realiza estas extracciones (los "prompts") se gestiona mediante archivos externos, permitiendo flexibilidad.

### 2.4. Almacenamiento Estructurado (Memoria Relacional)

La informaci贸n estructurada y enriquecida generada por el `module_pipeline` se almacena de forma persistente en una base de datos central (actualmente Supabase/PostgreSQL). Esta base de datos constituye la "Memoria Relacional" del sistema, guardando los hechos, entidades y sus relaciones de manera organizada.

### 2.5. Consulta, Exploraci贸n y Utilizaci贸n

Una vez que los datos est谩n en la base de datos, otros m贸dulos del sistema (conceptualizados como `module_dashboard_review` para revisi贸n editorial, `module_chat_interface` para investigaci贸n) acceden a esta informaci贸n para sus respectivas funcionalidades. El `module_pipeline` es, por tanto, un productor clave de la informaci贸n que estos m贸dulos consumir谩n.

### 2.6. Curaci贸n Editorial y Retroalimentaci贸n

Existe un componente de revisi贸n y curaci贸n editorial. El feedback generado puede influir en la mejora de los prompts del `module_pipeline` o en el reentrenamiento de modelos auxiliares (como el de asignaci贸n de importancia).

## 3. Conceptos Clave para el `module_pipeline`

Para comprender el trabajo del `module_pipeline`, es fundamental conocer estos conceptos sobre los datos que maneja y produce:

- **Hechos (Facts):** Unidades de informaci贸n que describen un evento, situaci贸n o declaraci贸n espec铆fica, extra铆das del texto. Cada hecho incluye atributos como descripci贸n, fecha, ubicaci贸n y entidades involucradas.

- **Entidades (Entities):** Personas, organizaciones, lugares u otros conceptos nombrados relevantes en el contexto de los hechos. El pipeline las identifica y busca normalizarlas.

- **Citas Textuales:** Fragmentos exactos de texto atribuidos a una entidad espec铆fica.

- **Datos Cuantitativos:** Informaci贸n num茅rica estructurada extra铆da del texto (ej., cifras econ贸micas, estad铆sticas).

- **Relaciones:** V铆nculos significativos entre los diferentes elementos extra铆dos (ej., una entidad participa en un hecho, un hecho contradice a otro).

- **Importancia Contextual:** Una m茅trica (escala 1-10) que el `module_pipeline` asigna a los hechos, indicando su relevancia o significancia. Este proceso puede estar guiado por un modelo de machine learning externo o por criterios definidos.

- **Modelo de "Tres Memorias" (Contexto para el Pipeline):**
  - **Memoria Profunda:** Almac茅n de los textos originales (art铆culos, documentos). El `module_pipeline` consume de aqu铆.
  - **Memoria Relacional:** La base de datos estructurada (Supabase) donde el `module_pipeline` escribe sus resultados (hechos, entidades, etc.).
  - **Memoria Superficial (Cach茅s):** Almacenes temporales o de acceso r谩pido (ej., `cache_entidades` en la BD) que el `module_pipeline` puede consultar durante el procesamiento para optimizar la normalizaci贸n de entidades u otras tareas.

- **Trazabilidad:** Es importante poder rastrear la informaci贸n estructurada hasta su origen en el texto original. El `module_pipeline` debe facilitar esto mediante la correcta asignaci贸n de IDs y referencias.

- **Fragmentos de Documentos:** El `module_pipeline` debe ser capaz de procesar no solo art铆culos completos sino tambi茅n segmentos o fragmentos de documentos m谩s largos, manteniendo el contexto del documento original.

## 4. Configuraci贸n y Adaptabilidad del `module_pipeline`

Una caracter铆stica importante del `module_pipeline` es su adaptabilidad, lograda principalmente a trav茅s de:

- **Prompts del LLM Externalizados:** Las instrucciones detalladas que gu铆an al LLM en cada fase de extracci贸n se almacenan en archivos de texto externos. Esto permite que los prompts sean ajustados y mejorados sin modificar el c贸digo central del pipeline. Estos archivos se gestionan dentro del directorio `prompts/` del `module_pipeline`.

- **Modelos de ML Auxiliares:** Para tareas como la asignaci贸n de "importancia", el `module_pipeline` puede interactuar con modelos de machine learning que son entrenados y versionados por procesos externos (conceptualizados como "IA Nocturna"). El pipeline integra y utiliza las predicciones de estos modelos.

Esta estructura permite que el `module_pipeline` evolucione y se adapte a nuevas necesidades o mejoras en los modelos de IA.
